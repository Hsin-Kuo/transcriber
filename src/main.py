"""
Whisper è½‰éŒ„æœå‹™ - æ–°æ‡‰ç”¨å…¥å£
æ¡ç”¨æ¸…æ™°çš„ä¸‰å±¤æ¶æ§‹è¨­è¨ˆ
"""

import os
import asyncio
import signal
import subprocess
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime, timezone, timedelta

from fastapi import FastAPI, HTTPException, status
from fastapi.middleware.cors import CORSMiddleware
from faster_whisper import WhisperModel
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# è³‡æ–™åº«å’Œ Repositories
from src.database.mongodb import MongoDB
from src.database.repositories.task_repo import TaskRepository
from src.database.repositories.tag_repo import TagRepository
from src.database.repositories.audit_log_repo import AuditLogRepository

# Routers
from src.routers import auth as auth_router
from src.routers import tasks as tasks_router
from src.routers import transcriptions as transcriptions_router
from src.routers import tags as tags_router
from src.routers import audio as audio_router
from src.routers import summaries as summaries_router

# Services
from src.services.utils.diarization_processor import DiarizationProcessor

# Utils
from src.utils.audit_logger import init_audit_logger

# å…±äº«ç‹€æ…‹
from src.utils.shared_state import (
    transcription_tasks,
    task_cancelled,
    task_temp_dirs,
    task_diarization_processes,
    tasks_lock
)

# è¨­å®š
DEFAULT_MODEL = "medium"
# OUTPUT_DIR å·²ç§»é™¤ - æ–‡å­—æª”å’Œ segments ç¾åœ¨å­˜å„²åœ¨ MongoDB ä¸­

# æ™‚å€è¨­å®š (UTC+8 å°åŒ—æ™‚é–“)
TZ_UTC8 = timezone(timedelta(hours=8))

# å…¨åŸŸæœå‹™å¯¦ä¾‹
whisper_model = None
current_model_name = None
diarization_pipeline = None
task_repo = None
tag_repo = None
audit_log_repo = None
main_loop = None
executor = ThreadPoolExecutor(max_workers=2)  # é™ä½ä¸¦ç™¼æ•¸é¿å…è¨˜æ†¶é«”çˆ†ç‚¸

# æª¢æŸ¥ Diarization æ˜¯å¦å¯ç”¨
try:
    from pyannote.audio import Pipeline
    DIARIZATION_AVAILABLE = True
except ImportError:
    DIARIZATION_AVAILABLE = False
    print("âš ï¸  pyannote.audio æœªå®‰è£ï¼Œspeaker diarization åŠŸèƒ½ä¸å¯ç”¨")


# ========== å‰µå»º FastAPI æ‡‰ç”¨ ==========

app = FastAPI(
    title="Whisper è½‰éŒ„æœå‹™",
    description="åŸºæ–¼ä¸‰å±¤æ¶æ§‹çš„éŸ³æª”è½‰éŒ„æœå‹™",
    version="3.0.0"
)

# CORS ä¸­é–“ä»¶
cors_origins_str = os.getenv("CORS_ORIGINS", "*")
cors_origins = [origin.strip() for origin in cors_origins_str.split(",")] if cors_origins_str != "*" else ["*"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=cors_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["*"],
    max_age=3600,
)

# ========== è¨»å†Šæ‰€æœ‰è·¯ç”± ==========

app.include_router(auth_router.router)
app.include_router(tasks_router.router)
app.include_router(transcriptions_router.router)
app.include_router(tags_router.router)
app.include_router(audio_router.router)
app.include_router(summaries_router.router)


# ========== é€²ç¨‹æ¸…ç†å·¥å…·å‡½æ•¸ ==========

def cleanup_worker_processes():
    """æ¸…ç†æ‰€æœ‰ ProcessPoolExecutor worker é€²ç¨‹"""
    try:
        result = subprocess.run(
            ["pgrep", "-f", "multiprocessing.spawn"],
            capture_output=True,
            text=True
        )
        if result.stdout.strip():
            pids = result.stdout.strip().split('\n')
            print(f"ğŸ§¹ æ¸…ç† {len(pids)} å€‹ worker é€²ç¨‹...")
            subprocess.run(["pkill", "-9", "-f", "multiprocessing.spawn"], check=False)
            subprocess.run(["pkill", "-9", "-f", "multiprocessing.resource_tracker"], check=False)
            return len(pids)
        return 0
    except Exception as e:
        print(f"âš ï¸  æ¸…ç† worker é€²ç¨‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return 0


def signal_handler(signum, frame):
    """è™•ç†çµ‚æ­¢ä¿¡è™Ÿï¼Œç¢ºä¿æ¸…ç†æ‰€æœ‰è³‡æº"""
    print(f"\nâš ï¸  æ”¶åˆ°çµ‚æ­¢ä¿¡è™Ÿ ({signal.Signals(signum).name})ï¼Œæ­£åœ¨æ¸…ç†...")
    cleanup_worker_processes()
    print(f"âœ… æ¸…ç†å®Œæˆï¼Œé€€å‡ºç¨‹åº")
    exit(0)


# è¨»å†Šä¿¡è™Ÿè™•ç†å™¨
signal.signal(signal.SIGTERM, signal_handler)
signal.signal(signal.SIGINT, signal_handler)


# ========== å•Ÿå‹•èˆ‡é—œé–‰äº‹ä»¶ ==========

@app.on_event("startup")
async def startup_event():
    """æ‡‰ç”¨å•Ÿå‹•æ™‚çš„åˆå§‹åŒ–"""
    global whisper_model, current_model_name, task_repo, tag_repo, audit_log_repo, main_loop, diarization_pipeline

    print("ğŸš€ å•Ÿå‹• Whisper è½‰éŒ„æœå‹™ v3.0.0", flush=True)
    print("=" * 50, flush=True)

    # æ¸…ç†æ®˜ç•™çš„ ProcessPoolExecutor worker é€²ç¨‹
    print("ğŸ§¹ æ¸…ç†æ®˜ç•™çš„ worker é€²ç¨‹...", flush=True)
    try:
        cleaned = cleanup_worker_processes()
        if cleaned > 0:
            print(f"   âœ… å·²æ¸…ç† {cleaned} å€‹æ®˜ç•™é€²ç¨‹", flush=True)
        else:
            print("   âœ… æ²’æœ‰ç™¼ç¾æ®˜ç•™çš„ worker é€²ç¨‹", flush=True)
    except Exception as e:
        print(f"   âš ï¸  æ¸…ç†é€²ç¨‹æ™‚å‡ºéŒ¯: {e}", flush=True)

    # ç²å–ä¸»äº‹ä»¶å¾ªç’°
    print("ğŸ“¡ ç²å–äº‹ä»¶å¾ªç’°...", flush=True)
    main_loop = asyncio.get_running_loop()
    print("âœ… äº‹ä»¶å¾ªç’°å·²å°±ç·’", flush=True)

    # 1. é€£æ¥ MongoDB
    mongodb_url = os.getenv('MONGODB_URL', 'mongodb://localhost:27017')
    mongodb_db = os.getenv('MONGODB_DB_NAME', 'whisper_transcriber')
    print(f"ğŸ”Œ æ­£åœ¨é€£æ¥ MongoDB...", flush=True)
    print(f"   URL: {mongodb_url}", flush=True)
    print(f"   Database: {mongodb_db}", flush=True)
    try:
        await asyncio.wait_for(MongoDB.connect(), timeout=10.0)
        print(f"âœ… å·²é€£æ¥åˆ° MongoDB: {mongodb_db}", flush=True)
    except asyncio.TimeoutError:
        print(f"âŒ MongoDB é€£æ¥è¶…æ™‚ï¼ˆ10ç§’ï¼‰", flush=True)
        print(f"   è«‹ç¢ºä¿ MongoDB æ­£åœ¨é‹è¡Œï¼šdocker ps | grep mongo", flush=True)
        print(f"   URL: {mongodb_url}", flush=True)
        raise
    except Exception as e:
        print(f"âŒ MongoDB é€£æ¥å¤±æ•—: {e}", flush=True)
        print(f"   è«‹ç¢ºä¿ MongoDB æ­£åœ¨é‹è¡Œä¸¦æª¢æŸ¥ .env é…ç½®", flush=True)
        print(f"   URL: {mongodb_url}", flush=True)
        raise

    # 2. åˆå§‹åŒ– Repositories
    print(f"ğŸ“‚ æ­£åœ¨åˆå§‹åŒ– Repositories...")
    db = MongoDB.get_db()
    task_repo = TaskRepository(db)
    tag_repo = TagRepository(db)
    audit_log_repo = AuditLogRepository(db)

    # å»ºç«‹ç´¢å¼•
    try:
        await task_repo.create_indexes()
        await audit_log_repo.create_indexes()
        # å»ºç«‹ Summaries ç´¢å¼•
        from src.database.repositories.summary_repo import SummaryRepository
        summary_repo = SummaryRepository(db)
        await summary_repo.create_indexes()
        print(f"âœ… è³‡æ–™åº«ç´¢å¼•å»ºç«‹å®Œæˆ")
    except Exception as e:
        print(f"âš ï¸  ç´¢å¼•å»ºç«‹å¤±æ•—: {e}")

    # çµ±è¨ˆä»»å‹™æ•¸é‡
    task_count = await db.tasks.count_documents({})
    print(f"âœ… è³‡æ–™åº«å·²å°±ç·’ï¼ˆå…± {task_count} å€‹ä»»å‹™ï¼‰")

    # åˆå§‹åŒ– AuditLogger
    print(f"ğŸ“ æ­£åœ¨åˆå§‹åŒ– AuditLogger...")
    init_audit_logger(audit_log_repo)
    print(f"âœ… AuditLogger åˆå§‹åŒ–å®Œæˆ")

    # 3. åˆå§‹åŒ– TaskServiceï¼ˆä½¿ç”¨å…±äº«çš„å…¨åŸŸå­—å…¸ï¼‰
    print(f"ğŸ”§ æ­£åœ¨åˆå§‹åŒ– TaskService...")
    task_service = tasks_router.init_task_service(
        db,
        memory_tasks=transcription_tasks,
        cancelled_tasks=task_cancelled,
        temp_dirs=task_temp_dirs,
        diarization_processes=task_diarization_processes,
        lock=tasks_lock
    )
    print(f"âœ… TaskService åˆå§‹åŒ–å®Œæˆ")

    # 4. æ¸…ç†ç•°å¸¸ä¸­æ–·çš„ä»»å‹™
    print(f"ğŸ§¹ æ¸…ç†ç•°å¸¸ä¸­æ–·çš„ä»»å‹™...")
    await task_service.cleanup_orphaned_tasks()

    # 5. å•Ÿå‹•å®šæœŸè¨˜æ†¶é«”æ¸…ç†
    asyncio.create_task(task_service.periodic_memory_cleanup())

    # 5.1. å•Ÿå‹•å®šæœŸå­¤ç«‹é€²ç¨‹æ¸…ç†
    asyncio.create_task(task_service.periodic_orphaned_process_cleanup())

    # 5.5. å•Ÿå‹•ä»»å‹™éšŠåˆ—è™•ç†å™¨ï¼ˆåœ¨ TranscriptionService åˆå§‹åŒ–å¾Œï¼‰
    # æ³¨æ„ï¼šé€™è£¡æš«æ™‚å…ˆå‰µå»ºä»»å‹™ï¼Œç¨å¾Œåœ¨ TranscriptionService åˆå§‹åŒ–å¾Œæœƒå¯¦éš›å•Ÿå‹•
    queue_processor_task = None

    # 6. è¼‰å…¥ Whisper æ¨¡å‹
    print(f"ğŸ™ æ­£åœ¨è¼‰å…¥ Whisper æ¨¡å‹ï¼š{DEFAULT_MODEL}...")
    print(f"ğŸ”§ é…ç½®ï¼šdevice=auto, compute_type=int8")
    current_model_name = DEFAULT_MODEL
    whisper_model = WhisperModel(
        current_model_name,
        device="auto",
        compute_type="int8",
        cpu_threads=2,  # å„ªåŒ–ï¼šé…åˆ ProcessPoolExecutorï¼Œé™ä½å–®é€²ç¨‹ä¸¦è¡Œåº¦
        num_workers=1   # å„ªåŒ–ï¼šé¿å…é€²ç¨‹å…§éåº¦ä¸¦è¡Œï¼ˆå¤–éƒ¨å·²æœ‰ ProcessPoolExecutorï¼‰
    )
    print(f"âœ… Whisper æ¨¡å‹è¼‰å…¥å®Œæˆï¼")

    # 8. è¼‰å…¥ Diarization æ¨¡å‹ï¼ˆå¯é¸ï¼‰
    if DIARIZATION_AVAILABLE:
        hf_token = os.getenv("HF_TOKEN")
        if hf_token:
            diarization_pipeline = DiarizationProcessor.load_pipeline(hf_token)
        else:
            print("â„¹ï¸  æœªè¨­å®š HF_TOKENï¼Œspeaker diarization åŠŸèƒ½ä¸å¯ç”¨")

    # 9. åˆå§‹åŒ– TranscriptionService
    print(f"ğŸ”§ æ­£åœ¨åˆå§‹åŒ– TranscriptionService...")
    transcription_service = transcriptions_router.init_transcription_service(
        whisper_model=whisper_model,
        task_service=task_service,
        model_name=current_model_name,  # å‚³éæ¨¡å‹åç¨±ä¾› ProcessPoolExecutor ä½¿ç”¨
        diarization_pipeline=diarization_pipeline,
        executor=executor
    )
    print(f"âœ… TranscriptionService åˆå§‹åŒ–å®Œæˆ")

    # 10. å•Ÿå‹•ä»»å‹™éšŠåˆ—è™•ç†å™¨
    print(f"ğŸš€ æ­£åœ¨å•Ÿå‹•ä»»å‹™éšŠåˆ—è™•ç†å™¨...")
    asyncio.create_task(task_service.process_pending_queue(transcription_service, max_concurrent=2))
    print(f"âœ… ä»»å‹™éšŠåˆ—è™•ç†å™¨å·²å•Ÿå‹•")

    print("=" * 50)
    print(f"âœ¨ æœå‹™å·²å°±ç·’ï¼")
    print(f"ğŸ“š API æ–‡æª”ï¼šhttp://localhost:8000/docs")
    print(f"ğŸ”— å¥åº·æª¢æŸ¥ï¼šhttp://localhost:8000/health")
    print(f"ğŸ“‹ ä»»å‹™éšŠåˆ—ï¼šæœ€å¤š 2 å€‹ä¸¦ç™¼ä»»å‹™")
    print("=" * 50)


@app.on_event("shutdown")
async def shutdown_event():
    """æ‡‰ç”¨é—œé–‰æ™‚çš„æ¸…ç†"""
    print(f"ğŸ‘‹ æ­£åœ¨é—œé–‰æœå‹™...")

    # é—œé–‰ç·šç¨‹æ± 
    if executor:
        executor.shutdown(wait=True)
        print(f"âœ… ç·šç¨‹æ± å·²é—œé–‰")

    # æ¸…ç†æ‰€æœ‰ ProcessPoolExecutor worker é€²ç¨‹
    cleaned = cleanup_worker_processes()
    if cleaned > 0:
        print(f"âœ… å·²æ¸…ç† {cleaned} å€‹ worker é€²ç¨‹")

    # æ–·é–‹ MongoDB
    await MongoDB.close()
    print(f"âœ… MongoDB é€£æ¥å·²é—œé–‰")

    print(f"ğŸ‘‹ æœå‹™å·²é—œé–‰")


# ========== åŸºæœ¬ç«¯é» ==========

@app.get("/")
async def root():
    """æ ¹ç«¯é»"""
    return {
        "service": "Whisper è½‰éŒ„æœå‹™",
        "version": "3.0.0",
        "architecture": "ä¸‰å±¤æ¶æ§‹",
        "status": "running",
        "docs": "/docs",
        "health": "/health"
    }


@app.get("/health")
async def health_check():
    """å¥åº·æª¢æŸ¥ç«¯é»"""
    return {
        "status": "healthy",
        "whisper_model": current_model_name,
        "diarization_available": diarization_pipeline is not None,
        "database": "connected" if MongoDB.get_db() is not None else "disconnected"
    }


@app.get("/api/admin/statistics")
async def get_admin_statistics():
    """ç²å–å¾Œå°çµ±è¨ˆè³‡æ–™ï¼ˆæš«æ™‚ç„¡éœ€èªè­‰ï¼‰"""
    try:
        db = MongoDB.get_db()

        # 1. ç¸½é«”çµ±è¨ˆ
        total_tasks = await db.tasks.count_documents({})
        completed_tasks = await db.tasks.count_documents({"status": "completed"})
        processing_tasks = await db.tasks.count_documents({"status": "processing"})
        failed_tasks = await db.tasks.count_documents({"status": "failed"})

        # 2. Token ä½¿ç”¨çµ±è¨ˆ
        token_pipeline = [
            {
                "$match": {
                    "stats.token_usage.total": {"$exists": True, "$ne": None}
                }
            },
            {
                "$group": {
                    "_id": None,
                    "total_tokens": {"$sum": "$stats.token_usage.total"},
                    "total_prompt_tokens": {"$sum": "$stats.token_usage.prompt"},
                    "total_completion_tokens": {"$sum": "$stats.token_usage.completion"},
                    "tasks_with_tokens": {"$sum": 1}
                }
            }
        ]
        token_stats_cursor = db.tasks.aggregate(token_pipeline)
        token_stats_list = await token_stats_cursor.to_list(length=1)
        token_stats = token_stats_list[0] if token_stats_list else {
            "total_tokens": 0,
            "total_prompt_tokens": 0,
            "total_completion_tokens": 0,
            "tasks_with_tokens": 0
        }

        # 3. æ¨¡å‹ä½¿ç”¨çµ±è¨ˆï¼ˆåŸºæ–¼æ–°çš„ models æ¬„ä½ï¼‰
        # 3.1 æ¨™é»ç¬¦è™Ÿæ¨¡å‹çµ±è¨ˆ
        punctuation_model_pipeline = [
            {
                "$match": {
                    "models.punctuation": {"$exists": True, "$ne": None}
                }
            },
            {
                "$group": {
                    "_id": "$models.punctuation",
                    "count": {"$sum": 1}
                }
            },
            {
                "$sort": {"count": -1}
            }
        ]
        punct_model_cursor = db.tasks.aggregate(punctuation_model_pipeline)
        punct_model_stats = await punct_model_cursor.to_list(length=None)

        # 3.2 è½‰éŒ„æ¨¡å‹çµ±è¨ˆï¼ˆæœªä¾†ä½¿ç”¨ï¼‰
        transcription_model_pipeline = [
            {
                "$match": {
                    "models.transcription": {"$exists": True, "$ne": None}
                }
            },
            {
                "$group": {
                    "_id": "$models.transcription",
                    "count": {"$sum": 1}
                }
            },
            {
                "$sort": {"count": -1}
            }
        ]
        trans_model_cursor = db.tasks.aggregate(transcription_model_pipeline)
        trans_model_stats = await trans_model_cursor.to_list(length=None)

        # 3.3 èªªè©±è€…è¾¨è­˜æ¨¡å‹çµ±è¨ˆï¼ˆæœªä¾†ä½¿ç”¨ï¼‰
        diarization_model_pipeline = [
            {
                "$match": {
                    "models.diarization": {"$exists": True, "$ne": None}
                }
            },
            {
                "$group": {
                    "_id": "$models.diarization",
                    "count": {"$sum": 1}
                }
            },
            {
                "$sort": {"count": -1}
            }
        ]
        diar_model_cursor = db.tasks.aggregate(diarization_model_pipeline)
        diar_model_stats = await diar_model_cursor.to_list(length=None)

        # 4. æ¯æ—¥çµ±è¨ˆï¼ˆæœ€è¿‘ 30 å¤©ï¼‰
        thirty_days_ago = (datetime.now(TZ_UTC8) - timedelta(days=30)).strftime("%Y-%m-%d")

        daily_pipeline = [
            {
                "$match": {
                    "timestamps.created_at": {"$gte": thirty_days_ago}
                }
            },
            {
                "$group": {
                    "_id": {"$substr": ["$timestamps.created_at", 0, 10]},
                    "tasks_count": {"$sum": 1},
                    "tokens_used": {"$sum": {"$ifNull": ["$stats.token_usage.total", 0]}}
                }
            },
            {
                "$sort": {"_id": 1}
            }
        ]
        daily_stats_cursor = db.tasks.aggregate(daily_pipeline)
        daily_stats = await daily_stats_cursor.to_list(length=None)

        # 5. ä½¿ç”¨è€…çµ±è¨ˆ
        user_stats_pipeline = [
            {
                "$group": {
                    "_id": "$user.user_id",
                    "tasks_count": {"$sum": 1},
                    "tokens_used": {"$sum": {"$ifNull": ["$stats.token_usage.total", 0]}}
                }
            },
            {
                "$sort": {"tasks_count": -1}
            },
            {
                "$limit": 10
            }
        ]
        user_stats_cursor = db.tasks.aggregate(user_stats_pipeline)
        top_users = await user_stats_cursor.to_list(length=None)

        # 6. å¹³å‡è™•ç†æ™‚é–“
        avg_duration_pipeline = [
            {
                "$match": {
                    "status": "completed",
                    "stats.duration_seconds": {"$exists": True, "$ne": None}
                }
            },
            {
                "$group": {
                    "_id": None,
                    "avg_duration": {"$avg": "$stats.duration_seconds"},
                    "min_duration": {"$min": "$stats.duration_seconds"},
                    "max_duration": {"$max": "$stats.duration_seconds"}
                }
            }
        ]
        duration_stats_cursor = db.tasks.aggregate(avg_duration_pipeline)
        duration_stats_list = await duration_stats_cursor.to_list(length=1)
        duration_stats = duration_stats_list[0] if duration_stats_list else {
            "avg_duration": 0,
            "min_duration": 0,
            "max_duration": 0
        }

        # 7. æ¨™é»ç¬¦è™Ÿæœå‹™ä½¿ç”¨çµ±è¨ˆ
        punct_pipeline = [
            {
                "$group": {
                    "_id": "$config.punct_provider",
                    "count": {"$sum": 1}
                }
            },
            {
                "$sort": {"count": -1}
            }
        ]
        punct_stats_cursor = db.tasks.aggregate(punct_pipeline)
        punct_stats = await punct_stats_cursor.to_list(length=None)

        return {
            "overview": {
                "total_tasks": total_tasks,
                "completed_tasks": completed_tasks,
                "processing_tasks": processing_tasks,
                "failed_tasks": failed_tasks,
                "success_rate": round(completed_tasks / total_tasks * 100, 2) if total_tasks > 0 else 0
            },
            "token_usage": {
                "total_tokens": token_stats.get("total_tokens", 0),
                "prompt_tokens": token_stats.get("total_prompt_tokens", 0),
                "completion_tokens": token_stats.get("total_completion_tokens", 0),
                "tasks_with_tokens": token_stats.get("tasks_with_tokens", 0),
                "avg_tokens_per_task": round(token_stats.get("total_tokens", 0) / token_stats.get("tasks_with_tokens", 1), 2) if token_stats.get("tasks_with_tokens", 0) > 0 else 0
            },
            "model_usage": {
                "punctuation": [
                    {
                        "model": stat["_id"] or "æœªçŸ¥",
                        "count": stat["count"]
                    }
                    for stat in punct_model_stats
                ],
                "transcription": [
                    {
                        "model": stat["_id"] or "æœªçŸ¥",
                        "count": stat["count"]
                    }
                    for stat in trans_model_stats
                ],
                "diarization": [
                    {
                        "model": stat["_id"] or "æœªçŸ¥",
                        "count": stat["count"]
                    }
                    for stat in diar_model_stats
                ]
            },
            "daily_stats": [
                {
                    "date": stat["_id"],
                    "tasks_count": stat["tasks_count"],
                    "tokens_used": stat["tokens_used"]
                }
                for stat in daily_stats
            ],
            "top_users": [
                {
                    "user_id": stat["_id"],
                    "tasks_count": stat["tasks_count"],
                    "tokens_used": stat["tokens_used"]
                }
                for stat in top_users
            ],
            "performance": {
                "avg_duration_seconds": round(duration_stats.get("avg_duration", 0), 2),
                "min_duration_seconds": round(duration_stats.get("min_duration", 0), 2),
                "max_duration_seconds": round(duration_stats.get("max_duration", 0), 2)
            },
            "punct_provider_usage": [
                {
                    "provider": stat["_id"] or "none",
                    "count": stat["count"]
                }
                for stat in punct_stats
            ]
        }

    except Exception as e:
        print(f"âŒ ç²å–çµ±è¨ˆè³‡æ–™å¤±æ•—ï¼š{e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ç²å–çµ±è¨ˆè³‡æ–™å¤±æ•—ï¼š{str(e)}"
        )


@app.get("/api/admin/audit-logs")
async def get_audit_logs(
    limit: int = 100,
    skip: int = 0,
    log_type: str = None,
    user_id: str = None
):
    """ç²å–æ“ä½œè¨˜éŒ„ï¼ˆç®¡ç†å“¡ï¼‰

    Args:
        limit: é™åˆ¶æ•¸é‡ï¼ˆé è¨­ 100ï¼‰
        skip: è·³éæ•¸é‡ï¼ˆé è¨­ 0ï¼‰
        log_type: éæ¿¾æ—¥èªŒé¡å‹ï¼ˆå¯é¸ï¼‰
        user_id: éæ¿¾ç”¨æˆ¶ IDï¼ˆå¯é¸ï¼‰

    Returns:
        æ“ä½œè¨˜éŒ„åˆ—è¡¨
    """
    try:
        if user_id:
            logs = await audit_log_repo.get_by_user(user_id, limit=limit, skip=skip, log_type=log_type)
        else:
            logs = await audit_log_repo.get_recent(limit=limit, skip=skip, log_type=log_type)

        # è½‰æ› ObjectId ç‚ºå­—ä¸²
        for log in logs:
            if "_id" in log:
                log["_id"] = str(log["_id"])

        return {
            "logs": logs,
            "total": len(logs),
            "limit": limit,
            "skip": skip
        }
    except Exception as e:
        print(f"âŒ ç²å–æ“ä½œè¨˜éŒ„å¤±æ•—ï¼š{e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ç²å–æ“ä½œè¨˜éŒ„å¤±æ•—ï¼š{str(e)}"
        )


@app.get("/api/admin/audit-logs/failed")
async def get_failed_audit_logs(
    days: int = 7,
    limit: int = 100
):
    """ç²å–å¤±æ•—çš„æ“ä½œè¨˜éŒ„ï¼ˆç®¡ç†å“¡ï¼‰

    Args:
        days: æœ€è¿‘å¹¾å¤©ï¼ˆé è¨­ 7ï¼‰
        limit: é™åˆ¶æ•¸é‡ï¼ˆé è¨­ 100ï¼‰

    Returns:
        å¤±æ•—æ“ä½œè¨˜éŒ„åˆ—è¡¨
    """
    try:
        logs = await audit_log_repo.get_failed_operations(days=days, limit=limit)

        return {
            "failed_logs": logs,
            "total": len(logs),
            "days": days
        }
    except Exception as e:
        print(f"âŒ ç²å–å¤±æ•—æ“ä½œè¨˜éŒ„å¤±æ•—ï¼š{e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ç²å–å¤±æ•—æ“ä½œè¨˜éŒ„å¤±æ•—ï¼š{str(e)}"
        )


@app.get("/api/admin/audit-logs/statistics")
async def get_audit_statistics(
    days: int = 30
):
    """ç²å–æ“ä½œè¨˜éŒ„çµ±è¨ˆï¼ˆç®¡ç†å“¡ï¼‰

    Args:
        days: æœ€è¿‘å¹¾å¤©ï¼ˆé è¨­ 30ï¼‰

    Returns:
        æ“ä½œçµ±è¨ˆ
    """
    try:
        stats = await audit_log_repo.get_statistics(days=days)

        return {
            "statistics": stats,
            "days": days
        }
    except Exception as e:
        print(f"âŒ ç²å–æ“ä½œçµ±è¨ˆå¤±æ•—ï¼š{e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ç²å–æ“ä½œçµ±è¨ˆå¤±æ•—ï¼š{str(e)}"
        )


@app.get("/api/admin/audit-logs/resource/{resource_id}")
async def get_resource_audit_logs(
    resource_id: str,
    limit: int = 50
):
    """ç²å–ç‰¹å®šè³‡æºçš„æ“ä½œè¨˜éŒ„ï¼ˆç®¡ç†å“¡ï¼‰

    Args:
        resource_id: è³‡æº IDï¼ˆå¦‚ task_idï¼‰
        limit: é™åˆ¶æ•¸é‡ï¼ˆé è¨­ 50ï¼‰

    Returns:
        è³‡æºæ“ä½œè¨˜éŒ„åˆ—è¡¨
    """
    try:
        logs = await audit_log_repo.get_by_resource(resource_id, limit=limit)

        return {
            "resource_id": resource_id,
            "logs": logs,
            "total": len(logs)
        }
    except Exception as e:
        print(f"âŒ ç²å–è³‡æºæ“ä½œè¨˜éŒ„å¤±æ•—ï¼š{e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ç²å–è³‡æºæ“ä½œè¨˜éŒ„å¤±æ•—ï¼š{str(e)}"
        )


# ========== ä¸»ç¨‹åºå…¥å£ ==========

if __name__ == "__main__":
    import uvicorn

    # å¾ç’°å¢ƒè®Šæ•¸è®€å–é…ç½®
    host = os.getenv("HOST", "0.0.0.0")
    port = int(os.getenv("PORT", 8000))
    reload = os.getenv("RELOAD", "false").lower() == "true"

    uvicorn.run(
        "src.main:app",
        host=host,
        port=port,
        reload=reload,
        log_level="info"
    )
