"""
WhisperProcessor - Whisper è½‰éŒ„è™•ç†å™¨
è·è²¬ï¼šWhisper æ¨¡å‹çš„å°è£ï¼ˆç„¡ç‹€æ…‹å·¥å…·é¡ï¼‰
"""

from pathlib import Path
from typing import Optional, Tuple, List, Dict, Any
import subprocess
import json
import re
from pydub import AudioSegment
from faster_whisper import WhisperModel
from concurrent.futures import ProcessPoolExecutor, as_completed


def transcribe_chunk_worker(
    chunk_path: str,
    model_name: str,
    device: str,
    compute_type: str,
    cpu_threads: int,
    num_workers: int,
    language: Optional[str] = None
) -> Tuple[int, str, List[Dict], str]:
    """
    ç¨ç«‹é€²ç¨‹ä¸­åŸ·è¡Œçš„ chunk è½‰éŒ„å‡½æ•¸ï¼ˆå¿…é ˆæ˜¯é ‚å±¤å‡½æ•¸ä»¥æ”¯æŒ pickleï¼‰

    Args:
        chunk_path: chunk æ–‡ä»¶è·¯å¾‘ï¼ˆå­—ç¬¦ä¸²ï¼Œå¯åºåˆ—åŒ–ï¼‰
        model_name: Whisper æ¨¡å‹åç¨±
        device: è¨­å‚™ï¼ˆauto/cpu/cudaï¼‰
        compute_type: è¨ˆç®—é¡å‹ï¼ˆint8/float16/float32ï¼‰
        cpu_threads: CPU ç·šç¨‹æ•¸
        num_workers: worker æ•¸é‡
        language: èªè¨€ä»£ç¢¼

    Returns:
        (chunk_idx, text, segments, detected_language)
    """
    from faster_whisper import WhisperModel
    from pathlib import Path
    import re

    print(f"[Worker] é€²ç¨‹å•Ÿå‹•ï¼Œè™•ç† chunk: {chunk_path}", flush=True)

    # å¾æ–‡ä»¶åæå– chunk_idxï¼ˆä¾‹å¦‚ï¼š_temp_input_chunk_3.wav â†’ 3ï¼‰
    chunk_idx = int(re.search(r'chunk_(\d+)', chunk_path).group(1))

    print(f"[Worker {chunk_idx}] è¼‰å…¥ Whisper æ¨¡å‹: {model_name}", flush=True)

    # åœ¨é€²ç¨‹å…§ç¨ç«‹å‰µå»º Whisper æ¨¡å‹å¯¦ä¾‹
    model = WhisperModel(
        model_name,
        device=device,
        compute_type=compute_type,
        cpu_threads=cpu_threads,
        num_workers=num_workers
    )

    print(f"[Worker {chunk_idx}] é–‹å§‹è½‰éŒ„", flush=True)

    # åŸ·è¡Œè½‰éŒ„
    segments_list, info = model.transcribe(
        chunk_path,
        language=language,
        beam_size=5,
        vad_filter=True,
        vad_parameters=dict(min_silence_duration_ms=500)
    )

    # æ”¶é›†çµæœ
    segments = []
    text_parts = []
    for segment in segments_list:
        segments.append({
            "start": segment.start,
            "end": segment.end,
            "text": segment.text
        })
        text_parts.append(segment.text)

    full_text = " ".join(text_parts)
    detected_language = info.language

    print(f"[Worker {chunk_idx}] è½‰éŒ„å®Œæˆï¼Œæ–‡å­—é•·åº¦: {len(full_text)}", flush=True)

    # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
    try:
        Path(chunk_path).unlink()
        print(f"[Worker {chunk_idx}] å·²åˆªé™¤è‡¨æ™‚æ–‡ä»¶", flush=True)
    except Exception as e:
        print(f"[Worker {chunk_idx}] æ¸…ç†è‡¨æ™‚æ–‡ä»¶å¤±æ•—: {e}", flush=True)

    return chunk_idx, full_text, segments, detected_language


class WhisperProcessor:
    """Whisper è½‰éŒ„è™•ç†å™¨

    å°è£ Whisper æ¨¡å‹çš„è½‰éŒ„åŠŸèƒ½ï¼Œæä¾›ç„¡ç‹€æ…‹çš„è½‰éŒ„æ–¹æ³•
    """

    def __init__(self, model: WhisperModel, model_name: str = "medium"):
        """åˆå§‹åŒ– WhisperProcessor

        Args:
            model: Whisper æ¨¡å‹å¯¦ä¾‹
            model_name: æ¨¡å‹åç¨±ï¼ˆç”¨æ–¼ worker é€²ç¨‹ä¸­é‡æ–°è¼‰å…¥æ¨¡å‹ï¼‰
        """
        self.model = model
        self.model_name = model_name  # ä¿å­˜æ¨¡å‹åç¨±ä¾› worker ä½¿ç”¨

    def transcribe(
        self,
        audio_path: Path,
        language: Optional[str] = None
    ) -> Tuple[str, List[Dict], str]:
        """è½‰éŒ„éŸ³æª”ï¼ˆå–®æ¬¡è½‰éŒ„ï¼Œä¸åˆ†æ®µï¼‰

        Args:
            audio_path: éŸ³æª”è·¯å¾‘
            language: èªè¨€ä»£ç¢¼ï¼ˆNone è¡¨ç¤ºè‡ªå‹•åµæ¸¬ï¼‰

        Returns:
            (å®Œæ•´æ–‡å­—, segments åˆ—è¡¨, åµæ¸¬åˆ°çš„èªè¨€)
        """
        segments_list, detected_language = self._transcribe_with_timestamps(
            audio_path, language
        )

        # åˆä½µæ‰€æœ‰ segment çš„æ–‡å­—
        full_text = " ".join(seg["text"] for seg in segments_list)

        return full_text, segments_list, detected_language

    def transcribe_in_chunks(
        self,
        audio_path: Path,
        chunk_duration_ms: int = 420000,  # 7 åˆ†é˜
        language: Optional[str] = None,
        progress_callback: Optional[callable] = None
    ) -> Tuple[str, List[Dict], str]:
        """å°‡éŸ³æª”åˆ†æ®µå¾Œè½‰éŒ„ï¼ˆæé«˜é•·éŸ³æª”çš„æº–ç¢ºåº¦ï¼‰

        Args:
            audio_path: éŸ³æª”è·¯å¾‘
            chunk_duration_ms: æ¯æ®µé•·åº¦ï¼ˆæ¯«ç§’ï¼‰
            language: èªè¨€ä»£ç¢¼ï¼ˆNone è¡¨ç¤ºè‡ªå‹•åµæ¸¬ï¼‰
            progress_callback: é€²åº¦å›èª¿å‡½æ•¸ callback(chunk_idx, total_chunks)

        Returns:
            (å®Œæ•´æ–‡å­—, segments åˆ—è¡¨, åµæ¸¬åˆ°çš„èªè¨€)
        """
        # ç²å–éŸ³æª”ç¸½é•·åº¦
        total_duration_ms = self._get_audio_duration(audio_path)
        total_minutes = total_duration_ms / 1000 / 60
        print(f"ğŸ“Š éŸ³æª”ç¸½é•·åº¦ï¼š{total_minutes:.1f} åˆ†é˜")

        # å¦‚æœéŸ³æª”ä¸é•·ï¼Œç›´æ¥è½‰éŒ„
        if total_duration_ms <= chunk_duration_ms:
            print(f"ğŸ“ éŸ³æª”é•·åº¦åœ¨ {chunk_duration_ms/1000/60:.0f} åˆ†é˜å…§ï¼Œç›´æ¥è½‰éŒ„...")
            return self.transcribe(audio_path, language)

        # é•·éŸ³æª”ï¼šåˆ†æ®µè™•ç†
        num_chunks = (total_duration_ms + chunk_duration_ms - 1) // chunk_duration_ms
        print(f"ğŸ”„ éŸ³æª”è¼ƒé•·ï¼Œå°‡åˆ†ç‚º {num_chunks} æ®µè™•ç†ï¼ˆæ¯æ®µç´„ {chunk_duration_ms/1000/60:.0f} åˆ†é˜ï¼‰...")

        # åˆ‡åˆ†éŸ³æª”
        chunk_files = self._split_audio_into_chunks(
            audio_path, total_duration_ms, chunk_duration_ms
        )

        # è½‰éŒ„æ¯å€‹ chunk
        all_text_parts = []
        all_segments = []
        detected_language = None
        chunk_duration_seconds = chunk_duration_ms / 1000.0

        for chunk_idx, chunk_path in enumerate(chunk_files, start=1):
            print(f"ğŸ™ è½‰éŒ„ç¬¬ {chunk_idx}/{num_chunks} æ®µ...")

            # é€²åº¦å›èª¿
            if progress_callback:
                progress_callback(chunk_idx, num_chunks)

            # è½‰éŒ„é€™å€‹ chunk
            chunk_text, chunk_segments, chunk_lang = self.transcribe(
                chunk_path, language
            )

            all_text_parts.append(chunk_text)

            # èª¿æ•´æ™‚é–“æˆ³ä¸¦åŠ å…¥ segments
            time_offset = (chunk_idx - 1) * chunk_duration_seconds
            for seg in chunk_segments:
                adjusted_segment = {
                    "start": seg["start"] + time_offset,
                    "end": seg["end"] + time_offset,
                    "text": seg["text"]
                }
                all_segments.append(adjusted_segment)

            # è¨˜éŒ„åµæ¸¬åˆ°çš„èªè¨€ï¼ˆä½¿ç”¨ç¬¬ä¸€æ®µçš„çµæœï¼‰
            if detected_language is None:
                detected_language = chunk_lang

            # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
            try:
                chunk_path.unlink()
            except Exception as e:
                print(f"âš ï¸ æ¸…ç† chunk æª”æ¡ˆå¤±æ•—ï¼š{e}")

        # åˆä½µæ‰€æœ‰æ–‡å­—
        full_text = " ".join(all_text_parts)

        print(f"âœ… é †åºè½‰éŒ„å®Œæˆï¼ç¸½å…± {num_chunks} æ®µï¼Œ{len(all_segments)} å€‹ segmentsï¼ˆæ™‚é–“æˆ³å·²èª¿æ•´ï¼‰")
        return full_text, all_segments, detected_language

    def transcribe_in_chunks_parallel(
        self,
        audio_path: Path,
        chunk_duration_ms: int = 420000,  # 7 åˆ†é˜
        language: Optional[str] = None,
        max_workers: int = 3,  # å„ªåŒ–ï¼šé»˜èª 3 å€‹ä¸¦è¡Œé€²ç¨‹
        progress_callback: Optional[callable] = None,
        cancel_check: Optional[callable] = None
    ) -> Tuple[str, List[Dict], str]:
        """å°‡éŸ³æª”åˆ†æ®µå¾Œä¸¦è¡Œè½‰éŒ„ï¼ˆä½¿ç”¨ ProcessPoolExecutorï¼ŒçœŸæ­£çš„å¤šé€²ç¨‹ä¸¦è¡Œï¼‰

        Args:
            audio_path: éŸ³æª”è·¯å¾‘
            chunk_duration_ms: æ¯æ®µé•·åº¦ï¼ˆæ¯«ç§’ï¼‰
            language: èªè¨€ä»£ç¢¼ï¼ˆNone è¡¨ç¤ºè‡ªå‹•åµæ¸¬ï¼‰
            max_workers: ä¸¦è¡Œå·¥ä½œé€²ç¨‹æ•¸ï¼ˆé»˜èª 3ï¼‰
            progress_callback: é€²åº¦å›èª¿å‡½æ•¸ callback(completed_count, total_chunks)
            cancel_check: å–æ¶ˆæª¢æŸ¥å‡½æ•¸ï¼Œè¿”å› True è¡¨ç¤ºä»»å‹™è¢«å–æ¶ˆ

        Returns:
            (å®Œæ•´æ–‡å­—, segments åˆ—è¡¨, åµæ¸¬åˆ°çš„èªè¨€)
        """
        print(f"ğŸš€ [ä¸¦è¡Œè½‰éŒ„] é–‹å§‹ä¸¦è¡Œè½‰éŒ„æµç¨‹ï¼ˆProcessPoolExecutorï¼‰", flush=True)

        # 1. ç²å–éŸ³æª”é•·åº¦ä¸¦åˆ†å‰²
        total_duration_ms = self._get_audio_duration(audio_path)
        total_minutes = total_duration_ms / 1000 / 60
        print(f"ğŸ“Š éŸ³æª”ç¸½é•·åº¦ï¼š{total_minutes:.1f} åˆ†é˜", flush=True)

        if total_duration_ms <= chunk_duration_ms:
            print(f"ğŸ“ éŸ³æª”é•·åº¦åœ¨ {chunk_duration_ms/1000/60:.0f} åˆ†é˜å…§ï¼Œç›´æ¥è½‰éŒ„...", flush=True)
            return self.transcribe(audio_path, language)

        num_chunks = (total_duration_ms + chunk_duration_ms - 1) // chunk_duration_ms
        print(f"ğŸ”„ éŸ³æª”è¼ƒé•·ï¼Œå°‡åˆ†ç‚º {num_chunks} æ®µä¸¦è¡Œè™•ç†...", flush=True)

        chunk_files = self._split_audio_into_chunks(audio_path, total_duration_ms, chunk_duration_ms)

        # 2. ä½¿ç”¨ ProcessPoolExecutor ä¸¦è¡Œè½‰éŒ„
        results = {}
        completed_count = 0
        executor = None

        print(f"ğŸ”§ [ä¸¦è¡Œè½‰éŒ„] å‰µå»ºé€²ç¨‹æ± ï¼Œmax_workers={max_workers}", flush=True)

        try:
            executor = ProcessPoolExecutor(max_workers=max_workers)

            # æäº¤æ‰€æœ‰ä»»å‹™
            print(f"ğŸ”§ [ä¸¦è¡Œè½‰éŒ„] æº–å‚™æäº¤ {num_chunks} å€‹ä»»å‹™...", flush=True)

            # æº–å‚™åƒæ•¸ï¼ˆå¾ç•¶å‰æ¨¡å‹å¯¦ä¾‹ç²å–é…ç½®ï¼‰
            future_to_idx = {}
            for chunk_path in chunk_files:
                future = executor.submit(
                    transcribe_chunk_worker,
                    str(chunk_path),  # è½‰ç‚ºå­—ç¬¦ä¸²ä»¥æ”¯æŒåºåˆ—åŒ–
                    self.model_name,
                    "auto",
                    "int8",
                    2,  # å„ªåŒ–å¾Œçš„ cpu_threads
                    1,  # å„ªåŒ–å¾Œçš„ num_workersï¼ˆé¿å…é€²ç¨‹å…§éåº¦ä¸¦è¡Œï¼‰
                    language
                )
                chunk_idx = int(re.search(r'chunk_(\d+)', str(chunk_path)).group(1))
                future_to_idx[future] = chunk_idx

            print(f"âœ… [ä¸¦è¡Œè½‰éŒ„] æ‰€æœ‰ä»»å‹™å·²æäº¤åˆ°é€²ç¨‹æ± ", flush=True)

            # è¨ˆç®—åˆå§‹æ­£åœ¨è™•ç†ä¸­çš„ chunk æ•¸é‡ï¼ˆworker æœƒç«‹å³æ‹¿èµ°ä»»å‹™ï¼‰
            processing_count = min(max_workers, num_chunks)
            # ç™¼é€åˆå§‹é€²åº¦ï¼ˆ0 å®Œæˆï¼Œä½†æœ‰ processing_count å€‹æ­£åœ¨è™•ç†ï¼‰
            if progress_callback:
                progress_callback(0, num_chunks, processing_count)

            # æ”¶é›†çµæœ
            for future in as_completed(future_to_idx.keys()):
                # æª¢æŸ¥å–æ¶ˆ
                if cancel_check and cancel_check():
                    print(f"âš ï¸ ç”¨æˆ¶å–æ¶ˆï¼Œçµ‚æ­¢æ‰€æœ‰ä»»å‹™", flush=True)
                    # å–æ¶ˆæ‰€æœ‰æœªå®Œæˆçš„ future
                    for f in future_to_idx.keys():
                        f.cancel()
                    # å¼·åˆ¶é—œé–‰ executorï¼ˆä¸ç­‰å¾…ï¼‰
                    print(f"ğŸ›‘ å¼·åˆ¶é—œé–‰é€²ç¨‹æ± ...", flush=True)
                    executor.shutdown(wait=False, cancel_futures=True)
                    raise Exception("ä»»å‹™è¢«å–æ¶ˆ")

                try:
                    chunk_idx, text, segments, lang = future.result()
                    results[chunk_idx] = (text, segments, lang)
                    completed_count += 1

                    # è¨ˆç®—æ­£åœ¨è™•ç†ä¸­çš„ chunk æ•¸é‡
                    # å‰©é¤˜æœªå®Œæˆçš„ chunk ä¸­ï¼Œæœ€å¤šæœ‰ max_workers å€‹æ­£åœ¨è™•ç†
                    remaining = num_chunks - completed_count
                    processing_count = min(max_workers, remaining)

                    print(f"âœ… Chunk {chunk_idx} å®Œæˆï¼ˆå·²å®Œæˆ {completed_count}/{num_chunks}ï¼Œè™•ç†ä¸­ {processing_count}ï¼‰", flush=True)

                    # æ›´æ–°é€²åº¦
                    if progress_callback:
                        progress_callback(completed_count, num_chunks, processing_count)

                except Exception as e:
                    chunk_idx = future_to_idx[future]
                    print(f"âŒ Chunk {chunk_idx} è½‰éŒ„å¤±æ•—ï¼š{e}", flush=True)
                    # ç«‹å³å¤±æ•—ï¼šå–æ¶ˆæ‰€æœ‰å‰©é¤˜ä»»å‹™
                    for f in future_to_idx.keys():
                        f.cancel()
                    # å¼·åˆ¶é—œé–‰ executor
                    print(f"ğŸ›‘ è½‰éŒ„å¤±æ•—ï¼Œå¼·åˆ¶é—œé–‰é€²ç¨‹æ± ...", flush=True)
                    executor.shutdown(wait=False, cancel_futures=True)
                    raise Exception(f"ä¸¦è¡Œè½‰éŒ„å¤±æ•—ï¼š{e}")

        except Exception as e:
            # æ¸…ç†æ‰€æœ‰å‰©é¤˜çš„ chunk æª”æ¡ˆ
            for chunk_path in chunk_files:
                try:
                    chunk_path.unlink()
                except:
                    pass
            raise

        finally:
            # ç¢ºä¿ executor è¢«æ­£ç¢ºæ¸…ç†
            if executor is not None:
                print(f"ğŸ§¹ [ä¸¦è¡Œè½‰éŒ„] æ¸…ç†é€²ç¨‹æ± ...", flush=True)
                try:
                    # å„ªé›…é—œé–‰ï¼Œæœ€å¤šç­‰å¾… 10 ç§’
                    executor.shutdown(wait=True, cancel_futures=False)
                    print(f"âœ… [ä¸¦è¡Œè½‰éŒ„] é€²ç¨‹æ± å·²é—œé–‰", flush=True)
                except Exception as cleanup_error:
                    print(f"âš ï¸ [ä¸¦è¡Œè½‰éŒ„] é€²ç¨‹æ± é—œé–‰å¤±æ•—ï¼š{cleanup_error}", flush=True)

        # 3. æª¢æŸ¥ä¸¦åˆä½µçµæœ
        if len(results) != num_chunks:
            missing = set(range(1, num_chunks + 1)) - set(results.keys())
            raise Exception(f"ä¸¦è¡Œè½‰éŒ„ä¸å®Œæ•´ï¼Œç¼ºå°‘ chunks: {missing}")

        # æŒ‰ chunk_idx æ’åº
        sorted_results = [results[idx] for idx in sorted(results.keys())]

        all_text_parts = [text for text, _, _ in sorted_results]
        all_segments = []

        # åˆä½µ segments ä¸¦èª¿æ•´æ™‚é–“æˆ³
        chunk_duration_seconds = chunk_duration_ms / 1000.0
        for chunk_idx, (text, segments, lang) in enumerate(sorted_results, start=1):
            # è¨ˆç®—æ­¤ chunk åœ¨åŸéŸ³æª”ä¸­çš„æ™‚é–“åç§»
            time_offset = (chunk_idx - 1) * chunk_duration_seconds

            # èª¿æ•´æ¯å€‹ segment çš„æ™‚é–“æˆ³
            for seg in segments:
                adjusted_segment = {
                    "start": seg["start"] + time_offset,
                    "end": seg["end"] + time_offset,
                    "text": seg["text"]
                }
                all_segments.append(adjusted_segment)

        full_text = " ".join(all_text_parts)
        detected_language = sorted_results[0][2] if sorted_results else None

        print(f"âœ… ä¸¦è¡Œè½‰éŒ„å®Œæˆï¼ç¸½å…± {num_chunks} æ®µï¼Œ{len(all_segments)} å€‹ segmentsï¼ˆæ™‚é–“æˆ³å·²èª¿æ•´ï¼‰", flush=True)
        return full_text, all_segments, detected_language

    def transcribe_with_diarization(
        self,
        audio_path: Path,
        diarization_segments: List[Dict],
        language: Optional[str] = None
    ) -> Tuple[str, List[Dict], str]:
        """è½‰éŒ„éŸ³æª”ä¸¦åˆä½µèªªè©±è€…è¾¨è­˜çµæœ

        Args:
            audio_path: éŸ³æª”è·¯å¾‘
            diarization_segments: èªªè©±è€…è¾¨è­˜çµæœ
            language: èªè¨€ä»£ç¢¼

        Returns:
            (å¸¶èªªè©±è€…æ¨™è¨˜çš„æ–‡å­—, segments åˆ—è¡¨, åµæ¸¬åˆ°çš„èªè¨€)
        """
        # å…ˆè½‰éŒ„
        _, segments_list, detected_language = self.transcribe(audio_path, language)

        # åˆä½µ diarization çµæœ
        merged_text = self._merge_transcription_with_diarization(
            segments_list, diarization_segments
        )

        return merged_text, segments_list, detected_language

    # ========== ç§æœ‰è¼”åŠ©æ–¹æ³• ==========

    def _transcribe_with_timestamps(
        self,
        audio_path: Path,
        language: Optional[str] = None
    ) -> Tuple[List[Dict], str]:
        """è½‰éŒ„éŸ³æª”ä¸¦è¿”å›å¸¶æ™‚é–“æˆ³çš„ segments

        Args:
            audio_path: éŸ³æª”è·¯å¾‘
            language: èªè¨€ä»£ç¢¼ï¼ˆNone è¡¨ç¤ºè‡ªå‹•åµæ¸¬ï¼‰

        Returns:
            (segments åˆ—è¡¨, åµæ¸¬åˆ°çš„èªè¨€)
        """
        print(f"ğŸ¯ [_transcribe_with_timestamps] é–‹å§‹ Whisper æ¨¡å‹è½‰éŒ„")
        print(f"ğŸ¯ [_transcribe_with_timestamps] éŸ³æª”è·¯å¾‘: {audio_path}")
        print(f"ğŸ¯ [_transcribe_with_timestamps] èªè¨€: {language}")

        segments_list = []
        print(f"â³ [_transcribe_with_timestamps] èª¿ç”¨ model.transcribe()...")
        segments, info = self.model.transcribe(
            str(audio_path),
            language=language,
            beam_size=5
        )
        print(f"âœ… [_transcribe_with_timestamps] model.transcribe() å®Œæˆï¼")

        # ç²å– Whisper åµæ¸¬åˆ°çš„èªè¨€
        detected_language = info.language if hasattr(info, 'language') else None

        for segment in segments:
            segments_list.append({
                "start": segment.start,
                "end": segment.end,
                "text": segment.text
            })

        return segments_list, detected_language

    def _get_audio_duration(self, audio_path: Path) -> int:
        """ç²å–éŸ³æª”ç¸½é•·åº¦ï¼ˆæ¯«ç§’ï¼‰

        å„ªå…ˆä½¿ç”¨ ffprobeï¼ˆå¿«é€Ÿï¼Œä¸è¼‰å…¥è¨˜æ†¶é«”ï¼‰ï¼Œå¤±æ•—æ™‚å›é€€åˆ° pydub

        Args:
            audio_path: éŸ³æª”è·¯å¾‘

        Returns:
            éŸ³æª”é•·åº¦ï¼ˆæ¯«ç§’ï¼‰
        """
        # ä½¿ç”¨ ffprobe ç²å–éŸ³æª”è³‡è¨Šï¼Œä¸è¼‰å…¥åˆ°è¨˜æ†¶é«”
        try:
            result = subprocess.run([
                'ffprobe', '-v', 'quiet', '-print_format', 'json',
                '-show_format', str(audio_path)
            ], capture_output=True, text=True, timeout=10)

            if result.returncode == 0:
                probe_data = json.loads(result.stdout)
                total_duration_seconds = float(probe_data['format']['duration'])
                return int(total_duration_seconds * 1000)

        except Exception as e:
            print(f"âš ï¸ ffprobe å¤±æ•—ï¼Œå›é€€åˆ° pydub: {e}")

        # å›é€€åˆ° pydub
        audio = AudioSegment.from_file(audio_path)
        duration_ms = len(audio)
        del audio  # ç«‹å³é‡‹æ”¾è¨˜æ†¶é«”
        return duration_ms

    def _split_audio_into_chunks(
        self,
        audio_path: Path,
        total_duration_ms: int,
        chunk_duration_ms: int
    ) -> List[Path]:
        """å°‡éŸ³æª”åˆ‡åˆ†ç‚ºå¤šå€‹ MP3 å°æ®µ

        ä½¿ç”¨ ffmpeg æµå¼è™•ç†ï¼Œé¿å…è¨˜æ†¶é«”å•é¡Œ

        Args:
            audio_path: åŸå§‹éŸ³æª”è·¯å¾‘ï¼ˆMP3ï¼‰
            total_duration_ms: éŸ³æª”ç¸½é•·åº¦ï¼ˆæ¯«ç§’ï¼‰
            chunk_duration_ms: æ¯æ®µé•·åº¦ï¼ˆæ¯«ç§’ï¼‰

        Returns:
            chunk æª”æ¡ˆè·¯å¾‘åˆ—è¡¨ï¼ˆMP3 æ ¼å¼ï¼‰
        """
        chunk_files = []
        start_ms = 0
        chunk_idx = 1

        while start_ms < total_duration_ms:
            end_ms = min(start_ms + chunk_duration_ms, total_duration_ms)

            print(f"   æº–å‚™ç¬¬ {chunk_idx} æ®µ ({start_ms/1000/60:.1f}-{end_ms/1000/60:.1f} åˆ†é˜)...")

            # ä½¿ç”¨ ffmpeg ç›´æ¥åˆ‡åˆ†ç‚º MP3ï¼Œä¸è¼‰å…¥åˆ°è¨˜æ†¶é«”
            temp_path = audio_path.parent / f"_temp_{audio_path.stem}_chunk_{chunk_idx}.mp3"
            start_seconds = start_ms / 1000.0
            duration_seconds = (end_ms - start_ms) / 1000.0

            try:
                # ä½¿ç”¨ ffmpeg åˆ‡åˆ†éŸ³æª”ç‚º MP3ï¼ˆæµå¼è™•ç†ï¼Œä¸ä½”ç”¨è¨˜æ†¶é«”ï¼‰
                subprocess.run([
                    'ffmpeg', '-y', '-i', str(audio_path),
                    '-ss', str(start_seconds),
                    '-t', str(duration_seconds),
                    '-acodec', 'libmp3lame',  # MP3 ç·¨ç¢¼
                    '-b:a', '128k',  # 128kbps
                    '-ar', '16000',  # 16kHz æ¡æ¨£ç‡ï¼ˆWhisper æ¨è–¦ï¼‰
                    '-ac', '1',  # å–®è²é“
                    str(temp_path)
                ], check=True, capture_output=True, timeout=60)

            except subprocess.TimeoutExpired:
                print(f"   âš ï¸ åˆ‡åˆ†ç¬¬ {chunk_idx} æ®µè¶…æ™‚ï¼Œå˜—è©¦ä½¿ç”¨ pydub")
                # å›é€€åˆ° pydubï¼ˆè¼ƒæ…¢ä½†æ›´ç©©å®šï¼‰
                audio = AudioSegment.from_file(audio_path)
                chunk_audio = audio[start_ms:end_ms]
                chunk_audio.export(temp_path, format="mp3", bitrate="128k")
                del audio, chunk_audio  # ç«‹å³é‡‹æ”¾

            except Exception as e:
                print(f"   âš ï¸ ffmpeg åˆ‡åˆ†å¤±æ•—ï¼Œå›é€€åˆ° pydub: {e}")
                # å›é€€åˆ° pydub
                audio = AudioSegment.from_file(audio_path)
                chunk_audio = audio[start_ms:end_ms]
                chunk_audio.export(temp_path, format="mp3", bitrate="128k")
                del audio, chunk_audio  # ç«‹å³é‡‹æ”¾

            chunk_files.append(temp_path)
            start_ms = end_ms
            chunk_idx += 1

        return chunk_files

    def _merge_transcription_with_diarization(
        self,
        transcription_segments: List[Dict],
        diarization_segments: List[Dict]
    ) -> str:
        """åˆä½µè½‰éŒ„æ–‡å­—å’Œèªªè©±è€…æ¨™è¨˜

        Args:
            transcription_segments: Whisper è½‰éŒ„çµæœ (å¸¶æ™‚é–“æˆ³)
            diarization_segments: Speaker diarization çµæœ

        Returns:
            åˆä½µå¾Œçš„æ–‡å­—ï¼Œæ ¼å¼ï¼š[Speaker A] æ–‡å­—å…§å®¹
        """
        if not diarization_segments:
            # æ²’æœ‰ diarization çµæœï¼Œç›´æ¥è¿”å›ç´”æ–‡å­—
            return " ".join(seg.get("text", "") for seg in transcription_segments)

        # ç‚ºæ¯å€‹è½‰éŒ„ç‰‡æ®µåˆ†é…èªªè©±è€…
        result_lines = []
        current_speaker = None
        current_text = []

        for trans_seg in transcription_segments:
            trans_start = trans_seg.get("start", 0)
            trans_end = trans_seg.get("end", 0)
            trans_text = trans_seg.get("text", "")

            if not trans_text.strip():
                continue

            # æ‰¾åˆ°èˆ‡æ­¤è½‰éŒ„ç‰‡æ®µé‡ç–Šæœ€å¤šçš„èªªè©±è€…
            best_speaker = None
            max_overlap = 0

            for dia_seg in diarization_segments:
                dia_start = dia_seg["start"]
                dia_end = dia_seg["end"]

                # è¨ˆç®—é‡ç–Šæ™‚é–“
                overlap_start = max(trans_start, dia_start)
                overlap_end = min(trans_end, dia_end)
                overlap = max(0, overlap_end - overlap_start)

                if overlap > max_overlap:
                    max_overlap = overlap
                    best_speaker = dia_seg["speaker"]

            # å¦‚æœèªªè©±è€…æ”¹è®Šï¼Œè¼¸å‡ºä¹‹å‰çš„å…§å®¹
            if best_speaker != current_speaker and current_text:
                speaker_label = f"[{current_speaker}]" if current_speaker else ""
                result_lines.append(f"{speaker_label} {''.join(current_text)}")
                current_text = []

            current_speaker = best_speaker
            current_text.append(trans_text)

        # è¼¸å‡ºæœ€å¾Œä¸€æ®µ
        if current_text:
            speaker_label = f"[{current_speaker}]" if current_speaker else ""
            result_lines.append(f"{speaker_label} {''.join(current_text)}")

        return "\n\n".join(result_lines)

    def _merge_speaker_to_segments(
        self,
        transcription_segments: List[Dict],
        diarization_segments: List[Dict]
    ) -> List[Dict]:
        """å°‡èªªè©±è€…è³‡è¨Šæ•´åˆåˆ°è½‰éŒ„ segments ä¸­

        ç”¨æ–¼å­—å¹•æ¨¡å¼ï¼šä¸æ”¹è®Šæ–‡å­—å…§å®¹ï¼Œè€Œæ˜¯åœ¨ segments ä¸­æ·»åŠ  speaker æ¬„ä½

        Args:
            transcription_segments: Whisper è½‰éŒ„çµæœ (å¸¶æ™‚é–“æˆ³)
            diarization_segments: Speaker diarization çµæœ

        Returns:
            å¸¶ speaker æ¬„ä½çš„ segments åˆ—è¡¨
        """
        if not diarization_segments:
            # æ²’æœ‰ diarization çµæœï¼Œè¿”å›åŸ segments
            return transcription_segments

        enriched_segments = []

        for trans_seg in transcription_segments:
            trans_start = trans_seg.get("start", 0)
            trans_end = trans_seg.get("end", 0)

            # æ‰¾åˆ°èˆ‡æ­¤è½‰éŒ„ç‰‡æ®µé‡ç–Šæœ€å¤šçš„èªªè©±è€…
            best_speaker = None
            max_overlap = 0

            for dia_seg in diarization_segments:
                dia_start = dia_seg["start"]
                dia_end = dia_seg["end"]

                # è¨ˆç®—é‡ç–Šæ™‚é–“
                overlap_start = max(trans_start, dia_start)
                overlap_end = min(trans_end, dia_end)
                overlap = max(0, overlap_end - overlap_start)

                if overlap > max_overlap:
                    max_overlap = overlap
                    best_speaker = dia_seg["speaker"]

            # è¤‡è£½ segment ä¸¦æ·»åŠ  speaker æ¬„ä½
            enriched_seg = trans_seg.copy()
            enriched_seg["speaker"] = best_speaker if best_speaker else "UNKNOWN"
            enriched_segments.append(enriched_seg)

        return enriched_segments
