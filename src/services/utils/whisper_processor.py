"""
WhisperProcessor - Whisper è½‰éŒ„è™•ç†å™¨
è·è²¬ï¼šWhisper æ¨¡å‹çš„å°è£ï¼ˆç„¡ç‹€æ…‹å·¥å…·é¡ï¼‰
"""

from pathlib import Path
from typing import Optional, Tuple, List, Dict, Any
import subprocess
import json
from pydub import AudioSegment
from faster_whisper import WhisperModel


class WhisperProcessor:
    """Whisper è½‰éŒ„è™•ç†å™¨

    å°è£ Whisper æ¨¡å‹çš„è½‰éŒ„åŠŸèƒ½ï¼Œæä¾›ç„¡ç‹€æ…‹çš„è½‰éŒ„æ–¹æ³•
    """

    def __init__(self, model: WhisperModel):
        """åˆå§‹åŒ– WhisperProcessor

        Args:
            model: Whisper æ¨¡å‹å¯¦ä¾‹
        """
        self.model = model

    def transcribe(
        self,
        audio_path: Path,
        language: Optional[str] = None
    ) -> Tuple[str, List[Dict], str]:
        """è½‰éŒ„éŸ³æª”ï¼ˆå–®æ¬¡è½‰éŒ„ï¼Œä¸åˆ†æ®µï¼‰

        Args:
            audio_path: éŸ³æª”è·¯å¾‘
            language: èªè¨€ä»£ç¢¼ï¼ˆNone è¡¨ç¤ºè‡ªå‹•åµæ¸¬ï¼‰

        Returns:
            (å®Œæ•´æ–‡å­—, segments åˆ—è¡¨, åµæ¸¬åˆ°çš„èªè¨€)
        """
        segments_list, detected_language = self._transcribe_with_timestamps(
            audio_path, language
        )

        # åˆä½µæ‰€æœ‰ segment çš„æ–‡å­—
        full_text = " ".join(seg["text"] for seg in segments_list)

        return full_text, segments_list, detected_language

    def transcribe_in_chunks(
        self,
        audio_path: Path,
        chunk_duration_ms: int = 600000,  # 10 åˆ†é˜
        language: Optional[str] = None,
        progress_callback: Optional[callable] = None
    ) -> Tuple[str, List[Dict], str]:
        """å°‡éŸ³æª”åˆ†æ®µå¾Œè½‰éŒ„ï¼ˆæé«˜é•·éŸ³æª”çš„æº–ç¢ºåº¦ï¼‰

        Args:
            audio_path: éŸ³æª”è·¯å¾‘
            chunk_duration_ms: æ¯æ®µé•·åº¦ï¼ˆæ¯«ç§’ï¼‰
            language: èªè¨€ä»£ç¢¼ï¼ˆNone è¡¨ç¤ºè‡ªå‹•åµæ¸¬ï¼‰
            progress_callback: é€²åº¦å›èª¿å‡½æ•¸ callback(chunk_idx, total_chunks)

        Returns:
            (å®Œæ•´æ–‡å­—, segments åˆ—è¡¨, åµæ¸¬åˆ°çš„èªè¨€)
        """
        # ç²å–éŸ³æª”ç¸½é•·åº¦
        total_duration_ms = self._get_audio_duration(audio_path)
        total_minutes = total_duration_ms / 1000 / 60
        print(f"ğŸ“Š éŸ³æª”ç¸½é•·åº¦ï¼š{total_minutes:.1f} åˆ†é˜")

        # å¦‚æœéŸ³æª”ä¸é•·ï¼Œç›´æ¥è½‰éŒ„
        if total_duration_ms <= chunk_duration_ms:
            print(f"ğŸ“ éŸ³æª”é•·åº¦åœ¨ {chunk_duration_ms/1000/60:.0f} åˆ†é˜å…§ï¼Œç›´æ¥è½‰éŒ„...")
            return self.transcribe(audio_path, language)

        # é•·éŸ³æª”ï¼šåˆ†æ®µè™•ç†
        num_chunks = (total_duration_ms + chunk_duration_ms - 1) // chunk_duration_ms
        print(f"ğŸ”„ éŸ³æª”è¼ƒé•·ï¼Œå°‡åˆ†ç‚º {num_chunks} æ®µè™•ç†ï¼ˆæ¯æ®µç´„ {chunk_duration_ms/1000/60:.0f} åˆ†é˜ï¼‰...")

        # åˆ‡åˆ†éŸ³æª”
        chunk_files = self._split_audio_into_chunks(
            audio_path, total_duration_ms, chunk_duration_ms
        )

        # è½‰éŒ„æ¯å€‹ chunk
        all_text_parts = []
        all_segments = []
        detected_language = None

        for chunk_idx, chunk_path in enumerate(chunk_files, start=1):
            print(f"ğŸ™ è½‰éŒ„ç¬¬ {chunk_idx}/{num_chunks} æ®µ...")

            # é€²åº¦å›èª¿
            if progress_callback:
                progress_callback(chunk_idx, num_chunks)

            # è½‰éŒ„é€™å€‹ chunk
            chunk_text, chunk_segments, chunk_lang = self.transcribe(
                chunk_path, language
            )

            all_text_parts.append(chunk_text)
            all_segments.extend(chunk_segments)

            # è¨˜éŒ„åµæ¸¬åˆ°çš„èªè¨€ï¼ˆä½¿ç”¨ç¬¬ä¸€æ®µçš„çµæœï¼‰
            if detected_language is None:
                detected_language = chunk_lang

            # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
            try:
                chunk_path.unlink()
            except Exception as e:
                print(f"âš ï¸ æ¸…ç† chunk æª”æ¡ˆå¤±æ•—ï¼š{e}")

        # åˆä½µæ‰€æœ‰æ–‡å­—
        full_text = " ".join(all_text_parts)

        return full_text, all_segments, detected_language

    def transcribe_with_diarization(
        self,
        audio_path: Path,
        diarization_segments: List[Dict],
        language: Optional[str] = None
    ) -> Tuple[str, List[Dict], str]:
        """è½‰éŒ„éŸ³æª”ä¸¦åˆä½µèªªè©±è€…è¾¨è­˜çµæœ

        Args:
            audio_path: éŸ³æª”è·¯å¾‘
            diarization_segments: èªªè©±è€…è¾¨è­˜çµæœ
            language: èªè¨€ä»£ç¢¼

        Returns:
            (å¸¶èªªè©±è€…æ¨™è¨˜çš„æ–‡å­—, segments åˆ—è¡¨, åµæ¸¬åˆ°çš„èªè¨€)
        """
        # å…ˆè½‰éŒ„
        _, segments_list, detected_language = self.transcribe(audio_path, language)

        # åˆä½µ diarization çµæœ
        merged_text = self._merge_transcription_with_diarization(
            segments_list, diarization_segments
        )

        return merged_text, segments_list, detected_language

    # ========== ç§æœ‰è¼”åŠ©æ–¹æ³• ==========

    def _transcribe_with_timestamps(
        self,
        audio_path: Path,
        language: Optional[str] = None
    ) -> Tuple[List[Dict], str]:
        """è½‰éŒ„éŸ³æª”ä¸¦è¿”å›å¸¶æ™‚é–“æˆ³çš„ segments

        Args:
            audio_path: éŸ³æª”è·¯å¾‘
            language: èªè¨€ä»£ç¢¼ï¼ˆNone è¡¨ç¤ºè‡ªå‹•åµæ¸¬ï¼‰

        Returns:
            (segments åˆ—è¡¨, åµæ¸¬åˆ°çš„èªè¨€)
        """
        segments_list = []
        segments, info = self.model.transcribe(
            str(audio_path),
            language=language,
            beam_size=5
        )

        # ç²å– Whisper åµæ¸¬åˆ°çš„èªè¨€
        detected_language = info.language if hasattr(info, 'language') else None

        for segment in segments:
            segments_list.append({
                "start": segment.start,
                "end": segment.end,
                "text": segment.text
            })

        return segments_list, detected_language

    def _get_audio_duration(self, audio_path: Path) -> int:
        """ç²å–éŸ³æª”ç¸½é•·åº¦ï¼ˆæ¯«ç§’ï¼‰

        å„ªå…ˆä½¿ç”¨ ffprobeï¼ˆå¿«é€Ÿï¼Œä¸è¼‰å…¥è¨˜æ†¶é«”ï¼‰ï¼Œå¤±æ•—æ™‚å›é€€åˆ° pydub

        Args:
            audio_path: éŸ³æª”è·¯å¾‘

        Returns:
            éŸ³æª”é•·åº¦ï¼ˆæ¯«ç§’ï¼‰
        """
        # ä½¿ç”¨ ffprobe ç²å–éŸ³æª”è³‡è¨Šï¼Œä¸è¼‰å…¥åˆ°è¨˜æ†¶é«”
        try:
            result = subprocess.run([
                'ffprobe', '-v', 'quiet', '-print_format', 'json',
                '-show_format', str(audio_path)
            ], capture_output=True, text=True, timeout=10)

            if result.returncode == 0:
                probe_data = json.loads(result.stdout)
                total_duration_seconds = float(probe_data['format']['duration'])
                return int(total_duration_seconds * 1000)

        except Exception as e:
            print(f"âš ï¸ ffprobe å¤±æ•—ï¼Œå›é€€åˆ° pydub: {e}")

        # å›é€€åˆ° pydub
        audio = AudioSegment.from_file(audio_path)
        duration_ms = len(audio)
        del audio  # ç«‹å³é‡‹æ”¾è¨˜æ†¶é«”
        return duration_ms

    def _split_audio_into_chunks(
        self,
        audio_path: Path,
        total_duration_ms: int,
        chunk_duration_ms: int
    ) -> List[Path]:
        """å°‡éŸ³æª”åˆ‡åˆ†ç‚ºå¤šå€‹å°æ®µ

        ä½¿ç”¨ ffmpeg æµå¼è™•ç†ï¼Œé¿å…è¨˜æ†¶é«”å•é¡Œ

        Args:
            audio_path: åŸå§‹éŸ³æª”è·¯å¾‘
            total_duration_ms: éŸ³æª”ç¸½é•·åº¦ï¼ˆæ¯«ç§’ï¼‰
            chunk_duration_ms: æ¯æ®µé•·åº¦ï¼ˆæ¯«ç§’ï¼‰

        Returns:
            chunk æª”æ¡ˆè·¯å¾‘åˆ—è¡¨
        """
        chunk_files = []
        start_ms = 0
        chunk_idx = 1

        while start_ms < total_duration_ms:
            end_ms = min(start_ms + chunk_duration_ms, total_duration_ms)

            print(f"   æº–å‚™ç¬¬ {chunk_idx} æ®µ ({start_ms/1000/60:.1f}-{end_ms/1000/60:.1f} åˆ†é˜)...")

            # ä½¿ç”¨ ffmpeg ç›´æ¥åˆ‡åˆ†ï¼Œä¸è¼‰å…¥åˆ°è¨˜æ†¶é«”
            temp_path = audio_path.parent / f"_temp_{audio_path.stem}_chunk_{chunk_idx}.wav"
            start_seconds = start_ms / 1000.0
            duration_seconds = (end_ms - start_ms) / 1000.0

            try:
                # ä½¿ç”¨ ffmpeg åˆ‡åˆ†éŸ³æª”ï¼ˆæµå¼è™•ç†ï¼Œä¸ä½”ç”¨è¨˜æ†¶é«”ï¼‰
                subprocess.run([
                    'ffmpeg', '-y', '-i', str(audio_path),
                    '-ss', str(start_seconds),
                    '-t', str(duration_seconds),
                    '-acodec', 'pcm_s16le',  # WAV æ ¼å¼
                    '-ar', '16000',  # 16kHz æ¡æ¨£ç‡ï¼ˆWhisper æ¨è–¦ï¼‰
                    '-ac', '1',  # å–®è²é“
                    str(temp_path)
                ], check=True, capture_output=True, timeout=60)

            except subprocess.TimeoutExpired:
                print(f"   âš ï¸ åˆ‡åˆ†ç¬¬ {chunk_idx} æ®µè¶…æ™‚ï¼Œå˜—è©¦ä½¿ç”¨ pydub")
                # å›é€€åˆ° pydubï¼ˆè¼ƒæ…¢ä½†æ›´ç©©å®šï¼‰
                audio = AudioSegment.from_file(audio_path)
                chunk_audio = audio[start_ms:end_ms]
                chunk_audio.export(temp_path, format="wav")
                del audio, chunk_audio  # ç«‹å³é‡‹æ”¾

            except Exception as e:
                print(f"   âš ï¸ ffmpeg åˆ‡åˆ†å¤±æ•—ï¼Œå›é€€åˆ° pydub: {e}")
                # å›é€€åˆ° pydub
                audio = AudioSegment.from_file(audio_path)
                chunk_audio = audio[start_ms:end_ms]
                chunk_audio.export(temp_path, format="wav")
                del audio, chunk_audio  # ç«‹å³é‡‹æ”¾

            chunk_files.append(temp_path)
            start_ms = end_ms
            chunk_idx += 1

        return chunk_files

    def _merge_transcription_with_diarization(
        self,
        transcription_segments: List[Dict],
        diarization_segments: List[Dict]
    ) -> str:
        """åˆä½µè½‰éŒ„æ–‡å­—å’Œèªªè©±è€…æ¨™è¨˜

        Args:
            transcription_segments: Whisper è½‰éŒ„çµæœ (å¸¶æ™‚é–“æˆ³)
            diarization_segments: Speaker diarization çµæœ

        Returns:
            åˆä½µå¾Œçš„æ–‡å­—ï¼Œæ ¼å¼ï¼š[Speaker A] æ–‡å­—å…§å®¹
        """
        if not diarization_segments:
            # æ²’æœ‰ diarization çµæœï¼Œç›´æ¥è¿”å›ç´”æ–‡å­—
            return " ".join(seg.get("text", "") for seg in transcription_segments)

        # ç‚ºæ¯å€‹è½‰éŒ„ç‰‡æ®µåˆ†é…èªªè©±è€…
        result_lines = []
        current_speaker = None
        current_text = []

        for trans_seg in transcription_segments:
            trans_start = trans_seg.get("start", 0)
            trans_end = trans_seg.get("end", 0)
            trans_text = trans_seg.get("text", "")

            if not trans_text.strip():
                continue

            # æ‰¾åˆ°èˆ‡æ­¤è½‰éŒ„ç‰‡æ®µé‡ç–Šæœ€å¤šçš„èªªè©±è€…
            best_speaker = None
            max_overlap = 0

            for dia_seg in diarization_segments:
                dia_start = dia_seg["start"]
                dia_end = dia_seg["end"]

                # è¨ˆç®—é‡ç–Šæ™‚é–“
                overlap_start = max(trans_start, dia_start)
                overlap_end = min(trans_end, dia_end)
                overlap = max(0, overlap_end - overlap_start)

                if overlap > max_overlap:
                    max_overlap = overlap
                    best_speaker = dia_seg["speaker"]

            # å¦‚æœèªªè©±è€…æ”¹è®Šï¼Œè¼¸å‡ºä¹‹å‰çš„å…§å®¹
            if best_speaker != current_speaker and current_text:
                speaker_label = f"[{current_speaker}]" if current_speaker else ""
                result_lines.append(f"{speaker_label} {''.join(current_text)}")
                current_text = []

            current_speaker = best_speaker
            current_text.append(trans_text)

        # è¼¸å‡ºæœ€å¾Œä¸€æ®µ
        if current_text:
            speaker_label = f"[{current_speaker}]" if current_speaker else ""
            result_lines.append(f"{speaker_label} {''.join(current_text)}")

        return "\n\n".join(result_lines)
