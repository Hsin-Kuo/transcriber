"""
SummaryService - AI æ‘˜è¦æœå‹™
è·è²¬ï¼šä½¿ç”¨ Gemini API ç”Ÿæˆé€å­—ç¨¿æ‘˜è¦
"""

import os
import json
import re
from typing import Optional, Dict, Any, Tuple, List

from motor.motor_asyncio import AsyncIOMotorDatabase

from ..database.repositories.summary_repo import SummaryRepository
from ..database.repositories.task_repo import TaskRepository


class SummaryService:
    """AI æ‘˜è¦æœå‹™"""

    def __init__(
        self,
        db: AsyncIOMotorDatabase,
        default_model: str = "gemini-2.5-flash"
    ):
        """åˆå§‹åŒ– SummaryService

        Args:
            db: MongoDB è³‡æ–™åº«å¯¦ä¾‹
            default_model: é è¨­ Gemini æ¨¡åž‹
        """
        self.db = db
        self.summary_repo = SummaryRepository(db)
        self.task_repo = TaskRepository(db)
        self.default_model = default_model

        # Gemini å‚™æ´æ¨¡åž‹åˆ—è¡¨ï¼ˆæŒ‰å„ªå…ˆé †åºï¼‰
        self.fallback_models = [
            "gemini-2.5-flash-lite",
            "gemini-flash-latest",
            "gemini-flash-lite-latest",
        ]

    async def generate_summary(
        self,
        task_id: str,
        user_id: str
    ) -> Dict[str, Any]:
        """ç”Ÿæˆ AI æ‘˜è¦

        Args:
            task_id: ä»»å‹™ ID
            user_id: ä½¿ç”¨è€… IDï¼ˆç”¨æ–¼æ¬Šé™é©—è­‰ï¼‰

        Returns:
            ç”Ÿæˆçµæžœ {task_id, status, summary?, error?}
        """
        try:
            # 1. é©—è­‰ä»»å‹™å­˜åœ¨ä¸”å±¬æ–¼è©²ç”¨æˆ¶
            task = await self.task_repo.get_by_id(task_id)
            if not task:
                return {
                    "task_id": task_id,
                    "status": "failed",
                    "error": "æ‰¾ä¸åˆ°è©²ä»»å‹™"
                }

            if task.get("user", {}).get("user_id") != user_id:
                return {
                    "task_id": task_id,
                    "status": "failed",
                    "error": "ç„¡æ¬Šå­˜å–æ­¤ä»»å‹™"
                }

            # 2. ç²å–é€å­—ç¨¿å…§å®¹
            content = await self._get_transcript_content(task_id)
            if not content:
                return {
                    "task_id": task_id,
                    "status": "failed",
                    "error": "ç„¡æ³•ç²å–é€å­—ç¨¿å…§å®¹"
                }

            # 3. æ›´æ–°ä»»å‹™ç‹€æ…‹ç‚ºè™•ç†ä¸­
            await self.task_repo.update(task_id, {"summary_status": "processing"})

            # 4. åµæ¸¬èªžè¨€
            language = self._detect_language(content)

            # 5. èª¿ç”¨ Gemini API ç”Ÿæˆæ‘˜è¦
            summary_data, model_used, token_usage = await self._generate_with_gemini(content, language)

            if not summary_data:
                await self.task_repo.update(task_id, {"summary_status": "failed"})
                return {
                    "task_id": task_id,
                    "status": "failed",
                    "error": "AI ç”Ÿæˆæ‘˜è¦å¤±æ•—"
                }

            # 6. å„²å­˜æ‘˜è¦åˆ°è³‡æ–™åº«
            metadata = {
                "model": model_used,
                "language": language,
                "source_length": len(content)
            }

            # å¦‚æžœæœ‰ token ä½¿ç”¨é‡ï¼ŒåŠ å…¥ metadata
            if token_usage:
                metadata["token_usage"] = {
                    "total": token_usage.get("total", 0),
                    "prompt": token_usage.get("prompt", 0),
                    "completion": token_usage.get("completion", 0)
                }
                print(f"ðŸ“Š ä¿å­˜æ‘˜è¦ Token ä½¿ç”¨é‡: {token_usage}")

            doc = await self.summary_repo.upsert(task_id, summary_data, metadata)

            # 7. æ›´æ–°ä»»å‹™ç‹€æ…‹ç‚ºå®Œæˆ
            await self.task_repo.update(task_id, {
                "summary_status": "completed",
                "summary_id": task_id
            })

            # 8. è¿”å›žçµæžœ
            return {
                "task_id": task_id,
                "status": "completed",
                "summary": {
                    "task_id": task_id,
                    "content": doc["content"],
                    "metadata": doc["metadata"],
                    "created_at": doc["created_at"],
                    "updated_at": doc["updated_at"]
                }
            }

        except Exception as e:
            print(f"âŒ ç”Ÿæˆæ‘˜è¦å¤±æ•—: {e}")
            # æ›´æ–°ä»»å‹™ç‹€æ…‹ç‚ºå¤±æ•—
            try:
                await self.task_repo.update(task_id, {"summary_status": "failed"})
            except Exception:
                pass
            return {
                "task_id": task_id,
                "status": "failed",
                "error": str(e)
            }

    async def get_summary(
        self,
        task_id: str,
        user_id: str
    ) -> Optional[Dict[str, Any]]:
        """ç²å–æ‘˜è¦

        Args:
            task_id: ä»»å‹™ ID
            user_id: ä½¿ç”¨è€… ID

        Returns:
            æ‘˜è¦è³‡æ–™ï¼Œä¸å­˜åœ¨å‰‡è¿”å›ž None
        """
        # é©—è­‰æ¬Šé™
        task = await self.task_repo.get_by_id(task_id)
        if not task or task.get("user", {}).get("user_id") != user_id:
            return None

        doc = await self.summary_repo.get_by_task_id(task_id)
        if not doc:
            return None

        return {
            "task_id": task_id,
            "content": doc["content"],
            "metadata": doc["metadata"],
            "created_at": doc["created_at"],
            "updated_at": doc["updated_at"]
        }

    async def delete_summary(
        self,
        task_id: str,
        user_id: str
    ) -> bool:
        """åˆªé™¤æ‘˜è¦

        Args:
            task_id: ä»»å‹™ ID
            user_id: ä½¿ç”¨è€… ID

        Returns:
            æ˜¯å¦åˆªé™¤æˆåŠŸ
        """
        # é©—è­‰æ¬Šé™
        task = await self.task_repo.get_by_id(task_id)
        if not task or task.get("user", {}).get("user_id") != user_id:
            return False

        # åˆªé™¤æ‘˜è¦
        success = await self.summary_repo.delete(task_id)

        # æ›´æ–°ä»»å‹™ç‹€æ…‹
        if success:
            await self.task_repo.update(task_id, {
                "summary_status": None,
                "summary_id": None
            })

        return success

    async def _get_transcript_content(self, task_id: str) -> Optional[str]:
        """ç²å–é€å­—ç¨¿å…§å®¹

        Args:
            task_id: ä»»å‹™ ID

        Returns:
            é€å­—ç¨¿æ–‡å­—å…§å®¹
        """
        # å¾ž transcripts collection ç²å–
        transcript = await self.db.transcripts.find_one({"_id": task_id})
        if transcript and transcript.get("content"):
            return transcript["content"]

        # å¦‚æžœæ²’æœ‰ï¼Œå˜—è©¦å¾ž segments çµ„åˆ
        segments_doc = await self.db.segments.find_one({"_id": task_id})
        if segments_doc and segments_doc.get("segments"):
            texts = [seg.get("text", "") for seg in segments_doc["segments"]]
            return " ".join(texts)

        return None

    def _detect_language(self, text: str) -> str:
        """åµæ¸¬æ–‡å­—èªžè¨€

        Args:
            text: æ–‡å­—å…§å®¹

        Returns:
            èªžè¨€ä»£ç¢¼ (zh/en/ja/ko ç­‰)
        """
        # ç°¡å–®çš„èªžè¨€åµæ¸¬ï¼šæª¢æŸ¥ä¸­æ—¥éŸ“å­—ç¬¦æ¯”ä¾‹
        sample = text[:1000]  # åªæª¢æŸ¥å‰ 1000 å­—

        # ä¸­æ–‡å­—ç¬¦
        chinese_count = len(re.findall(r'[\u4e00-\u9fff]', sample))
        # æ—¥æ–‡å‡å
        japanese_count = len(re.findall(r'[\u3040-\u309f\u30a0-\u30ff]', sample))
        # éŸ“æ–‡
        korean_count = len(re.findall(r'[\uac00-\ud7af]', sample))

        total_cjk = chinese_count + japanese_count + korean_count

        if total_cjk > len(sample) * 0.1:  # CJK å­—ç¬¦è¶…éŽ 10%
            if japanese_count > chinese_count * 0.3:
                return "ja"
            elif korean_count > chinese_count * 0.3:
                return "ko"
            else:
                return "zh"

        return "en"

    async def _generate_with_gemini(
        self,
        text: str,
        language: str
    ) -> Tuple[Optional[Dict[str, Any]], str, Optional[Dict[str, int]]]:
        """ä½¿ç”¨ Gemini API ç”Ÿæˆæ‘˜è¦

        Args:
            text: é€å­—ç¨¿æ–‡å­—
            language: èªžè¨€ä»£ç¢¼

        Returns:
            (æ‘˜è¦å…§å®¹, ä½¿ç”¨çš„æ¨¡åž‹åç¨±, token_usage) å…ƒçµ„
        """
        import google.generativeai as genai

        # ç²å– API Keys
        api_keys = self._load_google_api_keys()
        if not api_keys:
            raise ValueError("æœªè¨­å®šä»»ä½• GOOGLE_API_KEY")

        # ç”Ÿæˆ prompt
        prompt = self._get_summary_prompt(language, text)

        # å˜—è©¦èª¿ç”¨ API
        current_model = self.default_model
        fallback_index = -1
        tried_models = [self.default_model]
        quota_exceeded_count = 0
        last_error = None

        max_attempts = len(api_keys) * (len(self.fallback_models) + 1)

        for attempt in range(max_attempts):
            try:
                # è¼ªè©¢ API Key
                api_key = api_keys[attempt % len(api_keys)]
                genai.configure(api_key=api_key)

                model = genai.GenerativeModel(current_model)

                # èª¿ç”¨ API
                resp = model.generate_content(
                    [{"role": "user", "parts": [prompt]}],
                    generation_config={"temperature": 0.3}
                )

                result_text = (resp.text or "").strip()

                # è§£æž JSON å›žæ‡‰
                summary_data = self._parse_summary_response(result_text)

                if summary_data:
                    if fallback_index >= 0:
                        print(f"âœ… ä½¿ç”¨å‚™æ´æ¨¡åž‹ {current_model} æˆåŠŸç”Ÿæˆæ‘˜è¦")

                    # æå– token ä½¿ç”¨é‡
                    token_usage = None
                    if hasattr(resp, 'usage_metadata') and resp.usage_metadata:
                        total = getattr(resp.usage_metadata, 'total_token_count', 0)
                        prompt_tokens = getattr(resp.usage_metadata, 'prompt_token_count', 0)
                        completion = getattr(resp.usage_metadata, 'candidates_token_count', 0)
                        token_usage = {
                            "total": total,
                            "prompt": prompt_tokens,
                            "completion": completion
                        }
                        print(f"ðŸ“Š Token ä½¿ç”¨: {total} (è¼¸å…¥: {prompt_tokens}, è¼¸å‡º: {completion})")

                    return summary_data, current_model, token_usage

            except Exception as e:
                last_error = e
                error_msg = str(e)

                # æª¢æŸ¥æ˜¯å¦ç‚ºé…é¡éŒ¯èª¤
                is_quota_error = (
                    "429" in error_msg or
                    "quota" in error_msg.lower() or
                    "Quota exceeded" in error_msg
                )

                if is_quota_error:
                    quota_exceeded_count += 1
                    print(f"âš ï¸ Google API Key é…é¡å·²ç”¨å®Œ (å˜—è©¦ {attempt + 1}ï¼Œæ¨¡åž‹: {current_model})")

                    # å¦‚æžœæ‰€æœ‰ keys éƒ½é…é¡è€—ç›¡ï¼Œå˜—è©¦åˆ‡æ›æ¨¡åž‹
                    if quota_exceeded_count >= len(api_keys):
                        fallback_index += 1

                        if fallback_index < len(self.fallback_models):
                            current_model = self.fallback_models[fallback_index]
                            print(f"ðŸ’¡ åˆ‡æ›åˆ°å‚™ç”¨æ¨¡åž‹ {current_model}")
                            tried_models.append(current_model)
                            quota_exceeded_count = 0
                            continue
                        else:
                            print(f"âŒ æ‰€æœ‰æ¨¡åž‹çš„é…é¡éƒ½å·²ç”¨å®Œ")
                            break
                else:
                    print(f"âš ï¸ API èª¿ç”¨å¤±æ•— (å˜—è©¦ {attempt + 1}): {error_msg}")

                if attempt < max_attempts - 1:
                    continue

        print(f"âŒ ç„¡æ³•ç”Ÿæˆæ‘˜è¦ã€‚å·²å˜—è©¦æ¨¡åž‹: {', '.join(tried_models)}")
        return None, "", None

    def _get_summary_prompt(self, language: str, text: str) -> str:
        """ç”Ÿæˆæ‘˜è¦çš„ prompt

        Args:
            language: èªžè¨€ä»£ç¢¼
            text: é€å­—ç¨¿æ–‡å­—

        Returns:
            å®Œæ•´çš„ prompt
        """
        # é™åˆ¶æ–‡å­—é•·åº¦é¿å…è¶…å‡º token é™åˆ¶
        max_length = 30000
        if len(text) > max_length:
            text = text[:max_length] + "..."

        if language == "zh":
            return f"""ã€è§’è‰²è¨­å®šã€‘
ä½ æ˜¯ä¸€å€‹è‡ªç„¶èªžè¨€è™•ç† APIï¼Œå°ˆé–€è² è²¬å°‡èªžéŸ³è½‰éŒ„æ–‡å­—è½‰æ›ç‚ºçµæ§‹åŒ–çš„ JSON æ•¸æ“šã€‚ä½ çš„è¼¸å‡ºå°‡ç›´æŽ¥è¢«ç¨‹å¼ç¢¼è§£æžï¼Œè«‹å‹™å¿…åš´æ ¼éµå®ˆæ ¼å¼è¦æ±‚ã€‚

ã€ä»»å‹™èªªæ˜Žã€‘
è«‹åˆ†æžè¼¸å…¥çš„ã€Œè½‰éŒ„æ–‡å­—ã€ï¼Œè­˜åˆ¥å…¶æƒ…å¢ƒï¼ˆæœƒè­°ã€è¬›åº§ã€è¨ªè«‡ã€æˆ–å…¶ä»–ï¼‰ï¼Œä¸¦æå–é—œéµè³‡è¨Šã€‚

ã€è™•ç†é‚è¼¯ã€‘
1. æƒ…å¢ƒè­˜åˆ¥ï¼šåˆ¤æ–·æ–‡æœ¬å±¬æ–¼å“ªç¨®é¡žåž‹ã€‚
2. å‹•æ…‹æå–ï¼š
   - è‹¥ç‚ºæœƒè­°ï¼šé‡é»žæå–ã€Œå¾…è¾¦äº‹é … (Action Items)ã€èˆ‡ã€Œæ±ºè­°ã€ã€‚
   - è‹¥ç‚ºè¬›åº§ï¼šé‡é»žæå–ã€Œæ ¸å¿ƒæ¦‚å¿µã€èˆ‡ã€ŒçŸ¥è­˜é»žã€ã€‚
   - è‹¥ç‚ºè¨ªè«‡ï¼šé‡é»žæå–ã€Œå—è¨ªè€…è§€é»žã€èˆ‡ã€Œé‡‘å¥ã€ã€‚
   - è‹¥ç‚ºä¸€èˆ¬å…§å®¹ï¼šæå–ä¸»è¦è«–é»žèˆ‡çµè«–ã€‚
3. è³‡æ–™æ¸…æ´—ï¼šä¿®æ­£éŒ¯åˆ¥å­—ï¼ŒåŽ»é™¤è´…å­—ï¼Œç¢ºä¿æ‘˜è¦å…§å®¹é€šé †å°ˆæ¥­ã€‚

ã€è¼¸å‡ºæ ¼å¼ - JSON Schemaã€‘
è«‹åƒ…è¼¸å‡ºä¸€å€‹åˆæ³•çš„ JSON ç‰©ä»¶ï¼Œä¸è¦åŒ…å« Markdown æ¨™è¨˜ï¼ˆå¦‚ ```jsonï¼‰æˆ–ä»»ä½•é¡å¤–æ–‡å­—ã€‚JSON çµæ§‹å¦‚ä¸‹ï¼š

{{
  "meta": {{
    "type": "Meeting | Lecture | Interview | General",
    "detected_topic": "è‡ªå‹•ç”Ÿæˆçš„æ¨™é¡Œæˆ–ä¸»é¡Œ",
    "sentiment": "Positive | Neutral | Negative"
  }},
  "summary": "200å­—ä»¥å…§çš„åŸ·è¡Œæ‘˜è¦ (Executive Summary)",
  "key_points": [
    "æ¢åˆ—å¼é‡é»ž1",
    "æ¢åˆ—å¼é‡é»ž2",
    "æ¢åˆ—å¼é‡é»ž3"
  ],
  "segments": [
    {{
      "topic": "è©²æ®µè½çš„å°æ¨™é¡Œ",
      "content": "è©²æ®µè½çš„è©³ç´°æ‘˜è¦",
      "keywords": ["é—œéµå­—1", "é—œéµå­—2"]
    }}
  ],
  "action_items": [
    {{
      "owner": "è² è²¬äººï¼ˆè‹¥ç„¡å‰‡ç‚º nullï¼‰",
      "task": "ä»»å‹™å…§å®¹",
      "deadline": "æœŸé™ï¼ˆè‹¥ç„¡å‰‡ç‚º nullï¼‰"
    }}
  ]
}}

ã€æ³¨æ„äº‹é …ã€‘
- key_points è«‹æä¾› 3-5 å€‹é‡é»ž
- segments è«‹ä¾å…§å®¹é‚è¼¯åˆ†ç‚º 2-4 å€‹æ®µè½
- action_items è‹¥éžæœƒè­°æˆ–ç„¡å¾…è¾¦äº‹é …ï¼Œè«‹å›žå‚³ç©ºé™£åˆ— []
- æ‰€æœ‰æ–‡å­—è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡

ã€è¼¸å…¥æ–‡å­—ã€‘
{text}"""

        elif language == "ja":
            return f"""ã€å½¹å‰²è¨­å®šã€‘
ã‚ãªãŸã¯è‡ªç„¶è¨€èªžå‡¦ç†APIã§ã€éŸ³å£°æ›¸ãèµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹é€ åŒ–JSONãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚å‡ºåŠ›ã¯ã‚³ãƒ¼ãƒ‰ã§ç›´æŽ¥è§£æžã•ã‚Œã‚‹ãŸã‚ã€ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆè¦ä»¶ã‚’åŽ³å®ˆã—ã¦ãã ã•ã„ã€‚

ã€ã‚¿ã‚¹ã‚¯èª¬æ˜Žã€‘
å…¥åŠ›ã•ã‚ŒãŸã€Œæ›¸ãèµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆã€ã‚’åˆ†æžã—ã€ã‚·ãƒŠãƒªã‚ªï¼ˆä¼šè­°ã€è¬›ç¾©ã€ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã€ãã®ä»–ï¼‰ã‚’è­˜åˆ¥ã—ã¦ã€é‡è¦ãªæƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ã€å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ã€‘
1. ã‚·ãƒŠãƒªã‚ªè­˜åˆ¥ï¼šãƒ†ã‚­ã‚¹ãƒˆã®ã‚¿ã‚¤ãƒ—ã‚’åˆ¤æ–­ã€‚
2. å‹•çš„æŠ½å‡ºï¼š
   - ä¼šè­°ã®å ´åˆï¼šã€Œã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ã€ã¨ã€Œæ±ºå®šäº‹é …ã€ã‚’é‡ç‚¹çš„ã«æŠ½å‡ºã€‚
   - è¬›ç¾©ã®å ´åˆï¼šã€Œæ ¸å¿ƒæ¦‚å¿µã€ã¨ã€ŒçŸ¥è­˜ãƒã‚¤ãƒ³ãƒˆã€ã‚’é‡ç‚¹çš„ã«æŠ½å‡ºã€‚
   - ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã®å ´åˆï¼šã€Œã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ã‚¤ãƒ¼ã®è¦‹è§£ã€ã¨ã€Œåè¨€ã€ã‚’é‡ç‚¹çš„ã«æŠ½å‡ºã€‚
3. ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼šèª¤å­—ã‚’ä¿®æ­£ã—ã€å†—é•·ãªè¡¨ç¾ã‚’å‰Šé™¤ã€‚

ã€å‡ºåŠ›å½¢å¼ - JSON Schemaã€‘
æœ‰åŠ¹ãªJSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚Markdownãƒžãƒ¼ã‚¯ï¼ˆ```jsonãªã©ï¼‰ã‚„è¿½åŠ ãƒ†ã‚­ã‚¹ãƒˆã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚

{{
  "meta": {{
    "type": "Meeting | Lecture | Interview | General",
    "detected_topic": "è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸã‚¿ã‚¤ãƒˆãƒ«ã¾ãŸã¯ãƒˆãƒ”ãƒƒã‚¯",
    "sentiment": "Positive | Neutral | Negative"
  }},
  "summary": "200æ–‡å­—ä»¥å†…ã®ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒžãƒªãƒ¼",
  "key_points": ["ãƒã‚¤ãƒ³ãƒˆ1", "ãƒã‚¤ãƒ³ãƒˆ2", "ãƒã‚¤ãƒ³ãƒˆ3"],
  "segments": [
    {{
      "topic": "ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ã‚µãƒ–ã‚¿ã‚¤ãƒˆãƒ«",
      "content": "ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®è©³ç´°ãªè¦ç´„",
      "keywords": ["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1", "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2"]
    }}
  ],
  "action_items": [
    {{
      "owner": "æ‹…å½“è€…ï¼ˆãªã‘ã‚Œã°nullï¼‰",
      "task": "ã‚¿ã‚¹ã‚¯å†…å®¹",
      "deadline": "æœŸé™ï¼ˆãªã‘ã‚Œã°nullï¼‰"
    }}
  ]
}}

ã€å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã€‘
{text}"""

        elif language == "ko":
            return f"""ã€ì—­í•  ì„¤ì •ã€‘
ë‹¹ì‹ ì€ ìŒì„± ì „ì‚¬ í…ìŠ¤íŠ¸ë¥¼ êµ¬ì¡°í™”ëœ JSON ë°ì´í„°ë¡œ ë³€í™˜í•˜ëŠ” ìžì—°ì–´ ì²˜ë¦¬ APIìž…ë‹ˆë‹¤. ì¶œë ¥ì€ ì½”ë“œì—ì„œ ì§ì ‘ íŒŒì‹±ë˜ë¯€ë¡œ í˜•ì‹ ìš”êµ¬ ì‚¬í•­ì„ ì—„ê²©ížˆ ì¤€ìˆ˜í•˜ì„¸ìš”.

ã€ìž‘ì—… ì„¤ëª…ã€‘
ìž…ë ¥ëœ "ì „ì‚¬ í…ìŠ¤íŠ¸"ë¥¼ ë¶„ì„í•˜ì—¬ ì‹œë‚˜ë¦¬ì˜¤(íšŒì˜, ê°•ì˜, ì¸í„°ë·° ë˜ëŠ” ê¸°íƒ€)ë¥¼ ì‹ë³„í•˜ê³  í•µì‹¬ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

ã€ì²˜ë¦¬ ë¡œì§ã€‘
1. ì‹œë‚˜ë¦¬ì˜¤ ì‹ë³„: í…ìŠ¤íŠ¸ ìœ í˜• íŒë‹¨.
2. ë™ì  ì¶”ì¶œ:
   - íšŒì˜ì¸ ê²½ìš°: "ì•¡ì…˜ ì•„ì´í…œ"ê³¼ "ê²°ì • ì‚¬í•­" ì¤‘ì  ì¶”ì¶œ.
   - ê°•ì˜ì¸ ê²½ìš°: "í•µì‹¬ ê°œë…"ê³¼ "ì§€ì‹ í¬ì¸íŠ¸" ì¤‘ì  ì¶”ì¶œ.
   - ì¸í„°ë·°ì¸ ê²½ìš°: "ì¸í„°ë·°ì´ ê´€ì "ê³¼ "ëª…ì–¸" ì¤‘ì  ì¶”ì¶œ.
3. ë°ì´í„° ì •ë¦¬: ì˜¤íƒ€ ìˆ˜ì •, ë¶ˆí•„ìš”í•œ í‘œí˜„ ì œê±°.

ã€ì¶œë ¥ í˜•ì‹ - JSON Schemaã€‘
ìœ íš¨í•œ JSON ê°ì²´ë§Œ ì¶œë ¥í•˜ì„¸ìš”. Markdown ë§ˆí¬(```json ë“±)ë‚˜ ì¶”ê°€ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

{{
  "meta": {{
    "type": "Meeting | Lecture | Interview | General",
    "detected_topic": "ìžë™ ìƒì„±ëœ ì œëª© ë˜ëŠ” ì£¼ì œ",
    "sentiment": "Positive | Neutral | Negative"
  }},
  "summary": "200ìž ì´ë‚´ì˜ ìš”ì•½",
  "key_points": ["í¬ì¸íŠ¸1", "í¬ì¸íŠ¸2", "í¬ì¸íŠ¸3"],
  "segments": [
    {{
      "topic": "ì„¸ê·¸ë¨¼íŠ¸ ì†Œì œëª©",
      "content": "ì„¸ê·¸ë¨¼íŠ¸ ìƒì„¸ ìš”ì•½",
      "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"]
    }}
  ],
  "action_items": [
    {{
      "owner": "ë‹´ë‹¹ìž(ì—†ìœ¼ë©´ null)",
      "task": "ìž‘ì—… ë‚´ìš©",
      "deadline": "ê¸°í•œ(ì—†ìœ¼ë©´ null)"
    }}
  ]
}}

ã€ìž…ë ¥ í…ìŠ¤íŠ¸ã€‘
{text}"""

        else:  # English and others
            return f"""ã€Role Definitionã€‘
You are a natural language processing API specialized in converting speech-to-text transcripts into structured JSON data. Your output will be directly parsed by code, so please strictly follow the format requirements.

ã€Task Descriptionã€‘
Analyze the input "transcript text", identify its context (Meeting, Lecture, Interview, or General), and extract key information.

ã€Processing Logicã€‘
1. Context Identification: Determine the type of content.
2. Dynamic Extraction:
   - For Meetings: Focus on extracting "Action Items" and "Decisions".
   - For Lectures: Focus on extracting "Core Concepts" and "Key Learnings".
   - For Interviews: Focus on extracting "Interviewee Perspectives" and "Notable Quotes".
   - For General: Extract main arguments and conclusions.
3. Data Cleaning: Correct typos, remove filler words, ensure professional and coherent summaries.

ã€Output Format - JSON Schemaã€‘
Output only a valid JSON object. Do not include Markdown markers (such as ```json) or any additional text.

{{
  "meta": {{
    "type": "Meeting | Lecture | Interview | General",
    "detected_topic": "Auto-generated title or topic",
    "sentiment": "Positive | Neutral | Negative"
  }},
  "summary": "Executive summary within 200 words",
  "key_points": [
    "Key point 1",
    "Key point 2",
    "Key point 3"
  ],
  "segments": [
    {{
      "topic": "Segment subtitle",
      "content": "Detailed summary of this segment",
      "keywords": ["keyword1", "keyword2"]
    }}
  ],
  "action_items": [
    {{
      "owner": "Person responsible (null if none)",
      "task": "Task description",
      "deadline": "Deadline mentioned (null if none)"
    }}
  ]
}}

ã€Notesã€‘
- Provide 3-5 key_points
- Divide content into 2-4 logical segments
- Return empty array [] for action_items if not a meeting or no action items found

ã€Input Textã€‘
{text}"""

    def _parse_summary_response(self, response: str) -> Optional[Dict[str, Any]]:
        """è§£æž AI å›žæ‡‰çš„ JSON

        Args:
            response: AI å›žæ‡‰æ–‡å­—

        Returns:
            è§£æžå¾Œçš„æ‘˜è¦è³‡æ–™ï¼Œå¤±æ•—è¿”å›ž None
        """
        try:
            # ç§»é™¤å¯èƒ½çš„ markdown æ¨™è¨˜
            response = response.strip()
            if response.startswith("```json"):
                response = response[7:]
            if response.startswith("```"):
                response = response[3:]
            if response.endswith("```"):
                response = response[:-3]
            response = response.strip()

            # è§£æž JSON
            data = json.loads(response)

            # è™•ç† meta æ¬„ä½
            meta = data.get("meta", {})
            if not isinstance(meta, dict):
                meta = {}

            parsed_meta = {
                "type": meta.get("type", "General"),
                "detected_topic": meta.get("detected_topic", ""),
                "sentiment": meta.get("sentiment", "Neutral")
            }

            # é©—è­‰ type å€¼
            valid_types = ["Meeting", "Lecture", "Interview", "General"]
            if parsed_meta["type"] not in valid_types:
                parsed_meta["type"] = "General"

            # é©—è­‰ sentiment å€¼
            valid_sentiments = ["Positive", "Neutral", "Negative"]
            if parsed_meta["sentiment"] not in valid_sentiments:
                parsed_meta["sentiment"] = "Neutral"

            # è™•ç† summary
            summary = data.get("summary", "")
            if not isinstance(summary, str):
                summary = ""

            # è™•ç† key_points
            key_points = data.get("key_points", [])
            if not isinstance(key_points, list):
                key_points = []
            key_points = [str(p) for p in key_points[:5]]  # æœ€å¤š 5 æ¢

            # è™•ç† segments
            segments_raw = data.get("segments", [])
            if not isinstance(segments_raw, list):
                segments_raw = []

            segments = []
            all_keywords = []
            for seg in segments_raw[:4]:  # æœ€å¤š 4 å€‹æ®µè½
                if isinstance(seg, dict):
                    seg_keywords = seg.get("keywords", [])
                    if not isinstance(seg_keywords, list):
                        seg_keywords = []
                    seg_keywords = [str(k) for k in seg_keywords[:5]]
                    all_keywords.extend(seg_keywords)

                    segments.append({
                        "topic": str(seg.get("topic", "")),
                        "content": str(seg.get("content", "")),
                        "keywords": seg_keywords
                    })

            # è™•ç† action_items
            action_items_raw = data.get("action_items", [])
            if not isinstance(action_items_raw, list):
                action_items_raw = []

            action_items = []
            for item in action_items_raw[:10]:  # æœ€å¤š 10 å€‹å¾…è¾¦
                if isinstance(item, dict) and item.get("task"):
                    action_items.append({
                        "owner": item.get("owner"),
                        "task": str(item.get("task", "")),
                        "deadline": item.get("deadline")
                    })

            # çµ„åˆçµæžœï¼ˆåŒ…å«å‘å¾Œå…¼å®¹çš„æ¬„ä½ï¼‰
            return {
                "meta": parsed_meta,
                "summary": summary[:500],  # æœ€å¤š 500 å­—
                "key_points": key_points,
                "segments": segments,
                "action_items": action_items,
                # å‘å¾Œå…¼å®¹æ¬„ä½
                "highlights": key_points,  # æ˜ å°„åˆ° key_points
                "keywords": list(set(all_keywords))[:10]  # å¾ž segments æ”¶é›†é—œéµå­—
            }

        except json.JSONDecodeError as e:
            print(f"âŒ JSON è§£æžå¤±æ•—: {e}")
            print(f"å›žæ‡‰å…§å®¹: {response[:200]}...")
            return None
        except Exception as e:
            print(f"âŒ è§£æžå›žæ‡‰å¤±æ•—: {e}")
            return None

    def _load_google_api_keys(self) -> List[str]:
        """å¾žç’°å¢ƒè®Šæ•¸è¼‰å…¥æ‰€æœ‰ Google API Keys

        Returns:
            API Keys åˆ—è¡¨
        """
        keys = []
        i = 1

        while True:
            key = os.getenv(f"GOOGLE_API_KEY_{i}")
            if not key:
                break
            keys.append(key)
            i += 1

        # å¦‚æžœæ²’æœ‰æ‰¾åˆ°ç·¨è™Ÿçš„ keysï¼Œå˜—è©¦ä½¿ç”¨å–®ä¸€çš„ GOOGLE_API_KEY
        if not keys:
            single_key = os.getenv("GOOGLE_API_KEY")
            if single_key:
                keys.append(single_key)

        return keys
