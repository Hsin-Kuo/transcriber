#!/usr/bin/env python3
"""
Whisper è½‰éŒ„æœå‹™ - FastAPI ç‰ˆæœ¬
æ¨¡å‹åªéœ€è¼‰å…¥ä¸€æ¬¡ï¼Œå¯é‡è¤‡ä½¿ç”¨æå‡æ•ˆç‡
"""

import os
import sys
import tempfile
import shutil
import asyncio
import uuid
import json
from pathlib import Path
from typing import Optional, Dict, Any, List
from datetime import datetime, timezone, timedelta
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from threading import Lock
import multiprocessing
from faster_whisper import WhisperModel
from pydub import AudioSegment
from pydub.silence import detect_nonsilent
from fastapi import FastAPI, UploadFile, File, Form, HTTPException, BackgroundTasks
from fastapi.responses import FileResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
from dotenv import load_dotenv
load_dotenv()

# Speaker Diarization
try:
    from pyannote.audio import Pipeline
    DIARIZATION_AVAILABLE = True
except ImportError:
    DIARIZATION_AVAILABLE = False
    print("âš ï¸  pyannote.audio æœªå®‰è£ï¼Œspeaker diarization åŠŸèƒ½ä¸å¯ç”¨")

# â€”â€” è¨­å®š â€”â€” #
DEFAULT_MODEL = "medium"
CHUNK_DURATION_MS = 10 * 60 * 1000
OPENAI_MODEL = "gpt-4o-mini"
GEMINI_MODEL = "gemini-2.0-flash"

# é€²åº¦éšæ®µæ™‚é–“æ¬Šé‡ï¼ˆç”¨æ–¼è¨ˆç®—é€²åº¦ç™¾åˆ†æ¯”ï¼‰
PROGRESS_WEIGHTS = {
    "audio_conversion": 2.2,      # éŸ³è¨Šè½‰æª”ï¼š30s
    "audio_chunking": 2.2,        # éŸ³è¨Šåˆ‡åˆ†ï¼š30s
    "chunk_start": 0.4,           # æ¯å€‹ chunk é–‹å§‹ï¼š30s / 6 = 0.4% per chunk
    "chunk_complete": 14.5,       # æ¯å€‹ chunk å®Œæˆï¼š5min / 6 = 14.5% per chunk
    "punctuation": 13.0,          # åŠ æ¨™é»ï¼š3min
}

# æ™‚å€è¨­å®š (UTC+8 å°åŒ—æ™‚é–“)
TZ_UTC8 = timezone(timedelta(hours=8))

def get_current_time():
    """å–å¾— UTC+8 ç•¶å‰æ™‚é–“"""
    return datetime.now(TZ_UTC8).strftime("%Y-%m-%d %H:%M:%S")

def format_duration(seconds: float) -> str:
    """æ ¼å¼åŒ–æ™‚é•·ç‚ºå‹å¥½çš„æ–‡å­—"""
    if seconds < 60:
        return f"{int(seconds)} ç§’"
    elif seconds < 3600:
        minutes = int(seconds // 60)
        secs = int(seconds % 60)
        return f"{minutes} åˆ† {secs} ç§’"
    else:
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        return f"{hours} å°æ™‚ {minutes} åˆ†"

def calculate_progress_percentage(task_data: Dict[str, Any]) -> float:
    """æ ¹æ“šä»»å‹™ç‹€æ…‹è¨ˆç®—é€²åº¦ç™¾åˆ†æ¯”ï¼ˆåŸºæ–¼æ™‚é–“æ¬Šé‡ï¼‰"""
    progress = 0.0

    # 1. éŸ³è¨Šè½‰æª”å®Œæˆ
    if task_data.get("audio_converted", False):
        progress += PROGRESS_WEIGHTS["audio_conversion"]

    # 2. éŸ³è¨Šåˆ‡åˆ†å®Œæˆ
    if task_data.get("chunks_created", False):
        progress += PROGRESS_WEIGHTS["audio_chunking"]

    # 3. Chunks è™•ç†
    chunks = task_data.get("chunks", [])
    for chunk in chunks:
        if chunk.get("status") == "processing":
            progress += PROGRESS_WEIGHTS["chunk_start"]
        elif chunk.get("status") == "completed":
            progress += PROGRESS_WEIGHTS["chunk_start"]
            progress += PROGRESS_WEIGHTS["chunk_complete"]

    # 4. æ¨™é»è™•ç†
    if task_data.get("punctuation_completed", False):
        progress += PROGRESS_WEIGHTS["punctuation"]
    elif task_data.get("punctuation_started", False):
        # æ¨™é»è™•ç†ä¸­ï¼Œæ ¹æ“šæ®µæ•¸è¨ˆç®—é€²åº¦
        punct_current = task_data.get("punctuation_current_chunk", 0)
        punct_total = task_data.get("punctuation_total_chunks", 1)
        punct_progress = (punct_current / punct_total) * PROGRESS_WEIGHTS["punctuation"]
        progress += punct_progress

    return min(progress, 100.0)

# è¼¸å‡ºç›®éŒ„è¨­å®š
OUTPUT_DIR = Path(__file__).parent.parent / "output"
OUTPUT_DIR.mkdir(exist_ok=True)

# ä»»å‹™ç‹€æ…‹æŒä¹…åŒ–æª”æ¡ˆ
TASKS_DB_FILE = OUTPUT_DIR / "tasks.json"

# â€”â€” Google API Keys ç®¡ç† â€”â€” #
def load_google_api_keys():
    """å¾ç’°å¢ƒè®Šæ•¸è¼‰å…¥æ‰€æœ‰ Google API Keys"""
    keys = []
    i = 1
    while True:
        key = os.getenv(f"GOOGLE_API_KEY_{i}")
        if not key:
            break
        keys.append(key)
        i += 1

    # å¦‚æœæ²’æœ‰æ‰¾åˆ°ç·¨è™Ÿçš„ keysï¼Œå˜—è©¦ä½¿ç”¨å–®ä¸€çš„ GOOGLE_API_KEY
    if not keys:
        single_key = os.getenv("GOOGLE_API_KEY")
        if single_key:
            keys.append(single_key)

    return keys

GOOGLE_API_KEYS = load_google_api_keys()
current_google_key_index = 0
google_keys_lock = Lock()

def get_next_google_api_key():
    """ç²å–ä¸‹ä¸€å€‹ Google API Keyï¼ˆè¼ªè©¢ï¼‰"""
    global current_google_key_index

    if not GOOGLE_API_KEYS:
        raise ValueError("æœªè¨­å®šä»»ä½• GOOGLE_API_KEY")

    with google_keys_lock:
        key = GOOGLE_API_KEYS[current_google_key_index]
        current_google_key_index = (current_google_key_index + 1) % len(GOOGLE_API_KEYS)
        print(f"ğŸ”‘ ä½¿ç”¨ Google API Key #{current_google_key_index + 1}/{len(GOOGLE_API_KEYS)}")
        return key

# â€”â€” å…¨åŸŸæ¨¡å‹ (å•Ÿå‹•æ™‚è¼‰å…¥ï¼ŒæŒä¹…åŒ–åœ¨è¨˜æ†¶é«”ä¸­) â€”â€” #
whisper_model = None
current_model_name = None

# â€”â€” ä»»å‹™ç‹€æ…‹ç®¡ç† â€”â€” #
transcription_tasks: Dict[str, Dict[str, Any]] = {}  # å„²å­˜ä»»å‹™ç‹€æ…‹
task_temp_dirs: Dict[str, Path] = {}  # å„²å­˜ä»»å‹™çš„æš«å­˜ç›®éŒ„è·¯å¾‘
task_cancelled: Dict[str, bool] = {}  # æ¨™è¨˜å·²å–æ¶ˆçš„ä»»å‹™
task_diarization_processes: Dict[str, Any] = {}  # å„²å­˜ä»»å‹™çš„ diarization é€²ç¨‹
tasks_lock = Lock()  # ç·šç¨‹å®‰å…¨é–
executor = ThreadPoolExecutor(max_workers=1)  # ç·šç¨‹æ± ï¼Œæœ€å¤š 1 å€‹ä¸¦ç™¼è½‰éŒ„

def save_tasks_to_disk():
    """å°‡ä»»å‹™ç‹€æ…‹ä¿å­˜åˆ°ç£ç¢Ÿ"""
    try:
        with tasks_lock:
            with open(TASKS_DB_FILE, 'w', encoding='utf-8') as f:
                json.dump(transcription_tasks, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âŒ ä¿å­˜ä»»å‹™ç‹€æ…‹å¤±æ•—ï¼š{e}")

def load_tasks_from_disk():
    """å¾ç£ç¢Ÿè¼‰å…¥ä»»å‹™ç‹€æ…‹"""
    global transcription_tasks
    try:
        if TASKS_DB_FILE.exists():
            with open(TASKS_DB_FILE, 'r', encoding='utf-8') as f:
                loaded_tasks = json.load(f)

            # éæ¿¾æ‰é€²è¡Œä¸­çš„ä»»å‹™ï¼ˆå› ç‚ºé‡å•Ÿå¾Œç„¡æ³•ç¹¼çºŒï¼‰
            # åªä¿ç•™å·²å®Œæˆã€å¤±æ•—ã€å–æ¶ˆçš„ä»»å‹™
            with tasks_lock:
                for task_id, task in loaded_tasks.items():
                    if task["status"] in ["completed", "failed", "cancelled"]:
                        transcription_tasks[task_id] = task
                    else:
                        # å°‡é€²è¡Œä¸­çš„ä»»å‹™æ¨™è¨˜ç‚ºå¤±æ•—
                        task["status"] = "failed"
                        task["error"] = "æœå‹™é‡å•Ÿï¼Œä»»å‹™å·²ä¸­æ–·"
                        task["progress"] = "æœå‹™é‡å•Ÿï¼Œä»»å‹™å·²ä¸­æ–·"
                        transcription_tasks[task_id] = task

            print(f"âœ… å·²å¾ç£ç¢Ÿè¼‰å…¥ {len(transcription_tasks)} å€‹ä»»å‹™è¨˜éŒ„")
    except Exception as e:
        print(f"âŒ è¼‰å…¥ä»»å‹™ç‹€æ…‹å¤±æ•—ï¼š{e}")
        print(f"   å°‡å¾ç©ºç™½ç‹€æ…‹é–‹å§‹")

app = FastAPI(
    title="Whisper è½‰éŒ„æœå‹™",
    description="ä¸Šå‚³éŸ³æª”é€²è¡Œè½‰éŒ„ï¼Œæ”¯æ´è‡ªå‹•åˆ†æ®µèˆ‡æ¨™é»",
    version="2.0.0"
)

# æ·»åŠ  CORS ä¸­é–“ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿç”¢ç’°å¢ƒæ‡‰è©²é™åˆ¶å…·é«”åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ---------- å·¥å…·å‡½å¼ ----------

def cleanup_temp_dir(temp_dir: Path):
    """æ¸…ç†è‡¨æ™‚ç›®éŒ„"""
    try:
        if temp_dir.exists():
            shutil.rmtree(temp_dir)
            print(f"ğŸ—‘ï¸ å·²æ¸…ç†è‡¨æ™‚ç›®éŒ„ï¼š{temp_dir.name}")
    except Exception as e:
        print(f"âš ï¸ æ¸…ç†è‡¨æ™‚ç›®éŒ„å¤±æ•—ï¼š{e}")


def update_task_status(task_id: str, updates: Dict[str, Any]):
    """æ›´æ–°ä»»å‹™ç‹€æ…‹ï¼ˆç·šç¨‹å®‰å…¨ï¼‰"""
    with tasks_lock:
        if task_id in transcription_tasks:
            transcription_tasks[task_id].update(updates)
            transcription_tasks[task_id]["updated_at"] = get_current_time()

            # è‡ªå‹•è¨ˆç®—ä¸¦æ›´æ–°é€²åº¦ç™¾åˆ†æ¯”
            progress_pct = calculate_progress_percentage(transcription_tasks[task_id])
            transcription_tasks[task_id]["progress_percentage"] = round(progress_pct, 1)

    # ç•¶ä»»å‹™ç‹€æ…‹è®Šæ›´ç‚ºçµ‚æ­¢ç‹€æ…‹æ™‚ï¼Œä¿å­˜åˆ°ç£ç¢Ÿ
    if updates.get("status") in ["completed", "failed", "cancelled"]:
        save_tasks_to_disk()


# ---------- è½‰éŒ„å‡½å¼ (å¾åŸå§‹ transcribe.py ç§»æ¤) ----------

def transcribe_single_chunk(
    model,
    chunk_path: Path,
    language: Optional[str] = None,
    task_id: str = None,
    chunk_idx: int = None,
    time_offset: float = 0.0,
    return_segments: bool = False
) -> str:
    """è½‰éŒ„å–®ä¸€éŸ³æª”ç‰‡æ®µï¼ˆç”¨æ–¼ä¸¦è¡Œè™•ç†ï¼‰

    Args:
        model: Whisper æ¨¡å‹
        chunk_path: éŸ³æª”è·¯å¾‘
        language: èªè¨€ä»£ç¢¼ï¼ˆNone è¡¨ç¤ºè‡ªå‹•åµæ¸¬ï¼Œé è¨­å€¼ï¼‰
        task_id: ä»»å‹™ ID
        chunk_idx: Chunk ç´¢å¼•
        time_offset: æ™‚é–“åç§»ï¼ˆç§’ï¼‰ï¼Œç”¨æ–¼è¨ˆç®—ç›¸å°æ–¼å®Œæ•´éŸ³æª”çš„æ™‚é–“æˆ³
        return_segments: æ˜¯å¦è¿”å›å¸¶æ™‚é–“æˆ³çš„ segments

    Returns:
        å¦‚æœ return_segments=Trueï¼Œè¿”å› (text, segments)
        å¦å‰‡è¿”å› text
    """
    # æ¨™è¨˜æ­¤ chunk é–‹å§‹è™•ç†ï¼ˆå¯¦éš›é–‹å§‹åŸ·è¡Œæ™‚æ‰æ¨™è¨˜ï¼‰
    if task_id and chunk_idx:
        with tasks_lock:
            if task_id in transcription_tasks:
                chunks = transcription_tasks[task_id].get("chunks", [])
                if chunk_idx - 1 < len(chunks) and chunks[chunk_idx - 1]["status"] == "pending":
                    chunks[chunk_idx - 1]["status"] = "processing"
                    chunks[chunk_idx - 1]["started_at"] = get_current_time()
        update_task_status(task_id, {})  # è§¸ç™¼é€²åº¦è¨ˆç®—

    segments, info = model.transcribe(str(chunk_path), language=language, beam_size=5)

    # æ”¶é›† segments
    segments_list = []
    text_parts = []

    for segment in segments:
        text_parts.append(segment.text)
        if return_segments:
            segments_list.append({
                "start": segment.start + time_offset,  # åŠ ä¸Šæ™‚é–“åç§»
                "end": segment.end + time_offset,
                "text": segment.text
            })

    text = "".join(text_parts).strip()

    if return_segments:
        return text, segments_list
    return text


def transcribe_with_timestamps(model, audio_path: Path, language: Optional[str] = None) -> List[Dict]:
    """
    è½‰éŒ„éŸ³æª”ä¸¦è¿”å›å¸¶æ™‚é–“æˆ³çš„ segments

    Args:
        model: Whisper æ¨¡å‹
        audio_path: éŸ³æª”è·¯å¾‘
        language: èªè¨€ä»£ç¢¼ï¼ˆNone è¡¨ç¤ºè‡ªå‹•åµæ¸¬ï¼Œé è¨­å€¼ï¼‰

    Returns:
        List of segments with format:
        [{"start": 0.0, "end": 5.2, "text": "hello"}, ...]
    """
    segments_list = []
    segments, info = model.transcribe(str(audio_path), language=language, beam_size=5)

    for segment in segments:
        segments_list.append({
            "start": segment.start,
            "end": segment.end,
            "text": segment.text
        })

    return segments_list


def perform_diarization_in_process(audio_path_str: str, max_speakers: Optional[int], hf_token: str) -> Optional[List[Dict]]:
    """åœ¨ç¨ç«‹é€²ç¨‹ä¸­åŸ·è¡Œ speaker diarizationï¼ˆå¯è¢«å¼·åˆ¶çµ‚æ­¢ï¼‰

    æ­¤å‡½æ•¸è¨­è¨ˆç‚ºåœ¨å–®ç¨çš„é€²ç¨‹ä¸­é‹è¡Œï¼Œå› æ­¤ä¸ä¾è³´å…¨å±€è®Šé‡

    Args:
        audio_path_str: éŸ³æª”è·¯å¾‘ï¼ˆå­—ä¸²æ ¼å¼ï¼‰
        max_speakers: æœ€å¤§è¬›è€…äººæ•¸ï¼ˆå¯é¸ï¼Œ2-10ï¼‰
        hf_token: Hugging Face Token

    Returns:
        List of diarization segments with format:
        [{"start": 0.0, "end": 5.2, "speaker": "SPEAKER_00"}, ...]
    """
    try:
        # åœ¨é€²ç¨‹ä¸­é‡æ–°è¼‰å…¥ pipelineï¼ˆå› ç‚ºç„¡æ³•è·¨é€²ç¨‹å‚³éï¼‰
        from pyannote.audio import Pipeline
        from huggingface_hub import login

        if hf_token:
            login(token=hf_token, add_to_git_credential=False)

        print(f"ğŸ”Š [é€²ç¨‹] æ­£åœ¨è¼‰å…¥ diarization pipeline...")
        import torch
        pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization-3.1")

        # M1 Mac MPS åŠ é€Ÿ
        if torch.backends.mps.is_available():
            pipeline.to(torch.device("mps"))
            print(f"ğŸ”Š [é€²ç¨‹] ä½¿ç”¨ MPS åŠ é€Ÿ")

        print(f"ğŸ”Š [é€²ç¨‹] æ­£åœ¨åˆ†æèªªè©±è€…...")

        # æº–å‚™ diarization åƒæ•¸
        diarization_kwargs = {}
        if max_speakers is not None and 2 <= max_speakers <= 10:
            diarization_kwargs["min_speakers"] = 1
            diarization_kwargs["max_speakers"] = max_speakers
            print(f"   [é€²ç¨‹] è¨­å®šè¬›è€…äººæ•¸ç¯„åœï¼š1-{max_speakers} äºº")

        print(f"   [é€²ç¨‹] Diarization åƒæ•¸ï¼š{diarization_kwargs}")
        diarization = pipeline(audio_path_str, **diarization_kwargs)

        segments = []
        for turn, _, speaker in diarization.itertracks(yield_label=True):
            segments.append({
                "start": turn.start,
                "end": turn.end,
                "speaker": speaker
            })

        print(f"âœ… [é€²ç¨‹] èªªè©±è€…åˆ†æå®Œæˆï¼Œè­˜åˆ¥åˆ° {len(set(s['speaker'] for s in segments))} ä½èªªè©±è€…")
        return segments

    except Exception as e:
        print(f"âš ï¸  [é€²ç¨‹] Speaker diarization å¤±æ•—ï¼š{e}")
        return None


def perform_diarization(audio_path: Path, max_speakers: Optional[int] = None) -> Optional[List[Dict]]:
    """åŸ·è¡Œ speaker diarizationï¼ˆç·šç¨‹ç‰ˆæœ¬ï¼Œç”¨æ–¼éåˆ†æ®µæ¨¡å¼ï¼‰

    Args:
        audio_path: éŸ³æª”è·¯å¾‘
        max_speakers: æœ€å¤§è¬›è€…äººæ•¸ï¼ˆå¯é¸ï¼Œ2-10ï¼‰

    Returns:
        List of diarization segments with format:
        [{"start": 0.0, "end": 5.2, "speaker": "SPEAKER_00"}, ...]
    """
    if not diarization_pipeline:
        return None

    try:
        print(f"ğŸ”Š æ­£åœ¨åˆ†æèªªè©±è€…...")

        # æº–å‚™ diarization åƒæ•¸
        diarization_kwargs = {}
        if max_speakers is not None and 2 <= max_speakers <= 10:
            # pyannote.audio éœ€è¦åŒæ™‚è¨­å®š min_speakers å’Œ max_speakers
            diarization_kwargs["min_speakers"] = 1
            diarization_kwargs["max_speakers"] = max_speakers
            print(f"   è¨­å®šè¬›è€…äººæ•¸ç¯„åœï¼š1-{max_speakers} äºº")

        print(f"   Diarization åƒæ•¸ï¼š{diarization_kwargs}")
        diarization = diarization_pipeline(str(audio_path), **diarization_kwargs)

        segments = []
        for turn, _, speaker in diarization.itertracks(yield_label=True):
            segments.append({
                "start": turn.start,
                "end": turn.end,
                "speaker": speaker
            })

        print(f"âœ… èªªè©±è€…åˆ†æå®Œæˆï¼Œè­˜åˆ¥åˆ° {len(set(s['speaker'] for s in segments))} ä½èªªè©±è€…")
        return segments

    except Exception as e:
        print(f"âš ï¸  Speaker diarization å¤±æ•—ï¼š{e}")
        return None


def merge_transcription_with_diarization(
    transcription_segments: List[Dict],
    diarization_segments: List[Dict]
) -> str:
    """åˆä½µè½‰éŒ„æ–‡å­—å’Œèªªè©±è€…æ¨™è¨˜

    Args:
        transcription_segments: Whisper è½‰éŒ„çµæœ (å¸¶æ™‚é–“æˆ³)
        diarization_segments: Speaker diarization çµæœ

    Returns:
        åˆä½µå¾Œçš„æ–‡å­—ï¼Œæ ¼å¼ï¼š[Speaker A] æ–‡å­—å…§å®¹
    """
    if not diarization_segments:
        # æ²’æœ‰ diarization çµæœï¼Œç›´æ¥è¿”å›ç´”æ–‡å­—
        return " ".join(seg.get("text", "") for seg in transcription_segments)

    # ç‚ºæ¯å€‹è½‰éŒ„ç‰‡æ®µåˆ†é…èªªè©±è€…
    result_lines = []
    current_speaker = None
    current_text = []

    for trans_seg in transcription_segments:
        trans_start = trans_seg.get("start", 0)
        trans_end = trans_seg.get("end", 0)
        trans_text = trans_seg.get("text", "")

        if not trans_text.strip():
            continue

        # æ‰¾åˆ°èˆ‡æ­¤è½‰éŒ„ç‰‡æ®µé‡ç–Šæœ€å¤šçš„èªªè©±è€…
        best_speaker = None
        max_overlap = 0

        for dia_seg in diarization_segments:
            dia_start = dia_seg["start"]
            dia_end = dia_seg["end"]

            # è¨ˆç®—é‡ç–Šæ™‚é–“
            overlap_start = max(trans_start, dia_start)
            overlap_end = min(trans_end, dia_end)
            overlap = max(0, overlap_end - overlap_start)

            if overlap > max_overlap:
                max_overlap = overlap
                best_speaker = dia_seg["speaker"]

        # å¦‚æœèªªè©±è€…æ”¹è®Šï¼Œè¼¸å‡ºä¹‹å‰çš„å…§å®¹
        if best_speaker != current_speaker and current_text:
            speaker_label = f"[{current_speaker}]" if current_speaker else ""
            result_lines.append(f"{speaker_label} {''.join(current_text)}")
            current_text = []

        current_speaker = best_speaker
        current_text.append(trans_text)

    # è¼¸å‡ºæœ€å¾Œä¸€æ®µ
    if current_text:
        speaker_label = f"[{current_speaker}]" if current_speaker else ""
        result_lines.append(f"{speaker_label} {''.join(current_text)}")

    return "\n\n".join(result_lines)


def transcribe_audio_in_chunks(
    audio_path: Path,
    model,
    chunk_duration_ms: int = CHUNK_DURATION_MS,
    task_id: str = None,
    diarize: bool = False,
    max_speakers: Optional[int] = None,
    language: Optional[str] = None
) -> str:
    """å°‡éŸ³æª”åˆ†æ®µå¾Œä¸¦è¡Œè½‰éŒ„ï¼Œæé«˜é€Ÿåº¦å’Œæº–ç¢ºåº¦

    Args:
        audio_path: éŸ³æª”è·¯å¾‘
        model: Whisper æ¨¡å‹
        chunk_duration_ms: æ¯æ®µé•·åº¦ï¼ˆæ¯«ç§’ï¼‰
        task_id: ä»»å‹™ ID
        diarize: æ˜¯å¦å•Ÿç”¨èªªè©±è€…è¾¨è­˜ï¼ˆæœƒå…ˆå°å®Œæ•´éŸ³æª”åš diarizationï¼Œå†åˆ‡åˆ†è½‰éŒ„ï¼‰
        max_speakers: æœ€å¤§è¬›è€…äººæ•¸ï¼ˆå¯é¸ï¼Œ2-10ï¼‰
    """
    print(f"ğŸ”Š è¼‰å…¥éŸ³æª”ï¼š{audio_path.name}")
    audio = AudioSegment.from_file(audio_path)
    total_duration_ms = len(audio)
    total_minutes = total_duration_ms / 1000 / 60

    print(f"ğŸ“Š éŸ³æª”ç¸½é•·åº¦ï¼š{total_minutes:.1f} åˆ†é˜")

    # å¦‚æœéŸ³æª”ä¸é•·ï¼Œç›´æ¥è½‰éŒ„
    if total_duration_ms <= chunk_duration_ms:
        print(f"ğŸ“ éŸ³æª”é•·åº¦åœ¨ {chunk_duration_ms/1000/60:.0f} åˆ†é˜å…§ï¼Œç›´æ¥è½‰éŒ„...")
        return transcribe_single_chunk(model, audio_path, language=language)

    # æ­¥é©Ÿ 1ï¼šå¦‚æœå•Ÿç”¨ diarizationï¼Œåœ¨èƒŒæ™¯ä¸¦è¡ŒåŸ·è¡Œèªªè©±è€…è¾¨è­˜
    diarization_future = None
    diarization_start_time = None
    diarization_executor = None

    if diarize and diarization_pipeline:
        diarization_start_time = datetime.now(TZ_UTC8)
        print(f"ğŸ”Š åœ¨èƒŒæ™¯å•Ÿå‹•èªªè©±è€…è¾¨è­˜ï¼ˆèˆ‡è½‰éŒ„ä¸¦è¡ŒåŸ·è¡Œï¼‰...")
        if task_id:
            update_task_status(task_id, {
                "progress": "æ­£åœ¨åˆ†æèªªè©±è€…ï¼ˆèƒŒæ™¯åŸ·è¡Œï¼‰...",
                "diarization_status": "running"
            })

        # ä½¿ç”¨é€²ç¨‹æ± åŸ·è¡Œ diarizationï¼Œå¯ä»¥è¢«å¼·åˆ¶çµ‚æ­¢
        diarization_executor = ProcessPoolExecutor(max_workers=1)
        hf_token = os.getenv("HF_TOKEN", "")
        diarization_future = diarization_executor.submit(
            perform_diarization_in_process,
            str(audio_path),
            max_speakers,
            hf_token
        )

        # è¨˜éŒ„ diarization é€²ç¨‹ä¾›å–æ¶ˆæ™‚ä½¿ç”¨
        if task_id:
            with tasks_lock:
                task_diarization_processes[task_id] = diarization_executor

    # é•·éŸ³æª”ï¼šåˆ†æ®µè™•ç†
    num_chunks = (total_duration_ms + chunk_duration_ms - 1) // chunk_duration_ms
    print(f"ğŸ”„ éŸ³æª”è¼ƒé•·ï¼Œå°‡åˆ†ç‚º {num_chunks} æ®µè™•ç†ï¼ˆæ¯æ®µç´„ {chunk_duration_ms/1000/60:.0f} åˆ†é˜ï¼‰...")

    # åˆå§‹åŒ– chunks ç‹€æ…‹è¿½è¹¤
    if task_id:
        chunks_state = [
            {
                "chunk_id": i,
                "status": "pending",
                "started_at": None,
                "completed_at": None,
                "duration_seconds": None
            }
            for i in range(1, num_chunks + 1)
        ]

        update_task_status(task_id, {
            "total_chunks": num_chunks,
            "completed_chunks": 0,
            "chunks": chunks_state,
            "chunks_created": False  # åˆ‡åˆ†å°šæœªå®Œæˆ
            # ä¸è¨­ç½® estimated_completion_timeï¼Œå‰ç«¯æœƒé¡¯ç¤ºã€Œè¨ˆç®—ä¸­......ã€
        })

    # ç¬¬ä¸€æ­¥ï¼šæº–å‚™æ‰€æœ‰ chunksï¼ˆåˆ‡åˆ†éŸ³æª”ï¼‰
    print(f"ğŸ“¦ æº–å‚™åˆ‡åˆ†éŸ³æª”...")
    chunk_info_list = []  # å„²å­˜ (chunk_idx, start_ms, end_ms, temp_path)
    start_ms = 0
    chunk_idx = 1

    while start_ms < total_duration_ms:
        end_ms = min(start_ms + chunk_duration_ms, total_duration_ms)

        # å¦‚æœä¸æ˜¯æœ€å¾Œä¸€æ®µï¼Œå˜—è©¦åœ¨éœéŸ³è™•åˆ‡åˆ†
        if end_ms < total_duration_ms:
            search_start = max(end_ms - 30000, start_ms)
            search_end = min(end_ms + 30000, total_duration_ms)
            search_segment = audio[search_start:search_end]

            nonsilent_ranges = detect_nonsilent(
                search_segment,
                min_silence_len=500,
                silence_thresh=-40
            )

            if nonsilent_ranges:
                target_pos = end_ms - search_start
                best_split = end_ms
                min_diff = float('inf')

                for i in range(len(nonsilent_ranges) - 1):
                    gap_start = nonsilent_ranges[i][1]
                    gap_end = nonsilent_ranges[i + 1][0]
                    gap_mid = (gap_start + gap_end) // 2

                    diff = abs(gap_mid - target_pos)
                    if diff < min_diff:
                        min_diff = diff
                        best_split = search_start + gap_mid

                if abs(best_split - end_ms) < 60000:
                    end_ms = best_split

        print(f"   æº–å‚™ç¬¬ {chunk_idx}/{num_chunks} æ®µ ({start_ms/1000/60:.1f}-{end_ms/1000/60:.1f} åˆ†é˜)...")

        # æª¢æŸ¥æ˜¯å¦å·²è¢«å–æ¶ˆ
        if task_id and task_cancelled.get(task_id, False):
            raise RuntimeError("ä»»å‹™å·²è¢«ä½¿ç”¨è€…å–æ¶ˆ")

        # å°å‡º chunk åˆ°è‡¨æ™‚æª”æ¡ˆ
        chunk_audio = audio[start_ms:end_ms]
        temp_path = audio_path.parent / f"_temp_{audio_path.stem}_chunk_{chunk_idx}.wav"
        chunk_audio.export(temp_path, format="wav")

        chunk_info_list.append((chunk_idx, start_ms, end_ms, temp_path))

        start_ms = end_ms
        chunk_idx += 1

    print(f"âœ… å·²æº–å‚™ {len(chunk_info_list)} å€‹éŸ³æª”ç‰‡æ®µ")

    # æ¨™è¨˜åˆ‡åˆ†å®Œæˆä¸¦è¨ˆç®—é ä¼°å®Œæˆæ™‚é–“
    if task_id:
        import math
        # å¦‚æœå•Ÿç”¨ diarizationï¼Œä½¿ç”¨è¼ƒä½çš„ä¸¦è¡Œåº¦
        num_workers = 2 if diarize else 4
        rounds = math.ceil(num_chunks / num_workers)
        estimated_minutes = rounds * 14 * 1.1

        # å–å¾—ä»»å‹™é–‹å§‹æ™‚é–“ä¸¦è¨ˆç®—é ä¼°å®Œæˆæ™‚é–“
        with tasks_lock:
            if task_id in transcription_tasks:
                started_at_str = transcription_tasks[task_id].get("started_at")
                if started_at_str:
                    started_at = datetime.strptime(started_at_str, "%Y-%m-%d %H:%M:%S").replace(tzinfo=TZ_UTC8)
                    estimated_completion = started_at + timedelta(minutes=estimated_minutes)
                    estimated_completion_str = estimated_completion.strftime("%Y-%m-%d %H:%M:%S")
                else:
                    estimated_completion_str = None

        update_task_status(task_id, {
            "chunks_created": True,
            "progress": f"å·²åˆ‡åˆ†ç‚º {num_chunks} å€‹ç‰‡æ®µï¼Œé–‹å§‹è½‰éŒ„...",
            "estimated_completion_time": estimated_completion_str
        })

    # ç¬¬äºŒæ­¥ï¼šä¸¦è¡Œè½‰éŒ„æ‰€æœ‰ chunksï¼ˆèˆ‡ diarization åŒæ™‚é€²è¡Œï¼‰
    # å¦‚æœåŒæ™‚åŸ·è¡Œ diarizationï¼Œé™ä½è½‰éŒ„ä¸¦è¡Œåº¦ä»¥æ¸›å°‘è³‡æºç«¶çˆ­
    transcribe_workers = 2 if diarization_future else 4
    print(f"ğŸš€ é–‹å§‹ä¸¦è¡Œè½‰éŒ„ï¼ˆä¸¦è¡Œæ•¸ï¼š{transcribe_workers}ï¼‰{'ï¼ŒåŒæ™‚é€²è¡Œèªªè©±è€…è¾¨è­˜' if diarization_future else ''}...")
    chunks_text = [None] * num_chunks  # é å…ˆåˆ†é…é™£åˆ—ä¿æŒé †åº
    all_segments = [] if diarization_future else None  # å¦‚æœå•Ÿç”¨ diarizationï¼Œæ”¶é›†æ‰€æœ‰ segments

    try:
        with ThreadPoolExecutor(max_workers=transcribe_workers) as executor:
            # æäº¤æ‰€æœ‰è½‰éŒ„ä»»å‹™åˆ°ç·šç¨‹æ± 
            future_to_chunk = {}
            for chunk_idx, start_ms, end_ms, temp_path in chunk_info_list:
                # è¨ˆç®—é€™å€‹ chunk çš„æ™‚é–“åç§»ï¼ˆç›¸å°æ–¼å®Œæ•´éŸ³æª”çš„ç§’æ•¸ï¼‰
                time_offset_seconds = start_ms / 1000.0

                # å¦‚æœå•Ÿç”¨ diarizationï¼Œéœ€è¦è¿”å›å¸¶æ™‚é–“æˆ³çš„ segments
                future = executor.submit(
                    transcribe_single_chunk,
                    model,
                    temp_path,
                    language,  # ä½¿ç”¨æŒ‡å®šçš„èªè¨€
                    task_id,
                    chunk_idx,
                    time_offset_seconds,
                    bool(diarization_future)  # return_segments
                )
                future_to_chunk[future] = (chunk_idx, temp_path, time_offset_seconds)

            # ç­‰å¾…å®Œæˆä¸¦æ›´æ–°é€²åº¦
            for future in as_completed(future_to_chunk):
                chunk_idx, temp_path, time_offset = future_to_chunk[future]

                # æª¢æŸ¥æ˜¯å¦å·²è¢«å–æ¶ˆ
                if task_id and task_cancelled.get(task_id, False):
                    executor.shutdown(wait=False, cancel_futures=True)
                    # å¼·åˆ¶çµ‚æ­¢ diarization é€²ç¨‹
                    if diarization_future and diarization_executor:
                        print(f"ğŸ›‘ æ­£åœ¨å¼·åˆ¶çµ‚æ­¢èªªè©±è€…è¾¨è­˜é€²ç¨‹...")
                        diarization_executor.shutdown(wait=False, cancel_futures=True)
                    raise RuntimeError("ä»»å‹™å·²è¢«ä½¿ç”¨è€…å–æ¶ˆ")

                try:
                    chunk_start_time = None
                    if task_id:
                        with tasks_lock:
                            if task_id in transcription_tasks:
                                chunks = transcription_tasks[task_id].get("chunks", [])
                                if chunk_idx - 1 < len(chunks):
                                    start_str = chunks[chunk_idx - 1].get("started_at")
                                    if start_str:
                                        chunk_start_time = datetime.strptime(start_str, "%Y-%m-%d %H:%M:%S")

                    result = future.result()

                    # è™•ç†è¿”å›çµæœï¼ˆå¯èƒ½æ˜¯ç´”æ–‡å­—æˆ–(æ–‡å­—, segments)å…ƒçµ„ï¼‰
                    if diarization_future and isinstance(result, tuple):
                        chunk_text, chunk_segments = result
                        all_segments.extend(chunk_segments)  # æ”¶é›†æ‰€æœ‰ segments
                    else:
                        chunk_text = result

                    chunks_text[chunk_idx - 1] = chunk_text
                    print(f"   âœ… å®Œæˆç¬¬ {chunk_idx}/{num_chunks} æ®µ")

                    # è¨ˆç®— chunk è™•ç†æ™‚é–“
                    chunk_duration = None
                    if chunk_start_time:
                        chunk_end_time = datetime.now(TZ_UTC8)
                        chunk_duration = (chunk_end_time - chunk_start_time.replace(tzinfo=TZ_UTC8)).total_seconds()

                    # æ›´æ–° chunk ç‹€æ…‹ç‚º completed
                    completed = sum(1 for t in chunks_text if t is not None)
                    if task_id:
                        with tasks_lock:
                            if task_id in transcription_tasks:
                                chunks = transcription_tasks[task_id].get("chunks", [])
                                if chunk_idx - 1 < len(chunks):
                                    chunks[chunk_idx - 1]["status"] = "completed"
                                    chunks[chunk_idx - 1]["completed_at"] = get_current_time()
                                    if chunk_duration:
                                        chunks[chunk_idx - 1]["duration_seconds"] = round(chunk_duration, 1)
                        update_task_status(task_id, {
                            "progress": f"æ­£åœ¨è½‰éŒ„éŸ³è¨Š... ({completed}/{num_chunks} æ®µå®Œæˆ)",
                            "completed_chunks": completed
                        })

                except Exception as e:
                    print(f"   âŒ ç¬¬ {chunk_idx} æ®µè½‰éŒ„å¤±æ•—ï¼š{e}")
                    raise

    finally:
        # æ¸…ç†æ‰€æœ‰è‡¨æ™‚æª”æ¡ˆ
        for _, _, _, temp_path in chunk_info_list:
            try:
                if temp_path.exists():
                    temp_path.unlink()
            except Exception as e:
                print(f"âš ï¸ æ¸…ç†è‡¨æ™‚æª”æ¡ˆå¤±æ•—ï¼š{e}")

        # ç¢ºä¿ diarization executor è¢«æ­£ç¢ºé—œé–‰
        if diarization_executor:
            try:
                diarization_executor.shutdown(wait=False, cancel_futures=True)
                print(f"âœ… Diarization é€²ç¨‹å·²é—œé–‰")
            except Exception as e:
                print(f"âš ï¸ é—œé–‰ diarization executor å¤±æ•—ï¼š{e}")

        # æ¸…ç† diarization é€²ç¨‹è¨˜éŒ„
        if task_id:
            with tasks_lock:
                task_diarization_processes.pop(task_id, None)

    print("âœ… æ‰€æœ‰éŸ³æª”ç‰‡æ®µè½‰éŒ„å®Œæˆ")

    # ç¬¬ä¸‰æ­¥ï¼šå¦‚æœå•Ÿç”¨äº† diarizationï¼Œç­‰å¾…èªªè©±è€…è¾¨è­˜å®Œæˆä¸¦åˆä½µè³‡è¨Š
    if diarization_future and all_segments:
        print(f"â³ ç­‰å¾…èªªè©±è€…è¾¨è­˜å®Œæˆ...")
        if task_id:
            update_task_status(task_id, {"progress": "ç­‰å¾…èªªè©±è€…è¾¨è­˜å®Œæˆ..."})

        try:
            # ç­‰å¾… diarization å®Œæˆä¸¦å–å¾—çµæœ
            diarization_segments = diarization_future.result()
            if diarization_executor:
                diarization_executor.shutdown(wait=True)

            # è¨ˆç®— diarization å¯¦éš›è€—æ™‚
            if diarization_start_time:
                diarization_duration = (datetime.now(TZ_UTC8) - diarization_start_time).total_seconds()
            else:
                diarization_duration = 0

            if diarization_segments:
                num_speakers = len(set(s['speaker'] for s in diarization_segments))
                print(f"âœ… èªªè©±è€…è¾¨è­˜å®Œæˆï¼Œè­˜åˆ¥åˆ° {num_speakers} ä½èªªè©±è€… (è€—æ™‚ {format_duration(diarization_duration)})")
                if task_id:
                    update_task_status(task_id, {
                        "progress": f"èªªè©±è€…è¾¨è­˜å®Œæˆ ({num_speakers} ä½èªªè©±è€…ï¼Œè€—æ™‚ {format_duration(diarization_duration)})",
                        "diarization_status": "completed",
                        "diarization_num_speakers": num_speakers,
                        "diarization_duration_seconds": round(diarization_duration, 1)
                    })

                # åˆä½µèªªè©±è€…è³‡è¨Šèˆ‡è½‰éŒ„æ–‡å­—
                print(f"ğŸ”— æ­£åœ¨åˆä½µèªªè©±è€…è³‡è¨Šèˆ‡è½‰éŒ„æ–‡å­—...")
                if task_id:
                    update_task_status(task_id, {"progress": "æ­£åœ¨åˆä½µèªªè©±è€…è³‡è¨Š..."})

                # æŒ‰æ™‚é–“æ’åº segmentsï¼ˆç¢ºä¿é †åºæ­£ç¢ºï¼‰
                all_segments.sort(key=lambda s: s["start"])

                final_text = merge_transcription_with_diarization(all_segments, diarization_segments)
                print(f"âœ… èªªè©±è€…è³‡è¨Šåˆä½µå®Œæˆ")
                return final_text
            else:
                print(f"âš ï¸  èªªè©±è€…è¾¨è­˜å¤±æ•—ï¼Œè¿”å›ç´”æ–‡å­—è½‰éŒ„")
                if task_id:
                    update_task_status(task_id, {
                        "diarization_status": "failed"
                    })
                return " ".join(chunks_text)

        except Exception as e:
            print(f"âš ï¸  ç­‰å¾…èªªè©±è€…è¾¨è­˜æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            print(f"   å°‡è¿”å›ç´”æ–‡å­—è½‰éŒ„")
            if task_id:
                update_task_status(task_id, {
                    "diarization_status": "failed"
                })
            return " ".join(chunks_text)
    else:
        # æ²’æœ‰ diarizationï¼Œè¿”å›ç´”æ–‡å­—
        return " ".join(chunks_text)


def detect_language_with_llm(text: str, provider: str = "gemini") -> str:
    """ä½¿ç”¨ LLM è‡ªå‹•è¾¨è­˜æ–‡å­—èªè¨€

    Args:
        text: è¦è¾¨è­˜çš„æ–‡å­—ï¼ˆå»ºè­°ä½¿ç”¨å‰å¹¾ç™¾å­—å³å¯ï¼‰
        provider: ä½¿ç”¨çš„ LLM æä¾›è€… (gemini/openai)

    Returns:
        èªè¨€ä»£ç¢¼ (zh/en/ja/ko ç­‰)
    """
    # åªå–å‰ 100 å­—é€²è¡Œè¾¨è­˜ä»¥ç¯€çœæˆæœ¬
    sample_text = text[:100]

    prompt = f"""Please identify the primary language of the following text and respond with ONLY the language code (zh/en/ja/ko/es/fr/de/etc.).

Text:
{sample_text}

Language code:"""

    try:
        if provider == "gemini":
            result = call_gemini_with_retry(prompt).strip().lower()
        else:  # openai
            from openai import OpenAI
            client = OpenAI()
            resp = client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[
                    {"role": "system", "content": "You are a language detection assistant. Respond only with language codes."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
            )
            result = resp.choices[0].message.content.strip().lower()

        # é©—è­‰çµæœæ˜¯å¦ç‚ºæœ‰æ•ˆçš„èªè¨€ä»£ç¢¼
        valid_codes = ["zh", "en", "ja", "ko", "es", "fr", "de", "it", "pt", "ru", "ar", "hi", "th", "vi"]
        if result in valid_codes:
            return result

        # å¦‚æœä¸æ˜¯æ¨™æº–ä»£ç¢¼ï¼Œå˜—è©¦æ˜ å°„å¸¸è¦‹å›æ‡‰
        if "chinese" in result or "ä¸­æ–‡" in result:
            return "zh"
        elif "english" in result:
            return "en"
        elif "japanese" in result or "æ—¥æœ¬" in result:
            return "ja"
        elif "korean" in result or "éŸ“" in result:
            return "ko"

        # é è¨­è¿”å›ä¸­æ–‡
        print(f"âš ï¸ èªè¨€è¾¨è­˜çµæœä¸æ˜ç¢º: {result}ï¼Œé è¨­ä½¿ç”¨ä¸­æ–‡")
        return "zh"

    except Exception as e:
        print(f"âš ï¸ èªè¨€è¾¨è­˜å¤±æ•—: {e}ï¼Œé è¨­ä½¿ç”¨ä¸­æ–‡")
        return "zh"


def get_punctuation_prompt(language: str, text: str) -> tuple[str, str]:
    """æ ¹æ“šèªè¨€ç”Ÿæˆé©ç•¶çš„æ¨™é»æç¤ºèª

    Args:
        language: èªè¨€ä»£ç¢¼ (zh/en/ja/ko/auto)
        text: è¦è™•ç†çš„æ–‡å­—

    Returns:
        (system_message, user_message) å…ƒçµ„
    """
    # å¦‚æœæ˜¯è‡ªå‹•åµæ¸¬ï¼Œå…ˆè¾¨è­˜èªè¨€
    if language == "auto":
        language = detect_language_with_llm(text, provider="gemini")
        print(f"ğŸ” è‡ªå‹•è¾¨è­˜èªè¨€: {language}")

    if language == "zh":
        system_msg = "ä½ æ˜¯åš´è¬¹çš„é€å­—ç¨¿æ½¤é£¾åŠ©æ‰‹ï¼Œåªåšæ¨™é»èˆ‡åˆ†æ®µã€‚"
        user_msg = (
            "è«‹å°‡ä»¥ä¸‹ã€ä¸­æ–‡é€å­—ç¨¿ã€åŠ ä¸Šé©ç•¶æ¨™é»ç¬¦è™Ÿä¸¦åˆç†åˆ†æ®µã€‚"
            "ä¸è¦çœç•¥æˆ–æ·»åŠ å…§å®¹ï¼Œä¸è¦æ„è­¯ï¼Œä¿ç•™å›ºæœ‰åè©èˆ‡æ•¸å­—ã€‚"
            f"è¼¸å‡ºç´”æ–‡å­—å³å¯ï¼š\n\n{text}"
        )
    elif language == "en":
        system_msg = "You are a precise transcript editor. Only add punctuation and paragraphing."
        user_msg = (
            "Please add appropriate punctuation and paragraphing to the following English transcript. "
            "Do not omit or add content, do not paraphrase, preserve proper nouns and numbers. "
            f"Output plain text only:\n\n{text}"
        )
    elif language == "ja":
        system_msg = "ã‚ãªãŸã¯æ­£ç¢ºãªæ–‡å­—èµ·ã“ã—ç·¨é›†è€…ã§ã™ã€‚å¥èª­ç‚¹ã¨æ®µè½åˆ†ã‘ã®ã¿ã‚’è¡Œã„ã¾ã™ã€‚"
        user_msg = (
            "ä»¥ä¸‹ã®æ—¥æœ¬èªæ–‡å­—èµ·ã“ã—ã«é©åˆ‡ãªå¥èª­ç‚¹ã¨æ®µè½ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚"
            "å†…å®¹ã®çœç•¥ã‚„è¿½åŠ ã¯ã›ãšã€æ„è¨³ã›ãšã€å›ºæœ‰åè©ã¨æ•°å­—ã¯ãã®ã¾ã¾ä¿æŒã—ã¦ãã ã•ã„ã€‚"
            f"ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã¿å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š\n\n{text}"
        )
    elif language == "ko":
        system_msg = "ë‹¹ì‹ ì€ ì •í™•í•œ ì „ì‚¬ í¸ì§‘ìì…ë‹ˆë‹¤. êµ¬ë‘ì ê³¼ ë‹¨ë½ ë‚˜ëˆ„ê¸°ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤."
        user_msg = (
            "ë‹¤ìŒ í•œêµ­ì–´ ì „ì‚¬ì— ì ì ˆí•œ êµ¬ë‘ì ê³¼ ë‹¨ë½ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”. "
            "ë‚´ìš©ì„ ìƒëµí•˜ê±°ë‚˜ ì¶”ê°€í•˜ì§€ ë§ê³ , ì˜ì—­í•˜ì§€ ë§ê³ , ê³ ìœ ëª…ì‚¬ì™€ ìˆ«ìëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”. "
            f"ì¼ë°˜ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”:\n\n{text}"
        )
    else:
        # å…¶ä»–èªè¨€ä½¿ç”¨è‹±æ–‡æç¤º
        system_msg = "You are a precise transcript editor. Only add punctuation and paragraphing."
        user_msg = (
            f"Please add appropriate punctuation and paragraphing to the following transcript. "
            "Do not omit or add content, do not paraphrase, preserve proper nouns and numbers. "
            f"Output plain text only:\n\n{text}"
        )

    return system_msg, user_msg


def punctuate_with_openai(text: str, language: str = "zh") -> str:
    """ç”¨ OpenAI å¹«é€å­—ç¨¿åŠ æ¨™é»èˆ‡åˆ†æ®µï¼ˆæ”¯æ´å¤šèªè¨€ï¼‰

    Args:
        text: è¦åŠ æ¨™é»çš„æ–‡å­—
        language: èªè¨€ä»£ç¢¼ (zh/en/ja/ko/auto)ï¼Œauto æœƒè‡ªå‹•è¾¨è­˜
    """
    from openai import OpenAI
    client = OpenAI()

    # ç²å–å°æ‡‰èªè¨€çš„æç¤ºèª
    system_msg, user_msg = get_punctuation_prompt(language, text)

    resp = client.chat.completions.create(
        model=OPENAI_MODEL,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_msg},
        ],
        temperature=0.2,
    )
    return resp.choices[0].message.content.strip()


def call_gemini_with_retry(prompt: str, max_retries: int = None) -> str:
    """èª¿ç”¨ Gemini APIï¼Œæ”¯æ´è‡ªå‹•é‡è©¦å’Œ Key åˆ‡æ›"""
    import google.generativeai as genai

    if max_retries is None:
        max_retries = len(GOOGLE_API_KEYS)

    last_error = None

    for attempt in range(max_retries):
        try:
            # ç²å–ä¸‹ä¸€å€‹ API Key
            api_key = get_next_google_api_key()
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel(GEMINI_MODEL)

            # èª¿ç”¨ API
            resp = model.generate_content(
                [{"role": "user", "parts": [prompt]}],
                generation_config={"temperature": 0.2}
            )

            return (resp.text or "").strip()

        except Exception as e:
            last_error = e
            error_msg = str(e)
            print(f"âš ï¸ Google API Key èª¿ç”¨å¤±æ•— (å˜—è©¦ {attempt + 1}/{max_retries}): {error_msg}")

            # å¦‚æœé‚„æœ‰ key å¯ç”¨ï¼Œç¹¼çºŒå˜—è©¦
            if attempt < max_retries - 1:
                print(f"ğŸ”„ åˆ‡æ›åˆ°ä¸‹ä¸€å€‹ API Key...")
                continue
            else:
                print(f"âŒ æ‰€æœ‰ API Keys éƒ½å·²å˜—è©¦ï¼Œå¤±æ•—")
                raise RuntimeError(f"æ‰€æœ‰ Google API Keys éƒ½èª¿ç”¨å¤±æ•—ã€‚æœ€å¾ŒéŒ¯èª¤: {error_msg}") from last_error

    raise RuntimeError("ç„¡æ³•èª¿ç”¨ Gemini API") from last_error


def get_chunked_punctuation_prompt(language: str, chunk_text: str, chunk_idx: int, total_chunks: int) -> tuple[str, str]:
    """ç‚ºé•·æ–‡æœ¬åˆ†æ®µç”Ÿæˆæç¤ºèª

    Args:
        language: èªè¨€ä»£ç¢¼
        chunk_text: ç•¶å‰åˆ†æ®µæ–‡å­—
        chunk_idx: ç•¶å‰åˆ†æ®µç´¢å¼•ï¼ˆå¾1é–‹å§‹ï¼‰
        total_chunks: ç¸½åˆ†æ®µæ•¸

    Returns:
        (system_message, user_message) å…ƒçµ„
    """
    if language == "zh":
        system_msg = "ä½ æ˜¯åš´è¬¹çš„é€å­—ç¨¿æ½¤é£¾åŠ©æ‰‹ã€‚åªåšã€ä¸­æ–‡æ¨™é»è£œå…¨èˆ‡åˆç†åˆ†æ®µã€ï¼Œä¸è¦çœç•¥æˆ–æ·»åŠ å…§å®¹ï¼Œä¸è¦æ„è­¯ï¼Œéå¿…è¦ä¸è¦ç”¨åˆªç¯€è™Ÿï¼Œä¿ç•™å›ºæœ‰åè©èˆ‡æ•¸å­—ã€‚"
        if chunk_idx == 1:
            user_msg = f"è«‹ç‚ºä»¥ä¸‹ä¸­æ–‡é€å­—ç¨¿åŠ ä¸Šé©ç•¶æ¨™é»ä¸¦åˆ†æ®µï¼ˆé€™æ˜¯ç¬¬ 1 æ®µï¼‰ï¼š\n\n{chunk_text}"
        elif chunk_idx == total_chunks:
            user_msg = f"è«‹ç‚ºä»¥ä¸‹ä¸­æ–‡é€å­—ç¨¿åŠ ä¸Šé©ç•¶æ¨™é»ä¸¦åˆ†æ®µï¼ˆé€™æ˜¯æœ€å¾Œä¸€æ®µï¼Œæ¥çºŒå‰æ–‡ï¼‰ï¼š\n\n{chunk_text}"
        else:
            user_msg = f"è«‹ç‚ºä»¥ä¸‹ä¸­æ–‡é€å­—ç¨¿åŠ ä¸Šé©ç•¶æ¨™é»ä¸¦åˆ†æ®µï¼ˆé€™æ˜¯ç¬¬ {chunk_idx} æ®µï¼Œæ¥çºŒå‰æ–‡ï¼‰ï¼š\n\n{chunk_text}"
    elif language == "en":
        system_msg = "You are a precise transcript editor. Only add punctuation and paragraphing. Do not omit or add content, do not paraphrase, preserve proper nouns and numbers."
        if chunk_idx == 1:
            user_msg = f"Add punctuation and paragraphing to this English transcript (part 1):\n\n{chunk_text}"
        elif chunk_idx == total_chunks:
            user_msg = f"Add punctuation and paragraphing to this English transcript (final part, continuing from previous):\n\n{chunk_text}"
        else:
            user_msg = f"Add punctuation and paragraphing to this English transcript (part {chunk_idx}, continuing from previous):\n\n{chunk_text}"
    elif language == "ja":
        system_msg = "ã‚ãªãŸã¯æ­£ç¢ºãªæ–‡å­—èµ·ã“ã—ç·¨é›†è€…ã§ã™ã€‚å¥èª­ç‚¹ã¨æ®µè½åˆ†ã‘ã®ã¿ã‚’è¡Œã„ã¾ã™ã€‚å†…å®¹ã®çœç•¥ã‚„è¿½åŠ ã¯ã›ãšã€æ„è¨³ã›ãšã€å›ºæœ‰åè©ã¨æ•°å­—ã¯ãã®ã¾ã¾ä¿æŒã—ã¦ãã ã•ã„ã€‚"
        if chunk_idx == 1:
            user_msg = f"ä»¥ä¸‹ã®æ—¥æœ¬èªæ–‡å­—èµ·ã“ã—ã«å¥èª­ç‚¹ã¨æ®µè½ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ï¼ˆç¬¬1éƒ¨åˆ†ï¼‰ï¼š\n\n{chunk_text}"
        elif chunk_idx == total_chunks:
            user_msg = f"ä»¥ä¸‹ã®æ—¥æœ¬èªæ–‡å­—èµ·ã“ã—ã«å¥èª­ç‚¹ã¨æ®µè½ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ï¼ˆæœ€å¾Œã®éƒ¨åˆ†ã€å‰ã®ç¶šãï¼‰ï¼š\n\n{chunk_text}"
        else:
            user_msg = f"ä»¥ä¸‹ã®æ—¥æœ¬èªæ–‡å­—èµ·ã“ã—ã«å¥èª­ç‚¹ã¨æ®µè½ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ï¼ˆç¬¬{chunk_idx}éƒ¨åˆ†ã€å‰ã®ç¶šãï¼‰ï¼š\n\n{chunk_text}"
    elif language == "ko":
        system_msg = "ë‹¹ì‹ ì€ ì •í™•í•œ ì „ì‚¬ í¸ì§‘ìì…ë‹ˆë‹¤. êµ¬ë‘ì ê³¼ ë‹¨ë½ ë‚˜ëˆ„ê¸°ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤. ë‚´ìš©ì„ ìƒëµí•˜ê±°ë‚˜ ì¶”ê°€í•˜ì§€ ë§ê³ , ì˜ì—­í•˜ì§€ ë§ê³ , ê³ ìœ ëª…ì‚¬ì™€ ìˆ«ìëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”."
        if chunk_idx == 1:
            user_msg = f"ë‹¤ìŒ í•œêµ­ì–´ ì „ì‚¬ì— êµ¬ë‘ì ê³¼ ë‹¨ë½ì„ ì¶”ê°€í•´ì£¼ì„¸ìš” (1ë¶€):\n\n{chunk_text}"
        elif chunk_idx == total_chunks:
            user_msg = f"ë‹¤ìŒ í•œêµ­ì–´ ì „ì‚¬ì— êµ¬ë‘ì ê³¼ ë‹¨ë½ì„ ì¶”ê°€í•´ì£¼ì„¸ìš” (ë§ˆì§€ë§‰ ë¶€ë¶„, ì´ì „ ê³„ì†):\n\n{chunk_text}"
        else:
            user_msg = f"ë‹¤ìŒ í•œêµ­ì–´ ì „ì‚¬ì— êµ¬ë‘ì ê³¼ ë‹¨ë½ì„ ì¶”ê°€í•´ì£¼ì„¸ìš” ({chunk_idx}ë¶€, ì´ì „ ê³„ì†):\n\n{chunk_text}"
    else:
        # å…¶ä»–èªè¨€ä½¿ç”¨è‹±æ–‡æç¤º
        system_msg = "You are a precise transcript editor. Only add punctuation and paragraphing. Do not omit or add content, do not paraphrase, preserve proper nouns and numbers."
        if chunk_idx == 1:
            user_msg = f"Add punctuation and paragraphing to this transcript (part 1):\n\n{chunk_text}"
        elif chunk_idx == total_chunks:
            user_msg = f"Add punctuation and paragraphing to this transcript (final part, continuing from previous):\n\n{chunk_text}"
        else:
            user_msg = f"Add punctuation and paragraphing to this transcript (part {chunk_idx}, continuing from previous):\n\n{chunk_text}"

    return system_msg, user_msg


def punctuate_with_gemini(text: str, chunk_size: int = None, task_id: str = None, language: str = "zh") -> str:
    """ç”¨ Google Gemini å¹«é€å­—ç¨¿åŠ æ¨™é»èˆ‡åˆ†æ®µï¼ˆæ”¯æ´é•·æ–‡æœ¬åˆ†æ®µè™•ç†ã€å¤šèªè¨€ï¼‰

    Args:
        text: è¦åŠ æ¨™é»çš„æ–‡å­—
        chunk_size: åˆ†æ®µå¤§å°ï¼ˆå­—å…ƒæ•¸ï¼‰ï¼ŒNone å‰‡æ ¹æ“šèªè¨€è‡ªå‹•æ±ºå®š
        task_id: ä»»å‹™ IDï¼ˆç”¨æ–¼æ›´æ–°é€²åº¦ï¼‰
        language: èªè¨€ä»£ç¢¼ (zh/en/ja/ko/auto)ï¼Œauto æœƒè‡ªå‹•è¾¨è­˜
    """
    if not GOOGLE_API_KEYS:
        raise RuntimeError("æœªè¨­å®šä»»ä½• GOOGLE_API_KEY")

    # å¦‚æœæ˜¯è‡ªå‹•åµæ¸¬ï¼Œå…ˆè¾¨è­˜èªè¨€
    actual_language = language
    if language == "auto":
        actual_language = detect_language_with_llm(text, provider="gemini")
        print(f"ğŸ” è‡ªå‹•è¾¨è­˜èªè¨€: {actual_language}")

    # æ ¹æ“šèªè¨€æ±ºå®šåˆé©çš„åˆ†æ®µå¤§å°
    if chunk_size is None:
        if actual_language in ['en', 'es', 'fr', 'de', 'it', 'pt']:
            # è‹±æ–‡ç­‰å­—æ¯èªè¨€ï¼šä½¿ç”¨è¼ƒå¤§çš„å­—å…ƒæ•¸ï¼ˆå› ç‚ºå–®è©åŒ…å«ç©ºæ ¼ï¼Œå­—å…ƒæ•¸è¼ƒå¤šï¼‰
            chunk_size = 8000  # ç´„ 1300-1600 å€‹è‹±æ–‡å–®è©
        else:
            # ä¸­æ–‡ã€æ—¥æ–‡ã€éŸ“æ–‡ç­‰ï¼šä½¿ç”¨æ¨™æº–å­—å…ƒæ•¸
            chunk_size = 3000  # ç´„ 3000 å€‹å­—ç¬¦
        print(f"ğŸ“ æ ¹æ“šèªè¨€ '{actual_language}' è¨­å®šåˆ†æ®µå¤§å°ï¼š{chunk_size} å­—å…ƒ")

    # å¦‚æœæ–‡æœ¬ä¸é•·ï¼Œç›´æ¥è™•ç†
    if len(text) <= chunk_size:
        system_msg, user_msg = get_punctuation_prompt(actual_language, text)
        prompt = system_msg + "\n\n" + user_msg
        return call_gemini_with_retry(prompt)

    # é•·æ–‡æœ¬ï¼šåˆ†æ®µè™•ç†
    print(f"ğŸ“Š æ–‡æœ¬é•·åº¦ {len(text)} å­—ï¼Œå°‡åˆ†æ®µè™•ç†ï¼ˆæ¯æ®µç´„ {chunk_size} å­—ï¼‰...")
    chunks = []
    start = 0

    # æ ¹æ“šèªè¨€é¸æ“‡åˆ†æ®µæ¨™è¨˜
    split_markers = 'ã€‚ï¼Ÿï¼\n' if actual_language in ['zh', 'ja'] else '.?!\n'

    while start < len(text):
        end = start + chunk_size
        if end < len(text):
            for i in range(end, max(start + chunk_size // 2, end - 200), -1):
                if text[i] in split_markers:
                    end = i + 1
                    break
        chunks.append(text[start:end])
        start = end

    print(f"ğŸ”„ å…±åˆ†ç‚º {len(chunks)} æ®µè™•ç†...")

    # è¨˜éŒ„æ¨™é»åˆ†æ®µç¸½æ•¸
    if task_id:
        update_task_status(task_id, {
            "punctuation_total_chunks": len(chunks),
            "punctuation_current_chunk": 0
        })

    results = []

    for idx, chunk in enumerate(chunks, 1):
        print(f"   è™•ç†ç¬¬ {idx}/{len(chunks)} æ®µ...")

        # æª¢æŸ¥æ˜¯å¦å·²è¢«å–æ¶ˆ
        if task_id and task_cancelled.get(task_id, False):
            raise RuntimeError("ä»»å‹™å·²è¢«ä½¿ç”¨è€…å–æ¶ˆ")

        # æ›´æ–°ä»»å‹™é€²åº¦ï¼ˆé¡¯ç¤ºç•¶å‰æ¨™é»è™•ç†æ®µæ•¸ï¼‰
        if task_id:
            update_task_status(task_id, {
                "punctuation_current_chunk": idx,
                "progress": f"æ­£åœ¨æ·»åŠ æ¨™é»ç¬¦è™Ÿ... (ç¬¬ {idx}/{len(chunks)} æ®µ)"
            })

        # ä½¿ç”¨èªè¨€æ„ŸçŸ¥çš„æç¤ºèª
        system_msg, user_msg = get_chunked_punctuation_prompt(actual_language, chunk, idx, len(chunks))

        prompt = system_msg + "\n\n" + user_msg
        result = call_gemini_with_retry(prompt)
        results.append(result)

    print("âœ… æ‰€æœ‰æ®µè½è™•ç†å®Œæˆï¼Œæ­£åœ¨åˆä½µ...")
    return "\n\n".join(results)


def process_transcription_task(
    task_id: str,
    temp_audio_path: Path,
    filename: str,
    punct_provider: str,
    chunk_audio: bool,
    chunk_minutes: int,
    diarize: bool = False,
    max_speakers: Optional[int] = None,
    language: str = "zh"
):
    """
    åœ¨èƒŒæ™¯ç·šç¨‹ä¸­åŸ·è¡Œè½‰éŒ„ä»»å‹™
    é€™å€‹å‡½æ•¸æœƒæ›´æ–°ä»»å‹™ç‹€æ…‹ï¼Œä¸¦è™•ç†æ‰€æœ‰è½‰éŒ„é‚è¼¯

    æ³¨æ„ï¼šdiarization åƒ…åœ¨éåˆ†æ®µæ¨¡å¼ä¸‹å¯ç”¨ï¼Œåˆ†æ®µæ¨¡å¼æœƒå¿½ç•¥æ­¤åƒæ•¸
    max_speakers: æœ€å¤§è¬›è€…äººæ•¸ï¼ˆå¯é¸ï¼Œ2-10ï¼‰
    """
    temp_dir = temp_audio_path.parent

    try:
        # è¨˜éŒ„æš«å­˜ç›®éŒ„
        with tasks_lock:
            task_temp_dirs[task_id] = temp_dir

        # è¨˜éŒ„é–‹å§‹è™•ç†æ™‚é–“
        start_time = datetime.now(TZ_UTC8)

        # æ›´æ–°ç‹€æ…‹ï¼šè™•ç†ä¸­
        update_task_status(task_id, {
            "status": "processing",
            "progress": "æ­£åœ¨è½‰æ›éŸ³è¨Šæ ¼å¼...",
            "started_at": start_time.strftime("%Y-%m-%d %H:%M:%S")
        })

        # æª¢æŸ¥æ˜¯å¦å·²è¢«å–æ¶ˆ
        if task_cancelled.get(task_id, False):
            raise RuntimeError("ä»»å‹™å·²è¢«ä½¿ç”¨è€…å–æ¶ˆ")

        # è½‰æ›ç‚º WAV
        wav_path = temp_dir / "input.wav"
        print(f"ğŸ”„ [{task_id}] è½‰æª”ç‚º WAV...")
        AudioSegment.from_file(temp_audio_path).export(wav_path, format="wav")

        # æ¨™è¨˜éŸ³è¨Šè½‰æª”å®Œæˆ
        update_task_status(task_id, {
            "audio_converted": True,
            "progress": "éŸ³è¨Šè½‰æª”å®Œæˆï¼Œæº–å‚™è½‰éŒ„..."
        })

        # æª¢æŸ¥æ˜¯å¦å·²è¢«å–æ¶ˆ
        if task_cancelled.get(task_id, False):
            raise RuntimeError("ä»»å‹™å·²è¢«ä½¿ç”¨è€…å–æ¶ˆ")

        # åŸ·è¡Œè½‰éŒ„
        update_task_status(task_id, {"progress": "æ­£åœ¨è½‰éŒ„éŸ³è¨Š..."})

        if chunk_audio:
            # åˆ†æ®µæ¨¡å¼ï¼šç¾åœ¨æ”¯æ´ diarizationï¼ˆå…ˆå°å®Œæ•´éŸ³æª”åšèªªè©±è€…è¾¨è­˜ï¼Œå†åˆ†æ®µè½‰éŒ„ï¼‰
            chunk_duration_ms = chunk_minutes * 60 * 1000
            raw_text = transcribe_audio_in_chunks(
                wav_path,
                whisper_model,
                chunk_duration_ms,
                task_id=task_id,
                diarize=diarize,
                max_speakers=max_speakers,
                language=language
            )
        else:
            # éåˆ†æ®µæ¨¡å¼ï¼šå¯ä»¥ä½¿ç”¨ diarization
            if diarize and diarization_pipeline:
                print(f"ğŸ”Š [{task_id}] å•Ÿç”¨ speaker diarization...")

                # åŸ·è¡Œ speaker diarization
                diarization_start = datetime.now(TZ_UTC8)
                update_task_status(task_id, {
                    "progress": "æ­£åœ¨åˆ†æèªªè©±è€…...",
                    "diarization_status": "running"
                })
                diarization_segments = perform_diarization(wav_path, max_speakers=max_speakers)
                diarization_duration = (datetime.now(TZ_UTC8) - diarization_start).total_seconds()

                # åŸ·è¡Œè½‰éŒ„ï¼ˆå¸¶æ™‚é–“æˆ³ï¼‰
                update_task_status(task_id, {"progress": "æ­£åœ¨è½‰éŒ„éŸ³è¨Šï¼ˆå¸¶æ™‚é–“æˆ³ï¼‰..."})
                transcription_segments = transcribe_with_timestamps(whisper_model, wav_path, language=language)

                # åˆä½µçµæœ
                if diarization_segments:
                    num_speakers = len(set(s['speaker'] for s in diarization_segments))
                    update_task_status(task_id, {
                        "progress": "æ­£åœ¨åˆä½µèªªè©±è€…è³‡è¨Š...",
                        "diarization_status": "completed",
                        "diarization_num_speakers": num_speakers,
                        "diarization_duration_seconds": round(diarization_duration, 1)
                    })
                    raw_text = merge_transcription_with_diarization(
                        transcription_segments,
                        diarization_segments
                    )
                else:
                    # diarization å¤±æ•—ï¼Œå›é€€åˆ°ç´”æ–‡å­—
                    update_task_status(task_id, {
                        "diarization_status": "failed"
                    })
                    raw_text = " ".join(seg["text"] for seg in transcription_segments)
            else:
                print(f"ğŸ“ [{task_id}] é–‹å§‹è½‰é€å­—ç¨¿...")
                raw_text = transcribe_single_chunk(whisper_model, wav_path, language=language)

        # æª¢æŸ¥æ˜¯å¦å·²è¢«å–æ¶ˆ
        if task_cancelled.get(task_id, False):
            raise RuntimeError("ä»»å‹™å·²è¢«ä½¿ç”¨è€…å–æ¶ˆ")

        print(f"âœ… [{task_id}] è½‰éŒ„å®Œæˆï¼ˆ{len(raw_text)} å­—ï¼‰")

        # åŠ æ¨™é»
        final_text = raw_text
        if punct_provider == "gemini":
            update_task_status(task_id, {
                "punctuation_started": True,
                "progress": "æ­£åœ¨æ·»åŠ æ¨™é»ç¬¦è™Ÿï¼ˆGeminiï¼‰..."
            })
            print(f"âœ¨ [{task_id}] ä½¿ç”¨ Gemini åŠ æ¨™é»èˆ‡åˆ†æ®µ...")
            final_text = punctuate_with_gemini(raw_text, task_id=task_id, language=language)
            update_task_status(task_id, {"punctuation_completed": True})
        elif punct_provider == "openai":
            update_task_status(task_id, {
                "punctuation_started": True,
                "progress": "æ­£åœ¨æ·»åŠ æ¨™é»ç¬¦è™Ÿï¼ˆOpenAIï¼‰..."
            })
            if not os.getenv("OPENAI_API_KEY"):
                raise ValueError("æœªè¨­å®š OPENAI_API_KEY")
            print(f"âœ¨ [{task_id}] ä½¿ç”¨ OpenAI åŠ æ¨™é»èˆ‡åˆ†æ®µ...")
            final_text = punctuate_with_openai(raw_text, language=language)
            update_task_status(task_id, {"punctuation_completed": True})

        print(f"ğŸ‰ [{task_id}] è™•ç†å®Œæˆï¼")

        # ä¿å­˜æ–‡å­—æª”åˆ° output/ ç›®éŒ„
        update_task_status(task_id, {"progress": "æ­£åœ¨ä¿å­˜çµæœ..."})
        timestamp = datetime.now(TZ_UTC8).strftime("%Y%m%d_%H%M%S")
        safe_filename = Path(filename).stem.replace(" ", "_")
        output_filename = f"{safe_filename}_{timestamp}_transcript.txt"
        permanent_output = OUTPUT_DIR / output_filename
        permanent_output.write_text(final_text, encoding="utf-8")
        print(f"ğŸ’¾ [{task_id}] æ–‡å­—æª”å·²ä¿å­˜ï¼š{permanent_output.relative_to(OUTPUT_DIR.parent)}")

        # è¨ˆç®—ç¸½è™•ç†æ™‚é–“
        end_time = datetime.now(TZ_UTC8)
        duration_seconds = (end_time - start_time).total_seconds()

        # æ›´æ–°ç‹€æ…‹ï¼šå®Œæˆ
        update_task_status(task_id, {
            "status": "completed",
            "progress": "è½‰éŒ„å®Œæˆ",
            "result_file": str(permanent_output),
            "result_filename": output_filename,
            "text_length": len(final_text),
            "completed_at": get_current_time(),
            "duration_seconds": round(duration_seconds, 1)
        })

        print(f"â±ï¸ [{task_id}] ç¸½è™•ç†æ™‚é–“ï¼š{format_duration(duration_seconds)}")

    except Exception as e:
        # æª¢æŸ¥æ˜¯å¦ç‚ºå–æ¶ˆæ“ä½œ
        is_cancelled = "å–æ¶ˆ" in str(e)

        if is_cancelled:
            print(f"ğŸ›‘ [{task_id}] ä»»å‹™å·²è¢«ä½¿ç”¨è€…å–æ¶ˆ")
            update_task_status(task_id, {
                "status": "cancelled",
                "progress": "ä»»å‹™å·²å–æ¶ˆ",
                "error": "ä½¿ç”¨è€…å–æ¶ˆäº†ä»»å‹™"
            })
        else:
            # æ›´æ–°ç‹€æ…‹ï¼šå¤±æ•—
            print(f"âŒ [{task_id}] éŒ¯èª¤ï¼š{e}")
            update_task_status(task_id, {
                "status": "failed",
                "progress": f"éŒ¯èª¤ï¼š{str(e)}",
                "error": str(e)
            })

    finally:
        # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
        cleanup_temp_dir(temp_dir)

        # æ¸…ç†ä»»å‹™ç›¸é—œè¨˜éŒ„
        with tasks_lock:
            task_temp_dirs.pop(task_id, None)
            task_cancelled.pop(task_id, None)
            task_diarization_processes.pop(task_id, None)


# ---------- API Endpoints ----------

@app.on_event("startup")
async def startup_event():
    """å•Ÿå‹•æ™‚è¼‰å…¥ Whisper æ¨¡å‹å’Œä»»å‹™è¨˜éŒ„"""
    global whisper_model, current_model_name

    # è¼‰å…¥ä»»å‹™è¨˜éŒ„
    print(f"ğŸ“‚ æ­£åœ¨è¼‰å…¥ä»»å‹™è¨˜éŒ„...")
    load_tasks_from_disk()

    # è¼‰å…¥ Faster-Whisper æ¨¡å‹
    current_model_name = DEFAULT_MODEL
    print(f"ğŸ™ æ­£åœ¨è¼‰å…¥ Faster-Whisper æ¨¡å‹ï¼š{current_model_name}...")
    print(f"ğŸ”§ é…ç½®ï¼šdevice=auto, compute_type=int8ï¼ˆé‡å° M1 å„ªåŒ–ï¼‰")
    whisper_model = WhisperModel(
        current_model_name,
        device="auto",  # è‡ªå‹•é¸æ“‡ CPU
        compute_type="int8",  # ä½¿ç”¨ INT8 é‡åŒ–ï¼Œç¯€çœè¨˜æ†¶é«”ä¸¦æå‡é€Ÿåº¦
        cpu_threads=1,  # æ¯å€‹æ¨ç†ä»»å‹™ç”¨ 1 ç·šç¨‹ï¼ˆé¿å…èˆ‡ diarization ç«¶çˆ­ï¼‰
        num_workers=4  # å…è¨± 4 å€‹ä»»å‹™åŒæ™‚æ¨ç†
    )
    print(f"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼Œæœå‹™å·²å°±ç·’ï¼")

    # è¼‰å…¥ Speaker Diarization æ¨¡å‹ï¼ˆå¯é¸ï¼‰
    global diarization_pipeline
    diarization_pipeline = None

    if DIARIZATION_AVAILABLE:
        hf_token = os.getenv("HF_TOKEN")
        if hf_token:
            try:
                # ä½¿ç”¨ huggingface_hub ç™»å…¥
                from huggingface_hub import login
                login(token=hf_token, add_to_git_credential=False)

                print("ğŸ”Š æ­£åœ¨è¼‰å…¥ Speaker Diarization æ¨¡å‹...")
                import torch
                diarization_pipeline = Pipeline.from_pretrained(
                    "pyannote/speaker-diarization-3.1"
                )

                # M1 Mac MPS åŠ é€Ÿ
                if torch.backends.mps.is_available():
                    diarization_pipeline.to(torch.device("mps"))
                    print("âœ… Speaker Diarization æ¨¡å‹è¼‰å…¥å®Œæˆï¼ˆä½¿ç”¨ MPS åŠ é€Ÿï¼‰ï¼")
                else:
                    print("âœ… Speaker Diarization æ¨¡å‹è¼‰å…¥å®Œæˆï¼")
            except Exception as e:
                print(f"âš ï¸  Speaker Diarization æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼š{e}")
                print("   è«‹ç¢ºèªå·²åœ¨ Hugging Face åŒæ„ä½¿ç”¨æ¢æ¬¾ï¼šhttps://huggingface.co/pyannote/speaker-diarization-3.1")
        else:
            print("â„¹ï¸  æœªè¨­å®š HF_TOKENï¼Œspeaker diarization åŠŸèƒ½ä¸å¯ç”¨")
            print("   å¦‚éœ€ä½¿ç”¨ï¼Œè«‹ï¼š")
            print("   1. è¨ªå• https://huggingface.co/settings/tokens")
            print("   2. å‰µå»º access token")
            print("   3. åœ¨ .env æ·»åŠ ï¼šHF_TOKEN=your_token_here")


@app.get("/")
async def root():
    """æœå‹™ç‹€æ…‹"""
    with tasks_lock:
        active_count = sum(1 for t in transcription_tasks.values() if t["status"] in ["pending", "processing"])

    return {
        "service": "Whisper è½‰éŒ„æœå‹™",
        "version": "2.0.0",
        "status": "running",
        "model": current_model_name,
        "output_dir": str(OUTPUT_DIR.relative_to(OUTPUT_DIR.parent)),
        "concurrent_limit": executor._max_workers,
        "active_transcriptions": active_count,
        "endpoints": {
            "POST /transcribe": "ä¸Šå‚³éŸ³æª”é€²è¡Œè½‰éŒ„ï¼ˆç•°æ­¥ï¼Œç«‹å³è¿”å› task_idï¼‰",
            "GET /transcribe/{task_id}": "æŸ¥è©¢ä»»å‹™ç‹€æ…‹",
            "GET /transcribe/{task_id}/download": "ä¸‹è¼‰è½‰éŒ„çµæœ",
            "GET /transcribe/active/list": "åˆ—å‡ºæ‰€æœ‰ä»»å‹™ï¼ˆå«é€²è¡Œä¸­ï¼‰",
            "GET /transcripts": "åˆ—å‡ºå·²ä¿å­˜çš„æ–‡å­—æª”",
            "GET /health": "å¥åº·æª¢æŸ¥",
            "GET /docs": "Swagger API æ–‡æª”"
        }
    }


@app.post("/transcribe")
async def transcribe(
    file: UploadFile = File(..., description="éŸ³æª” (æ”¯æ´ mp3/m4a/wav/mp4 ç­‰æ ¼å¼)"),
    punct_provider: str = Form("gemini", description="æ¨™é»æä¾›è€… (openai/gemini/none)"),
    chunk_audio: bool = Form(True, description="æ˜¯å¦ä½¿ç”¨åˆ†æ®µæ¨¡å¼"),
    chunk_minutes: int = Form(10, description="åˆ†æ®µé•·åº¦ï¼ˆåˆ†é˜ï¼‰"),
    diarize: bool = Form(False, description="æ˜¯å¦å•Ÿç”¨èªªè©±è€…è¾¨è­˜"),
    max_speakers: Optional[int] = Form(None, description="æœ€å¤§è¬›è€…äººæ•¸ï¼ˆå¯é¸ï¼Œ2-10ï¼‰"),
    language: str = Form("zh", description="è½‰éŒ„èªè¨€ (zh/en/ja/ko/auto)")
):
    """
    ä¸Šå‚³éŸ³æª”é€²è¡Œè½‰éŒ„ï¼ˆç•°æ­¥æ¨¡å¼ï¼‰

    ç«‹å³è¿”å›ä»»å‹™ IDï¼Œè½‰éŒ„åœ¨èƒŒæ™¯åŸ·è¡Œ
    ä½¿ç”¨ GET /transcribe/{task_id} æŸ¥è©¢ç‹€æ…‹
    ä½¿ç”¨ GET /transcribe/{task_id}/download ä¸‹è¼‰çµæœ

    - **file**: éŸ³æª”æª”æ¡ˆ
    - **punct_provider**: æ¨™é»æä¾›è€… (openai/gemini/none)
    - **chunk_audio**: æ˜¯å¦ä½¿ç”¨åˆ†æ®µæ¨¡å¼ï¼ˆé•·éŸ³æª”å»ºè­°é–‹å•Ÿï¼‰
    - **chunk_minutes**: åˆ†æ®µé•·åº¦ï¼ˆåˆ†é˜ï¼‰
    - **diarize**: æ˜¯å¦å•Ÿç”¨èªªè©±è€…è¾¨è­˜ï¼ˆspeaker diarizationï¼‰
    - **max_speakers**: æœ€å¤§è¬›è€…äººæ•¸ï¼ˆå¯é¸ï¼Œ2-10ï¼Œç•™ç©ºå‰‡è‡ªå‹•åµæ¸¬ï¼‰
    """
    global whisper_model

    if not whisper_model:
        raise HTTPException(status_code=503, detail="æ¨¡å‹å°šæœªè¼‰å…¥å®Œæˆ")

    # ç”Ÿæˆä»»å‹™ ID
    task_id = str(uuid.uuid4())

    # å»ºç«‹è‡¨æ™‚ç›®éŒ„ä¸¦ä¿å­˜ä¸Šå‚³çš„æª”æ¡ˆ
    temp_dir = Path(tempfile.mkdtemp())
    file_suffix = Path(file.filename).suffix
    temp_audio = temp_dir / f"input{file_suffix}"

    try:
        with temp_audio.open("wb") as f:
            content = await file.read()
            f.write(content)

        print(f"ğŸ“ æ”¶åˆ°æª”æ¡ˆï¼š{file.filename} ({len(content) / 1024 / 1024:.2f} MB)")

        # æª¢æŸ¥ diarization å¯ç”¨æ€§
        if diarize and not diarization_pipeline:
            raise HTTPException(
                status_code=400,
                detail="Speaker diarization åŠŸèƒ½æœªå•Ÿç”¨ã€‚è«‹è¨­å®š HF_TOKEN ç’°å¢ƒè®Šæ•¸ä¸¦é‡å•Ÿæœå‹™ã€‚"
            )

        # å‰µå»ºä»»å‹™è¨˜éŒ„
        with tasks_lock:
            transcription_tasks[task_id] = {
                "task_id": task_id,
                "status": "pending",
                "filename": file.filename,
                "file_size_mb": round(len(content) / 1024 / 1024, 2),
                "progress": "ç­‰å¾…è™•ç†...",
                "punct_provider": punct_provider,
                "chunk_audio": chunk_audio,
                "chunk_minutes": chunk_minutes,
                "diarize": diarize,
                "max_speakers": max_speakers,
                "language": language,
                "diarization_status": None,  # "running" | "completed" | "failed" | None
                "diarization_num_speakers": None,  # è­˜åˆ¥åˆ°çš„è¬›è€…äººæ•¸
                "created_at": get_current_time(),
                "updated_at": get_current_time()
            }

        # ç«‹å³ä¿å­˜æ–°ä»»å‹™
        save_tasks_to_disk()

        # åœ¨èƒŒæ™¯ç·šç¨‹ä¸­åŸ·è¡Œè½‰éŒ„
        loop = asyncio.get_event_loop()
        loop.run_in_executor(
            executor,
            process_transcription_task,
            task_id,
            temp_audio,
            file.filename,
            punct_provider,
            chunk_audio,
            chunk_minutes,
            diarize,
            max_speakers,
            language
        )

        # ç«‹å³è¿”å›ä»»å‹™è³‡è¨Š
        return JSONResponse({
            "task_id": task_id,
            "status": "pending",
            "message": "è½‰éŒ„ä»»å‹™å·²æäº¤ï¼Œè«‹ä½¿ç”¨ task_id æŸ¥è©¢ç‹€æ…‹",
            "filename": file.filename,
            "created_at": transcription_tasks[task_id]["created_at"],
            "status_url": f"/transcribe/{task_id}",
            "download_url": f"/transcribe/{task_id}/download"
        })

    except Exception as e:
        # ç™¼ç”ŸéŒ¯èª¤æ™‚æ¸…ç†
        cleanup_temp_dir(temp_dir)
        print(f"âŒ æäº¤ä»»å‹™å¤±æ•—ï¼š{e}")
        raise HTTPException(status_code=500, detail=f"æäº¤ä»»å‹™å¤±æ•—ï¼š{str(e)}")


@app.get("/transcribe/{task_id}")
async def get_task_status(task_id: str):
    """æŸ¥è©¢è½‰éŒ„ä»»å‹™ç‹€æ…‹"""
    with tasks_lock:
        task = transcription_tasks.get(task_id)

    if not task:
        raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨")

    # ä½¿ç”¨çµ±ä¸€çš„å‡½æ•¸æ·»åŠ è¨ˆç®—æ¬„ä½
    return JSONResponse(enrich_task_data(task))


@app.get("/transcribe/{task_id}/download")
async def download_task_result(task_id: str):
    """ä¸‹è¼‰è½‰éŒ„çµæœ"""
    with tasks_lock:
        task = transcription_tasks.get(task_id)

    if not task:
        raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨")

    if task["status"] != "completed":
        raise HTTPException(
            status_code=400,
            detail=f"ä»»å‹™å°šæœªå®Œæˆï¼ˆç•¶å‰ç‹€æ…‹ï¼š{task['status']}ï¼‰"
        )

    result_file = Path(task["result_file"])
    if not result_file.exists():
        raise HTTPException(status_code=404, detail="çµæœæª”æ¡ˆä¸å­˜åœ¨")

    return FileResponse(
        result_file,
        media_type="text/plain",
        filename=task["result_filename"]
    )


@app.post("/transcribe/{task_id}/cancel")
async def cancel_task(task_id: str):
    """å–æ¶ˆæ­£åœ¨åŸ·è¡Œçš„ä»»å‹™"""
    with tasks_lock:
        task = transcription_tasks.get(task_id)

        if not task:
            raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨")

        # åªèƒ½å–æ¶ˆé€²è¡Œä¸­æˆ–ç­‰å¾…ä¸­çš„ä»»å‹™
        if task["status"] not in ["pending", "processing"]:
            raise HTTPException(
                status_code=400,
                detail=f"ç„¡æ³•å–æ¶ˆå·²çµæŸçš„ä»»å‹™ï¼ˆç•¶å‰ç‹€æ…‹ï¼š{task['status']}ï¼‰"
            )

        # æ¨™è¨˜ä»»å‹™ç‚ºå·²å–æ¶ˆ
        task_cancelled[task_id] = True

        # ç«‹å³çµ‚æ­¢ diarization é€²ç¨‹ï¼ˆå¦‚æœæ­£åœ¨é‹è¡Œï¼‰
        diarization_executor = task_diarization_processes.get(task_id)
        if diarization_executor:
            print(f"ğŸ›‘ æ­£åœ¨å¼·åˆ¶çµ‚æ­¢èªªè©±è€…è¾¨è­˜é€²ç¨‹...")
            try:
                diarization_executor.shutdown(wait=False, cancel_futures=True)
                print(f"âœ… èªªè©±è€…è¾¨è­˜é€²ç¨‹å·²çµ‚æ­¢")
            except Exception as e:
                print(f"âš ï¸ çµ‚æ­¢ diarization é€²ç¨‹å¤±æ•—ï¼š{e}")
            task_diarization_processes.pop(task_id, None)

        # ç«‹å³æ¸…ç†æš«å­˜ç›®éŒ„ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        temp_dir = task_temp_dirs.get(task_id)
        if temp_dir:
            cleanup_temp_dir(temp_dir)
            task_temp_dirs.pop(task_id, None)

    print(f"ğŸ›‘ ä»»å‹™ {task_id} å·²è¢«æ¨™è¨˜ç‚ºå–æ¶ˆ")

    return {
        "message": "ä»»å‹™å–æ¶ˆæŒ‡ä»¤å·²ç™¼é€",
        "task_id": task_id,
        "note": "ä»»å‹™å°‡åœ¨ç•¶å‰æ­¥é©Ÿå®Œæˆå¾Œåœæ­¢"
    }


@app.delete("/transcribe/{task_id}")
async def delete_task(task_id: str):
    """åˆªé™¤ä»»å‹™åŠå…¶ç›¸é—œæª”æ¡ˆ"""
    with tasks_lock:
        task = transcription_tasks.get(task_id)

        if not task:
            raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨")

        # ä¸å…è¨±åˆªé™¤é€²è¡Œä¸­çš„ä»»å‹™
        if task["status"] in ["pending", "processing"]:
            raise HTTPException(
                status_code=400,
                detail=f"ç„¡æ³•åˆªé™¤é€²è¡Œä¸­çš„ä»»å‹™ï¼ˆç•¶å‰ç‹€æ…‹ï¼š{task['status']}ï¼‰ï¼Œè«‹å…ˆå–æ¶ˆä»»å‹™"
            )

        # åˆªé™¤çµæœæª”æ¡ˆï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if task["status"] == "completed" and "result_file" in task:
            result_file = Path(task["result_file"])
            try:
                if result_file.exists():
                    result_file.unlink()
                    print(f"ğŸ—‘ï¸ å·²åˆªé™¤æª”æ¡ˆï¼š{result_file.name}")
            except Exception as e:
                print(f"âš ï¸ åˆªé™¤æª”æ¡ˆå¤±æ•—ï¼š{e}")

        # å¾ä»»å‹™åˆ—è¡¨ä¸­ç§»é™¤
        del transcription_tasks[task_id]

    # ä¿å­˜åˆ°ç£ç¢Ÿ
    save_tasks_to_disk()

    return {
        "message": "ä»»å‹™å·²åˆªé™¤",
        "task_id": task_id
    }


def enrich_task_data(task: Dict[str, Any]) -> Dict[str, Any]:
    """ç‚ºä»»å‹™è³‡æ–™æ·»åŠ è¨ˆç®—æ¬„ä½ï¼ˆé ä¼°æ™‚é–“ã€æ ¼å¼åŒ–æ™‚é•·ç­‰ï¼‰"""
    task_copy = task.copy()

    # æ·»åŠ é ä¼°å‰©é¤˜æ™‚é–“
    if task["status"] == "processing":
        # ä½¿ç”¨å›ºå®šçš„é ä¼°å®Œæˆæ™‚é–“ï¼ˆåœ¨åˆ‡åˆ†å®Œæˆæ™‚è¨ˆç®—ä¸€æ¬¡ï¼‰
        estimated_completion_time = task.get("estimated_completion_time")

        if estimated_completion_time:
            # æ ¼å¼åŒ–å®Œæˆæ™‚é–“ç‚º MM/DD HH:MM
            try:
                completion_dt = datetime.strptime(estimated_completion_time, "%Y-%m-%d %H:%M:%S")
                task_copy["estimated_completion_text"] = completion_dt.strftime("%m/%d %H:%M")
            except:
                task_copy["estimated_completion_text"] = "è¨ˆç®—ä¸­......"
        else:
            # å°šæœªåˆ‡åˆ†å®Œæˆï¼Œé¡¯ç¤ºè¨ˆç®—ä¸­
            task_copy["estimated_completion_text"] = "è¨ˆç®—ä¸­......"

    # æ·»åŠ æ ¼å¼åŒ–çš„ç¸½è™•ç†æ™‚é•·ï¼ˆå¦‚æœå·²å®Œæˆï¼‰
    if task["status"] == "completed" and "duration_seconds" in task:
        task_copy["duration_text"] = format_duration(task["duration_seconds"])

    return task_copy


@app.get("/transcribe/active/list")
async def list_active_tasks():
    """åˆ—å‡ºæ‰€æœ‰é€²è¡Œä¸­çš„è½‰éŒ„ä»»å‹™"""
    with tasks_lock:
        active = [
            task for task in transcription_tasks.values()
            if task["status"] in ["pending", "processing"]
        ]
        all_tasks = list(transcription_tasks.values())

    # ç‚ºæ‰€æœ‰ä»»å‹™æ·»åŠ è¨ˆç®—æ¬„ä½
    active_enriched = [enrich_task_data(task) for task in active]
    all_tasks_enriched = [enrich_task_data(task) for task in all_tasks]

    return {
        "active_count": len(active_enriched),
        "total_count": len(all_tasks_enriched),
        "active_tasks": sorted(active_enriched, key=lambda x: x["created_at"], reverse=True),
        "all_tasks": sorted(all_tasks_enriched, key=lambda x: x["created_at"], reverse=True)[:20]  # æœ€è¿‘ 20 å€‹
    }


@app.get("/transcripts")
async def list_transcripts():
    """åˆ—å‡ºå·²ä¿å­˜çš„è½‰éŒ„æ–‡å­—æª”"""
    try:
        transcripts = []
        for txt_file in sorted(OUTPUT_DIR.glob("*.txt"), key=lambda x: x.stat().st_mtime, reverse=True):
            stat = txt_file.stat()
            transcripts.append({
                "filename": txt_file.name,
                "size_kb": round(stat.st_size / 1024, 2),
                "created": datetime.fromtimestamp(stat.st_mtime).strftime("%Y-%m-%d %H:%M:%S"),
                "path": str(txt_file.relative_to(OUTPUT_DIR.parent))
            })

        return {
            "total": len(transcripts),
            "output_dir": str(OUTPUT_DIR.relative_to(OUTPUT_DIR.parent)),
            "transcripts": transcripts
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/health")
async def health():
    """å¥åº·æª¢æŸ¥"""
    return {
        "status": "healthy",
        "model_loaded": whisper_model is not None,
        "model_name": current_model_name
    }


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Whisper è½‰éŒ„æœå‹™")
    parser.add_argument("--host", default="0.0.0.0", help="ç¶å®šçš„ IP åœ°å€")
    parser.add_argument("--port", type=int, default=8000, help="ç¶å®šçš„ç«¯å£")
    parser.add_argument("--model", default=DEFAULT_MODEL, help="Whisper æ¨¡å‹åç¨±")
    args = parser.parse_args()

    # æ›´æ–°é è¨­æ¨¡å‹
    DEFAULT_MODEL = args.model

    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   Whisper è½‰éŒ„æœå‹™ - FastAPI ç‰ˆæœ¬   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

æœå‹™åœ°å€: http://{args.host}:{args.port}
API æ–‡æª”: http://{args.host}:{args.port}/docs
Whisper æ¨¡å‹: {args.model}
Google API Keys: {len(GOOGLE_API_KEYS)} å€‹å·²è¼‰å…¥

""")

    uvicorn.run(app, host=args.host, port=args.port)
