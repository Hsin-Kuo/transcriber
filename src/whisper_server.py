#!/usr/bin/env python3
"""
Whisper è½‰éŒ„æœå‹™ - FastAPI ç‰ˆæœ¬
æ¨¡å‹åªéœ€è¼‰å…¥ä¸€æ¬¡ï¼Œå¯é‡è¤‡ä½¿ç”¨æå‡æ•ˆç‡
"""

import os
import sys
import tempfile
import shutil
import asyncio
import uuid
import json
from pathlib import Path
from typing import Optional, Dict, Any, List
from datetime import datetime, timezone, timedelta
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from threading import Lock
import multiprocessing
from faster_whisper import WhisperModel
from pydub import AudioSegment
from pydub.silence import detect_nonsilent
from fastapi import FastAPI, UploadFile, File, Form, HTTPException, BackgroundTasks, Depends
from fastapi.responses import FileResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
from dotenv import load_dotenv
load_dotenv()

# èªè­‰ç³»çµ±æ¨¡çµ„
from src.database.mongodb import MongoDB, get_database
from src.database.repositories.task_repo import TaskRepository
from src.database.repositories.tag_repo import TagRepository
from src.routers import auth as auth_router
from src.auth.dependencies import check_quota, get_current_user
from src.auth.quota import QuotaManager

# Speaker Diarization
try:
    from pyannote.audio import Pipeline
    DIARIZATION_AVAILABLE = True
except ImportError:
    DIARIZATION_AVAILABLE = False
    print("âš ï¸  pyannote.audio æœªå®‰è£ï¼Œspeaker diarization åŠŸèƒ½ä¸å¯ç”¨")

# â€”â€” è¨­å®š â€”â€” #
DEFAULT_MODEL = "medium"
CHUNK_DURATION_MS = 10 * 60 * 1000
OPENAI_MODEL = "gpt-4o-mini"
GEMINI_MODEL = "gemini-2.0-flash"
# å¤šå±¤å‚™æ´æ¨¡å‹åˆ—è¡¨ï¼ˆæŒ‰å„ªå…ˆé †åºï¼‰
GEMINI_FALLBACK_MODELS = [
    "gemini-2.0-flash-lite",      # ç¬¬ä¸€å‚™æ´ï¼š2.0-flash-liteï¼ˆè¼•é‡ç‰ˆï¼Œé…é¡è¼ƒå¯¬é¬†ï¼‰
    "gemini-flash-lite-latest",   # ç¬¬äºŒå‚™æ´ï¼šflash-lite-latestï¼ˆæœ€è¼•é‡ï¼Œé€šå¸¸æœ‰é…é¡ï¼‰
    "gemini-2.5-flash",           # ç¬¬ä¸‰å‚™æ´ï¼š2.5-flashï¼ˆæœ€æ–°ç‰ˆæœ¬ï¼‰
    "gemini-flash-latest",        # ç¬¬å››å‚™æ´ï¼šflash-latestï¼ˆé€šç”¨ç‰ˆæœ¬ï¼‰
    "gemini-2.5-pro",             # ç¬¬äº”å‚™æ´ï¼š2.5-proï¼ˆæ›´å¼·å¤§ä½†è¼ƒæ…¢ï¼‰
]
GEMINI_FALLBACK_MODEL = "gemini-2.0-flash-lite"  # å‘å¾Œå…¼å®¹ï¼ˆå·²æ£„ç”¨ï¼‰

# é€²åº¦éšæ®µæ¬Šé‡ï¼ˆå›ºå®šåˆ†é…ï¼Œç¸½å’Œ 100%ï¼‰
PROGRESS_WEIGHTS = {
    "audio_conversion": 5.0,      # éŸ³è¨Šè½‰æª”ï¼š5%
    "audio_chunking": 5.0,        # éŸ³è¨Šåˆ‡åˆ†ï¼š5%ï¼ˆåƒ…åˆ†æ®µæ¨¡å¼ï¼‰
    "transcription": 77.0,        # è½‰éŒ„ï¼š77%ï¼ˆåˆ†æ®µæ¨¡å¼ï¼‰æˆ– 82%ï¼ˆéåˆ†æ®µæ¨¡å¼ï¼‰
    "punctuation": 13.0,          # åŠ æ¨™é»ï¼š13%
}

# æ™‚å€è¨­å®š (UTC+8 å°åŒ—æ™‚é–“)
TZ_UTC8 = timezone(timedelta(hours=8))

def get_current_time():
    """å–å¾— UTC+8 ç•¶å‰æ™‚é–“"""
    return datetime.now(TZ_UTC8).strftime("%Y-%m-%d %H:%M:%S")

def format_duration(seconds: float) -> str:
    """æ ¼å¼åŒ–æ™‚é•·ç‚ºå‹å¥½çš„æ–‡å­—"""
    if seconds < 60:
        return f"{int(seconds)} ç§’"
    elif seconds < 3600:
        minutes = int(seconds // 60)
        secs = int(seconds % 60)
        return f"{minutes} åˆ† {secs} ç§’"
    else:
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        return f"{hours} å°æ™‚ {minutes} åˆ†"

def calculate_progress_percentage(task_data: Dict[str, Any]) -> float:
    """æ ¹æ“šä»»å‹™ç‹€æ…‹å‹•æ…‹è¨ˆç®—é€²åº¦ç™¾åˆ†æ¯”

    - å®Œæˆæ™‚å¼·åˆ¶è¿”å› 100%
    - éç¨‹ä¸­æ ¹æ“šå¯¦éš› chunks æ•¸é‡å‹•æ…‹åˆ†é…æ¬Šé‡
    """
    # å¦‚æœä»»å‹™å·²å®Œæˆï¼Œç›´æ¥è¿”å› 100%
    if task_data.get("status") == "completed":
        return 100.0

    progress = 0.0
    chunks = task_data.get("chunks", [])
    is_chunked = len(chunks) > 0  # æ˜¯å¦ä½¿ç”¨åˆ†æ®µæ¨¡å¼

    # 1. éŸ³è¨Šè½‰æª”å®Œæˆï¼š5%
    if task_data.get("audio_converted", False):
        progress += PROGRESS_WEIGHTS["audio_conversion"]

    # 2. è½‰éŒ„éšæ®µ
    if is_chunked:
        # åˆ†æ®µæ¨¡å¼ï¼šaudio_chunking(5%) + transcription(77%)
        if task_data.get("chunks_created", False):
            progress += PROGRESS_WEIGHTS["audio_chunking"]

        # æ ¹æ“šå¯¦éš› chunks æ•¸é‡åˆ†é…è½‰éŒ„é€²åº¦
        num_chunks = len(chunks)
        if num_chunks > 0:
            completed_chunks = sum(1 for c in chunks if c.get("status") == "completed")
            processing_chunks = sum(1 for c in chunks if c.get("status") == "processing")

            # æ¯å€‹ chunk å®Œæˆè²¢ç»ï¼š77% / num_chunks
            # æ¯å€‹ chunk é€²è¡Œä¸­è²¢ç»ï¼š50% çš„å®Œæˆæ¬Šé‡
            chunk_weight = PROGRESS_WEIGHTS["transcription"] / num_chunks
            progress += completed_chunks * chunk_weight
            progress += processing_chunks * (chunk_weight * 0.5)
    else:
        # éåˆ†æ®µæ¨¡å¼ï¼štranscription(82%) = audio_chunking(5%) + transcription(77%)
        # ç°¡å–®åˆ¤æ–·ï¼šå¦‚æœå·²ç¶“é–‹å§‹æ¨™é»ï¼Œèªªæ˜è½‰éŒ„å®Œæˆ
        if task_data.get("punctuation_started", False) or task_data.get("punctuation_completed", False):
            progress += PROGRESS_WEIGHTS["audio_chunking"] + PROGRESS_WEIGHTS["transcription"]
        elif task_data.get("audio_converted", False):
            # è½‰éŒ„ä¸­ï¼Œçµ¦äºˆ 50% çš„è½‰éŒ„é€²åº¦
            progress += (PROGRESS_WEIGHTS["audio_chunking"] + PROGRESS_WEIGHTS["transcription"]) * 0.5

    # 3. æ¨™é»è™•ç†ï¼š13%
    if task_data.get("punctuation_completed", False):
        progress += PROGRESS_WEIGHTS["punctuation"]
    elif task_data.get("punctuation_started", False):
        # æ¨™é»è™•ç†ä¸­ï¼Œæ ¹æ“šæ®µæ•¸è¨ˆç®—é€²åº¦
        punct_current = task_data.get("punctuation_current_chunk", 0)
        punct_total = task_data.get("punctuation_total_chunks", 1)
        punct_progress = (punct_current / punct_total) * PROGRESS_WEIGHTS["punctuation"]
        progress += punct_progress

    return min(progress, 100.0)

# è¼¸å‡ºç›®éŒ„è¨­å®š
OUTPUT_DIR = Path(__file__).parent.parent / "output"
OUTPUT_DIR.mkdir(exist_ok=True)

# ä»»å‹™ç‹€æ…‹æŒä¹…åŒ–æª”æ¡ˆ
TASKS_DB_FILE = OUTPUT_DIR / "tasks.json"
TAG_COLORS_FILE = OUTPUT_DIR / "tag_colors.json"
TAG_ORDER_FILE = OUTPUT_DIR / "tag_order.json"

# â€”â€” Google API Keys ç®¡ç† â€”â€” #
def load_google_api_keys():
    """å¾ç’°å¢ƒè®Šæ•¸è¼‰å…¥æ‰€æœ‰ Google API Keys"""
    keys = []
    i = 1
    while True:
        key = os.getenv(f"GOOGLE_API_KEY_{i}")
        if not key:
            break
        keys.append(key)
        i += 1

    # å¦‚æœæ²’æœ‰æ‰¾åˆ°ç·¨è™Ÿçš„ keysï¼Œå˜—è©¦ä½¿ç”¨å–®ä¸€çš„ GOOGLE_API_KEY
    if not keys:
        single_key = os.getenv("GOOGLE_API_KEY")
        if single_key:
            keys.append(single_key)

    return keys

GOOGLE_API_KEYS = load_google_api_keys()
current_google_key_index = 0
google_keys_lock = Lock()

def get_next_google_api_key():
    """ç²å–ä¸‹ä¸€å€‹ Google API Keyï¼ˆè¼ªè©¢ï¼‰"""
    global current_google_key_index

    if not GOOGLE_API_KEYS:
        raise ValueError("æœªè¨­å®šä»»ä½• GOOGLE_API_KEY")

    with google_keys_lock:
        key = GOOGLE_API_KEYS[current_google_key_index]
        current_google_key_index = (current_google_key_index + 1) % len(GOOGLE_API_KEYS)
        print(f"ğŸ”‘ ä½¿ç”¨ Google API Key #{current_google_key_index + 1}/{len(GOOGLE_API_KEYS)}")
        return key

# â€”â€” å…¨åŸŸæ¨¡å‹ (å•Ÿå‹•æ™‚è¼‰å…¥ï¼ŒæŒä¹…åŒ–åœ¨è¨˜æ†¶é«”ä¸­) â€”â€” #
whisper_model = None
current_model_name = None

# â€”â€” ä»»å‹™ç‹€æ…‹ç®¡ç† â€”â€” #
# å·²é·ç§»åˆ° MongoDBï¼Œä¿ç•™ä¸‹æ–¹å­—å…¸åƒ…ç”¨æ–¼é‹è¡Œæ™‚ç‹€æ…‹ï¼ˆéæŒä¹…åŒ–ï¼‰
task_temp_dirs: Dict[str, Path] = {}  # å„²å­˜ä»»å‹™çš„æš«å­˜ç›®éŒ„è·¯å¾‘
task_cancelled: Dict[str, bool] = {}  # æ¨™è¨˜å·²å–æ¶ˆçš„ä»»å‹™
task_diarization_processes: Dict[str, Any] = {}  # å„²å­˜ä»»å‹™çš„ diarization é€²ç¨‹
transcription_tasks: Dict[str, Any] = {}  # [Refactor] å„²å­˜å³æ™‚(in-memory)ä»»å‹™ç‹€æ…‹ï¼Œä¾‹å¦‚è©³ç´°é€²åº¦

# å®šç¾©åªéœ€å­˜åœ¨è¨˜æ†¶é«”ä¸­çš„æ¬„ä½ï¼ˆåŸ·è¡ŒæœŸé–“æ‰æœ‰ç”¨ï¼Œå®Œæˆå¾Œç„¡æ„ç¾©ï¼‰
MEMORY_ONLY_FIELDS = {
    # å³æ™‚é€²åº¦è³‡è¨Š
    "progress",  # é€²åº¦æ–‡å­—æè¿°ï¼ˆå¦‚ "æ­£åœ¨è½‰éŒ„ chunk 3/10..."ï¼‰
    "progress_percentage",  # é€²åº¦ç™¾åˆ†æ¯”ï¼ˆå¯å¾ç‹€æ…‹å³æ™‚è¨ˆç®—ï¼‰

    # åˆ†å¡ŠåŸ·è¡Œç´°ç¯€
    "chunks",  # æ¯å€‹ chunk çš„è©³ç´°ç‹€æ…‹é™£åˆ—ï¼ˆè¶…å¤§ç‰©ä»¶ï¼Œé »ç¹æ›´æ–°ï¼‰
    "total_chunks",  # ç¸½åˆ†å¡Šæ•¸ï¼ˆå¯å¾ chunks é•·åº¦è¨ˆç®—ï¼‰
    "completed_chunks",  # å·²å®Œæˆåˆ†å¡Šæ•¸ï¼ˆå¯å¾ chunks è¨ˆç®—ï¼‰
    "chunks_created",  # åˆ†å¡Šæ˜¯å¦å·²å»ºç«‹æ——æ¨™
    "estimated_completion_time",  # é ä¼°å®Œæˆæ™‚é–“ï¼ˆåŸ·è¡ŒæœŸé–“çš„ä¼°ç®—å€¼ï¼‰

    # æ¨™é»ç¬¦è™Ÿè™•ç†ä¸­é–“ç‹€æ…‹
    "punctuation_started",  # æ¨™é»è™•ç†æ˜¯å¦å·²é–‹å§‹
    "punctuation_current_chunk",  # ç•¶å‰è™•ç†çš„æ¨™é»æ®µæ•¸
    "punctuation_total_chunks",  # æ¨™é»è™•ç†ç¸½æ®µæ•¸
    "punctuation_completed",  # æ¨™é»è™•ç†æ˜¯å¦å®Œæˆ

    # èªªè©±è€…è¾¨è­˜ä¸­é–“ç‹€æ…‹
    "diarization_started",  # èªªè©±è€…è¾¨è­˜æ˜¯å¦å·²é–‹å§‹
    "diarization_completed",  # èªªè©±è€…è¾¨è­˜æ˜¯å¦å®Œæˆ
    "diarization_status",  # èªªè©±è€…è¾¨è­˜å³æ™‚ç‹€æ…‹

    # å…¶ä»–ä¸­é–“è™•ç†æ——æ¨™
    "audio_converted",  # éŸ³æª”æ˜¯å¦å·²è½‰æ›
}

tag_colors: Dict[str, str] = {}  # å„²å­˜æ¨™ç±¤é¡è‰² (æ¨™ç±¤åç¨± -> é¡è‰²ç¢¼)
tag_order: list[str] = []  # å„²å­˜æ¨™ç±¤é †åº
tasks_lock = Lock()  # ç·šç¨‹å®‰å…¨é–ï¼ˆç”¨æ–¼é‹è¡Œæ™‚ç‹€æ…‹ï¼‰
executor = ThreadPoolExecutor(max_workers=1)  # ç·šç¨‹æ± ï¼ˆåƒ main branchï¼Œåªç”¨ 1 å€‹ worker é¿å…ç«¶çˆ­ï¼‰

# MongoDB ä»»å‹™è³‡æ–™åº«
task_repo: Optional[TaskRepository] = None  # åœ¨ startup äº‹ä»¶ä¸­åˆå§‹åŒ–
tag_repo: Optional[TagRepository] = None  # æ¨™ç±¤è³‡æ–™åº«
main_loop: Optional[asyncio.AbstractEventLoop] = None  # ä¸»äº‹ä»¶å¾ªç’°

# ==================== å·²æ£„ç”¨ï¼šJSON æª”æ¡ˆè™•ç†ï¼ˆå·²é·ç§»åˆ° MongoDBï¼‰ ====================
# ä»¥ä¸‹å‡½æ•¸å·²ä¸å†ä½¿ç”¨ï¼Œä¿ç•™ç¨‹å¼ç¢¼ä»¥é˜²è¬ä¸€éœ€è¦é‚„åŸ
# JSON æª”æ¡ˆä»ä¿ç•™åœ¨ç£ç¢Ÿä¸Šä½œç‚ºå‚™ä»½

# def save_tasks_to_disk():
#     """[å·²æ£„ç”¨] å°‡ä»»å‹™ç‹€æ…‹ä¿å­˜åˆ°ç£ç¢Ÿ"""
#     pass

# def load_tasks_from_disk():
#     """[å·²æ£„ç”¨] å¾ç£ç¢Ÿè¼‰å…¥ä»»å‹™ç‹€æ…‹"""
#     global tag_colors, tag_order
#     try:
#         # ä»ç„¶è¼‰å…¥æ¨™ç±¤é¡è‰²å’Œé †åºï¼ˆå°šæœªé·ç§»åˆ° MongoDBï¼‰
#         if TAG_COLORS_FILE.exists():
#             with open(TAG_COLORS_FILE, 'r', encoding='utf-8') as f:
#                 tag_colors = json.load(f)
#             print(f"âœ… å·²è¼‰å…¥ {len(tag_colors)} å€‹æ¨™ç±¤é¡è‰²è¨­å®š")
#
#         if TAG_ORDER_FILE.exists():
#             with open(TAG_ORDER_FILE, 'r', encoding='utf-8') as f:
#                 tag_order = json.load(f)
#             print(f"âœ… å·²è¼‰å…¥æ¨™ç±¤é †åºï¼ˆ{len(tag_order)} å€‹æ¨™ç±¤ï¼‰")
#     except Exception as e:
#         print(f"âŒ è¼‰å…¥æ¨™ç±¤è¨­å®šå¤±æ•—ï¼š{e}")

def load_tag_settings():
    """å¾ç£ç¢Ÿè¼‰å…¥æ¨™ç±¤è¨­å®šï¼ˆæ¨™ç±¤é¡è‰²å’Œé †åºï¼‰"""
    global tag_colors, tag_order
    try:
        if TAG_COLORS_FILE.exists():
            with open(TAG_COLORS_FILE, 'r', encoding='utf-8') as f:
                tag_colors = json.load(f)
            print(f"âœ… å·²è¼‰å…¥ {len(tag_colors)} å€‹æ¨™ç±¤é¡è‰²è¨­å®š")

        if TAG_ORDER_FILE.exists():
            with open(TAG_ORDER_FILE, 'r', encoding='utf-8') as f:
                tag_order = json.load(f)
            print(f"âœ… å·²è¼‰å…¥æ¨™ç±¤é †åºï¼ˆ{len(tag_order)} å€‹æ¨™ç±¤ï¼‰")
    except Exception as e:
        print(f"âŒ è¼‰å…¥æ¨™ç±¤è¨­å®šå¤±æ•—ï¼š{e}")

def save_tag_settings():
    """ä¿å­˜æ¨™ç±¤è¨­å®šåˆ°ç£ç¢Ÿï¼ˆæ¨™ç±¤é¡è‰²å’Œé †åºï¼‰"""
    try:
        with open(TAG_COLORS_FILE, 'w', encoding='utf-8') as f:
            json.dump(tag_colors, f, ensure_ascii=False, indent=2)
        with open(TAG_ORDER_FILE, 'w', encoding='utf-8') as f:
            json.dump(tag_order, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âŒ ä¿å­˜æ¨™ç±¤è¨­å®šå¤±æ•—ï¼š{e}")

# ==================== MongoDB è¼”åŠ©å‡½æ•¸ ====================
def run_async_in_thread(coro):
    """åœ¨èƒŒæ™¯åŒæ­¥ç·šç¨‹ä¸­ï¼Œå®‰å…¨åœ°åŸ·è¡Œä¸€å€‹å”ç¨‹ (coroutine)"""
    global main_loop
    if main_loop is None or not main_loop.is_running():
        # Fallback for scripts or tests that don't run the full app
        print("âš ï¸ Main event loop is not available, running in a new loop.")
        return asyncio.run(coro)

    future = asyncio.run_coroutine_threadsafe(coro, main_loop)
    return future.result()  # Blocks until the coroutine is done and returns the result.

async def get_task_from_db(task_id: str) -> Optional[Dict[str, Any]]:
    """å¾è³‡æ–™åº«ç²å–ä»»å‹™"""
    if task_repo is None:
        print(f"âš ï¸  TaskRepository æœªåˆå§‹åŒ–")
        return None
    return await task_repo.get_by_id(task_id)

async def update_task_in_db(task_id: str, updates: Dict[str, Any]) -> bool:
    """æ›´æ–°è³‡æ–™åº«ä¸­çš„ä»»å‹™"""
    if task_repo is None:
        print(f"âš ï¸  TaskRepository æœªåˆå§‹åŒ–")
        return False
    return await task_repo.update(task_id, updates)

async def create_task_in_db(task_data: Dict[str, Any]) -> Dict[str, Any]:
    """åœ¨è³‡æ–™åº«ä¸­å»ºç«‹ä»»å‹™"""
    if task_repo is None:
        print(f"âš ï¸  TaskRepository æœªåˆå§‹åŒ–")
        return task_data
    return await task_repo.create(task_data)

# Tag models (imported from src/models/tag.py)
from src.models.tag import TagCreate, TagUpdate, TagOrderUpdate as TagOrderUpdateModel, TagResponse

# Pydantic models
class TranscriptContentUpdate(BaseModel):
    content: str

class TaskMetadataUpdate(BaseModel):
    custom_name: str = None

class TaskTagsUpdate(BaseModel):
    tags: list[str] = []

class TagColorUpdate(BaseModel):
    color: str

class TagOrderUpdate(BaseModel):
    order: list[str] = []

class KeepAudioUpdate(BaseModel):
    keep_audio: bool

class BatchDeleteRequest(BaseModel):
    task_ids: List[str]

class BatchTagsRequest(BaseModel):
    task_ids: List[str]
    tags: List[str]


from src.models.task import TaskInDB as Task


app = FastAPI(
    title="Whisper è½‰éŒ„æœå‹™",
    description="ä¸Šå‚³éŸ³æª”é€²è¡Œè½‰éŒ„ï¼Œæ”¯æ´è‡ªå‹•åˆ†æ®µèˆ‡æ¨™é»",
    version="2.0.0"
)

# æ·»åŠ  CORS ä¸­é–“ä»¶
# å¾ç’°å¢ƒè®Šæ•¸è®€å–å…è¨±çš„ä¾†æº
cors_origins_str = os.getenv("CORS_ORIGINS", "*")
cors_origins = [origin.strip() for origin in cors_origins_str.split(",")] if cors_origins_str != "*" else ["*"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=cors_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["*"],  # å…è¨±å‰ç«¯è¨ªå•æ‰€æœ‰éŸ¿æ‡‰é ­
    max_age=3600,  # preflight è«‹æ±‚å¿«å–æ™‚é–“ï¼ˆç§’ï¼‰
)

# è¨»å†Šèªè­‰è·¯ç”±
app.include_router(auth_router.router)


# ---------- å·¥å…·å‡½å¼ ----------

def cleanup_temp_dir(temp_dir: Path):
    """æ¸…ç†è‡¨æ™‚ç›®éŒ„"""
    try:
        if temp_dir.exists():
            shutil.rmtree(temp_dir)
            print(f"ğŸ—‘ï¸ å·²æ¸…ç†è‡¨æ™‚ç›®éŒ„ï¼š{temp_dir.name}")
    except Exception as e:
        print(f"âš ï¸ æ¸…ç†è‡¨æ™‚ç›®éŒ„å¤±æ•—ï¼š{e}")


def get_task_field(task: Dict[str, Any], field: str) -> Any:
    """å®‰å…¨ç²å–ä»»å‹™æ¬„ä½ï¼ˆæ”¯æ´å·¢ç‹€èˆ‡æ‰å¹³æ ¼å¼ï¼‰

    Args:
        task: ä»»å‹™è³‡æ–™
        field: æ¬„ä½åç¨±ï¼ˆæ‰å¹³æ ¼å¼ï¼Œå¦‚ 'result_file', 'user_id'ï¼‰

    Returns:
        æ¬„ä½å€¼ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡è¿”å› None
    """
    # æ¬„ä½æ˜ å°„ï¼šæ‰å¹³åç¨± -> (å·¢ç‹€è·¯å¾‘åˆ—è¡¨)
    FIELD_PATHS = {
        # user ç›¸é—œ
        "user_id": [("user", "user_id"), None],
        "user_email": [("user", "user_email"), None],

        # file ç›¸é—œ
        "filename": [("file", "filename"), None],
        "file_size_mb": [("file", "size_mb"), None],

        # config ç›¸é—œ
        "punct_provider": [("config", "punct_provider"), None],
        "chunk_audio": [("config", "chunk_audio"), None],
        "diarize": [("config", "diarize"), None],
        "language": [("config", "language"), None],

        # result ç›¸é—œ
        "result_file": [("result", "transcription_file"), None],
        "result_filename": [("result", "transcription_filename"), None],
        "audio_file": [("result", "audio_file"), None],
        "audio_filename": [("result", "audio_filename"), None],
        "segments_file": [("result", "segments_file"), None],
        "text_length": [("result", "text_length"), None],

        # stats ç›¸é—œ
        "duration_seconds": [("stats", "duration_seconds"), None],
        "total_tokens_used": [("stats", "token_usage", "total"), None],

        # timestamps ç›¸é—œ
        "created_at": [("timestamps", "created_at"), None],
        "updated_at": [("timestamps", "updated_at"), None],
        "completed_at": [("timestamps", "completed_at"), None],
    }

    # å¦‚æœæ˜¯é ‚å±¤æ¬„ä½ï¼ˆstatus, progress, tags, keep_audio, custom_name ç­‰ï¼‰
    if field not in FIELD_PATHS:
        return task.get(field)

    # å˜—è©¦å¾å·¢ç‹€è·¯å¾‘ç²å–
    nested_path = FIELD_PATHS[field][0]
    if nested_path:
        value = task
        for key in nested_path:
            if isinstance(value, dict) and key in value:
                value = value[key]
            else:
                value = None
                break

        if value is not None:
            return value

    # å‘å¾Œå…¼å®¹ï¼šå˜—è©¦å¾æ‰å¹³çµæ§‹ç²å–
    return task.get(field)


def convert_nested_to_flat_task(task: Dict[str, Any]) -> Dict[str, Any]:
    """å°‡å·¢ç‹€çµæ§‹çš„ä»»å‹™è³‡æ–™è½‰æ›ç‚ºæ‰å¹³æ ¼å¼ï¼ˆå‘å¾Œå…¼å®¹ï¼‰

    Args:
        task: å·¢ç‹€æ ¼å¼çš„ä»»å‹™è³‡æ–™

    Returns:
        æ‰å¹³æ ¼å¼çš„ä»»å‹™è³‡æ–™
    """
    if not task:
        return task

    # å¦‚æœå·²ç¶“æ˜¯æ‰å¹³æ ¼å¼ï¼ˆèˆŠè³‡æ–™ï¼‰ï¼Œç›´æ¥è¿”å›
    if "user_id" in task and "filename" in task:
        return task

    # è½‰æ›ç‚ºæ‰å¹³æ ¼å¼
    flat_task = {
        "_id": task.get("_id"),
        "task_id": task.get("task_id"),
        "status": task.get("status"),
        "progress": task.get("progress"),
        "tags": task.get("tags", []),
        "custom_name": task.get("custom_name"),
        "keep_audio": task.get("keep_audio", False),
    }

    # ä½¿ç”¨è€…è³‡è¨Š
    if "user" in task:
        flat_task["user_id"] = task["user"]["user_id"]
        flat_task["user_email"] = task["user"]["user_email"]

    # æª”æ¡ˆè³‡è¨Š
    if "file" in task:
        flat_task["filename"] = task["file"]["filename"]
        flat_task["file_size_mb"] = task["file"]["size_mb"]

    # é…ç½®
    if "config" in task:
        flat_task["punct_provider"] = task["config"].get("punct_provider")
        flat_task["chunk_audio"] = task["config"].get("chunk_audio")
        flat_task["chunk_minutes"] = task["config"].get("chunk_minutes")
        flat_task["diarize"] = task["config"].get("diarize")
        flat_task["max_speakers"] = task["config"].get("max_speakers")
        flat_task["language"] = task["config"].get("language")

    # çµæœæª”æ¡ˆ
    if "result" in task and task["result"]:
        flat_task["audio_file"] = task["result"].get("audio_file")
        flat_task["audio_filename"] = task["result"].get("audio_filename")
        flat_task["result_file"] = task["result"].get("transcription_file")
        flat_task["result_filename"] = task["result"].get("transcription_filename")
        flat_task["segments_file"] = task["result"].get("segments_file")
        flat_task["text_length"] = task["result"].get("text_length")

    # çµ±è¨ˆè³‡è¨Š
    if "stats" in task and task["stats"]:
        flat_task["duration_seconds"] = task["stats"].get("duration_seconds")
        if "token_usage" in task["stats"] and task["stats"]["token_usage"]:
            flat_task["total_tokens_used"] = task["stats"]["token_usage"].get("total")
            flat_task["prompt_tokens_used"] = task["stats"]["token_usage"].get("prompt")
            flat_task["completion_tokens_used"] = task["stats"]["token_usage"].get("completion")
            flat_task["llm_model_used"] = task["stats"]["token_usage"].get("model")
        if "diarization" in task["stats"] and task["stats"]["diarization"]:
            flat_task["diarization_num_speakers"] = task["stats"]["diarization"].get("num_speakers")
            flat_task["diarization_duration_seconds"] = task["stats"]["diarization"].get("duration_seconds")

    # æ™‚é–“æˆ³è¨˜
    if "timestamps" in task:
        flat_task["created_at"] = task["timestamps"]["created_at"]
        flat_task["updated_at"] = task["timestamps"].get("updated_at")
        flat_task["started_at"] = task["timestamps"].get("started_at")
        flat_task["completed_at"] = task["timestamps"].get("completed_at")

    return flat_task


def convert_flat_to_nested_updates(updates: Dict[str, Any]) -> Dict[str, Any]:
    """å°‡æ‰å¹³æ¬„ä½æ›´æ–°è½‰æ›ç‚ºå·¢ç‹€çµæ§‹çš„ MongoDB æ›´æ–°æ ¼å¼

    Args:
        updates: æ‰å¹³æ ¼å¼çš„æ›´æ–°å­—å…¸

    Returns:
        MongoDB dot notation æ ¼å¼çš„æ›´æ–°å­—å…¸
    """
    # æ¬„ä½æ˜ å°„ï¼šèˆŠæ¬„ä½å -> æ–°å·¢ç‹€è·¯å¾‘
    FIELD_MAPPING = {
        # result ç›¸é—œ
        "audio_file": "result.audio_file",
        "audio_filename": "result.audio_filename",
        "result_file": "result.transcription_file",
        "result_filename": "result.transcription_filename",
        "segments_file": "result.segments_file",
        "text_length": "result.text_length",

        # stats ç›¸é—œ
        "duration_seconds": "stats.duration_seconds",
        "total_tokens_used": "stats.token_usage.total",
        "prompt_tokens_used": "stats.token_usage.prompt",
        "completion_tokens_used": "stats.token_usage.completion",
        "llm_model_used": "stats.token_usage.model",
        "diarization_num_speakers": "stats.diarization.num_speakers",
        "diarization_duration_seconds": "stats.diarization.duration_seconds",

        # timestamps ç›¸é—œ
        "created_at": "timestamps.created_at",
        "updated_at": "timestamps.updated_at",
        "started_at": "timestamps.started_at",
        "completed_at": "timestamps.completed_at",
    }

    nested_updates = {}
    for key, value in updates.items():
        # å¦‚æœåœ¨æ˜ å°„è¡¨ä¸­ï¼Œä½¿ç”¨æ–°è·¯å¾‘
        if key in FIELD_MAPPING:
            nested_updates[FIELD_MAPPING[key]] = value
        else:
            # å¦å‰‡ä¿æŒåŸæ¨£ï¼ˆstatus, progress, tags, keep_audio, custom_name ç­‰é ‚å±¤æ¬„ä½ï¼‰
            nested_updates[key] = value

    return nested_updates


def update_task_status(task_id: str, updates: Dict[str, Any], persist_to_db: bool = None):
    """æ›´æ–°ä»»å‹™ç‹€æ…‹ï¼ˆæ™ºèƒ½é¸æ“‡æ˜¯å¦æŒä¹…åŒ–åˆ° MongoDBï¼‰

    Args:
        task_id: ä»»å‹™ ID
        updates: è¦æ›´æ–°çš„æ¬„ä½å­—å…¸ï¼ˆæ‰å¹³æ ¼å¼ï¼Œæœƒè‡ªå‹•è½‰æ›ç‚ºå·¢ç‹€æ ¼å¼ï¼‰
        persist_to_db: æ˜¯å¦æŒä¹…åŒ–åˆ° MongoDB
            - Noneï¼ˆé è¨­ï¼‰: è‡ªå‹•åˆ¤æ–·ï¼ˆç‹€æ…‹è®Šæ›´æ™‚æŒä¹…åŒ–ï¼Œé€²åº¦æ›´æ–°æ™‚åƒ…è¨˜æ†¶é«”ï¼‰
            - True: å¼·åˆ¶æŒä¹…åŒ–
            - False: åƒ…æ›´æ–°è¨˜æ†¶é«”
    """
    # æ›´æ–°æ™‚é–“æˆ³
    updates["updated_at"] = get_current_time()

    # --- 1. ç¸½æ˜¯æ›´æ–°è¨˜æ†¶é«”ä¸­çš„å³æ™‚ç‹€æ…‹ï¼ˆä½¿ç”¨æ‰å¹³æ ¼å¼ï¼‰ ---
    with tasks_lock:
        if task_id not in transcription_tasks:
            transcription_tasks[task_id] = {}
        transcription_tasks[task_id].update(updates)
        if 'progress' in updates:
            print(f"ğŸ”„ [{task_id}] In-memory progress: {updates['progress']}")

    # --- 2. æ±ºå®šæ˜¯å¦éœ€è¦æŒä¹…åŒ–åˆ° MongoDB ---
    # åƒ main branch ä¸€æ¨£ï¼šåªåœ¨ä»»å‹™å®Œæˆæ™‚æ‰æŒä¹…åŒ–ï¼ˆé¿å…é »ç¹ DB å¯«å…¥ï¼‰
    if persist_to_db is None:
        # åªåœ¨æœ€çµ‚ç‹€æ…‹æ™‚è‡ªå‹•æŒä¹…åŒ–
        persist_to_db = updates.get("status") in ["completed", "failed", "cancelled"]

    # --- 3. æŒä¹…åŒ–åˆ° MongoDBï¼ˆåƒ…åœ¨å¿…è¦æ™‚ï¼‰ ---
    if persist_to_db:
        # æª¢æŸ¥æ˜¯å¦ç‚ºæœ€çµ‚ç‹€æ…‹ï¼ˆå®Œæˆã€å¤±æ•—ã€å–æ¶ˆï¼‰
        is_final_status = updates.get("status") in ["completed", "failed", "cancelled"]

        # éæ¿¾æ‰è¨˜æ†¶é«”å°ˆç”¨æ¬„ä½
        # ä½†å¦‚æœæ˜¯æœ€çµ‚ç‹€æ…‹ï¼Œä¿ç•™ progress æ¬„ä½ä»¥è¨˜éŒ„æœ€çµ‚é€²åº¦è¨Šæ¯
        db_updates = {
            k: v for k, v in updates.items()
            if k not in MEMORY_ONLY_FIELDS or (k == "progress" and is_final_status)
        }

        # è½‰æ›ç‚ºå·¢ç‹€çµæ§‹çš„ MongoDB æ›´æ–°æ ¼å¼
        db_updates = convert_flat_to_nested_updates(db_updates)

        # å¦‚æœéæ¿¾å¾Œé‚„æœ‰æ¬„ä½éœ€è¦æ›´æ–°
        if db_updates:
            # è¨ˆç®—é€²åº¦ç™¾åˆ†æ¯”ï¼ˆåƒ…ä½¿ç”¨è¨˜æ†¶é«”æ•¸æ“šï¼Œä¸æŸ¥è©¢ DBï¼‰
            if "status" in db_updates or "completed_chunks" in updates:
                with tasks_lock:
                    if task_id in transcription_tasks:
                        # ç›´æ¥ä½¿ç”¨è¨˜æ†¶é«”ä¸­çš„è³‡æ–™è¨ˆç®—é€²åº¦ï¼ˆé¿å…é˜»å¡å¼ DB æŸ¥è©¢ï¼‰
                        progress_pct = calculate_progress_percentage(transcription_tasks[task_id])
                        transcription_tasks[task_id]["progress_percentage"] = round(progress_pct, 1)

            # æ›´æ–°åˆ° MongoDBï¼ˆä½¿ç”¨ futureï¼Œä¸é˜»å¡ç•¶å‰ç·šç¨‹ï¼‰
            # ä½¿ç”¨ asyncio.create_task åœ¨èƒŒæ™¯åŸ·è¡Œï¼Œé¿å…é˜»å¡ worker ç·šç¨‹
            try:
                if main_loop and main_loop.is_running():
                    asyncio.run_coroutine_threadsafe(update_task_in_db(task_id, db_updates), main_loop)
                    print(f"ğŸ’¾ [{task_id}] æŒä¹…åŒ–åˆ° MongoDB (éé˜»å¡): {list(db_updates.keys())}")
                else:
                    print(f"âš ï¸ [{task_id}] Event loop ä¸å¯ç”¨ï¼Œè·³é DB æ›´æ–°")
            except Exception as e:
                print(f"âš ï¸ [{task_id}] DB æ›´æ–°å¤±æ•—ï¼ˆä¸å½±éŸ¿è½‰éŒ„ï¼‰: {e}")
        else:
            print(f"âš¡ [{task_id}] åƒ…è¨˜æ†¶é«”æ›´æ–°ï¼ˆç„¡éœ€æŒä¹…åŒ–ï¼‰")
    else:
        print(f"âš¡ [{task_id}] åƒ…è¨˜æ†¶é«”æ›´æ–°")

    # --- 4. ä»»å‹™çµæŸæ™‚çš„æ¸…ç† ---
    final_status = updates.get("status")
    if final_status in ["completed", "failed", "cancelled"]:
        print(f"âœ… [{task_id}] ä»»å‹™å·²çµæŸï¼š{final_status}")
        # å¾å³æ™‚ç‹€æ…‹å­—å…¸ä¸­ç§»é™¤ï¼Œä»¥é‡‹æ”¾è¨˜æ†¶é«”
        with tasks_lock:
            transcription_tasks.pop(task_id, None)
# ---------- è½‰éŒ„å‡½å¼ (å¾åŸå§‹ transcribe.py ç§»æ¤) ----------

def transcribe_single_chunk(
    model,
    chunk_path: Path,
    language: Optional[str] = None,
    task_id: str = None,
    chunk_idx: int = None,
    time_offset: float = 0.0,
    return_segments: bool = True
) -> tuple:
    """è½‰éŒ„å–®ä¸€éŸ³æª”ç‰‡æ®µï¼ˆç”¨æ–¼ä¸¦è¡Œè™•ç†ï¼‰

    Args:
        model: Whisper æ¨¡å‹
        chunk_path: éŸ³æª”è·¯å¾‘
        language: èªè¨€ä»£ç¢¼ï¼ˆNone è¡¨ç¤ºè‡ªå‹•åµæ¸¬ï¼Œé è¨­å€¼ï¼‰
        task_id: ä»»å‹™ ID
        chunk_idx: Chunk ç´¢å¼•
        time_offset: æ™‚é–“åç§»ï¼ˆç§’ï¼‰ï¼Œç”¨æ–¼è¨ˆç®—ç›¸å°æ–¼å®Œæ•´éŸ³æª”çš„æ™‚é–“æˆ³
        return_segments: æ˜¯å¦è¿”å›å¸¶æ™‚é–“æˆ³çš„ segmentsï¼ˆé è¨­ Trueï¼‰

    Returns:
        è¿”å› (text, segments, detected_language) å…ƒçµ„
    """
    # æ¨™è¨˜æ­¤ chunk é–‹å§‹è™•ç†ï¼ˆå¯¦éš›é–‹å§‹åŸ·è¡Œæ™‚æ‰æ¨™è¨˜ï¼‰
    if task_id and chunk_idx:
        # å¾è¨˜æ†¶é«”ç²å– chunksï¼ˆä¸æŸ¥è©¢ DBï¼Œé¿å…é˜»å¡ï¼‰
        with tasks_lock:
            chunks = transcription_tasks.get(task_id, {}).get("chunks", [])

            # å¦‚æœè¨˜æ†¶é«”ä¸­æ²’æœ‰ chunksï¼Œè·³éç‹€æ…‹æ›´æ–°ï¼ˆä¸é˜»å¡ç­‰å¾… DBï¼‰
            if chunks and chunk_idx - 1 < len(chunks) and chunks[chunk_idx - 1]["status"] == "pending":
                chunks[chunk_idx - 1]["status"] = "processing"
                chunks[chunk_idx - 1]["started_at"] = get_current_time()
                # ç›´æ¥åœ¨é€™è£¡æ›´æ–°ï¼Œé¿å…å†æ¬¡å–å¾—é–ï¼ˆé¿å…æ­»é–ï¼‰
                if task_id not in transcription_tasks:
                    transcription_tasks[task_id] = {}
                transcription_tasks[task_id]["chunks"] = chunks
                transcription_tasks[task_id]["updated_at"] = get_current_time()

    segments, info = model.transcribe(str(chunk_path), language=language, beam_size=5)

    # ç²å– Whisper åµæ¸¬åˆ°çš„èªè¨€
    detected_language = info.language if hasattr(info, 'language') else None

    # å§‹çµ‚æ”¶é›† segments
    segments_list = []
    text_parts = []

    for segment in segments:
        text_parts.append(segment.text)
        segments_list.append({
            "start": segment.start + time_offset,  # åŠ ä¸Šæ™‚é–“åç§»
            "end": segment.end + time_offset,
            "text": segment.text
        })

    text = "".join(text_parts).strip()

    return text, segments_list, detected_language


def transcribe_with_timestamps(model, audio_path: Path, language: Optional[str] = None) -> tuple:
    """
    è½‰éŒ„éŸ³æª”ä¸¦è¿”å›å¸¶æ™‚é–“æˆ³çš„ segments

    Args:
        model: Whisper æ¨¡å‹
        audio_path: éŸ³æª”è·¯å¾‘
        language: èªè¨€ä»£ç¢¼ï¼ˆNone è¡¨ç¤ºè‡ªå‹•åµæ¸¬ï¼Œé è¨­å€¼ï¼‰

    Returns:
        è¿”å› (segments_list, detected_language) å…ƒçµ„
        segments_list: List of segments with format [{"start": 0.0, "end": 5.2, "text": "hello"}, ...]
        detected_language: Whisper åµæ¸¬åˆ°çš„èªè¨€ä»£ç¢¼
    """
    segments_list = []
    segments, info = model.transcribe(str(audio_path), language=language, beam_size=5)

    # ç²å– Whisper åµæ¸¬åˆ°çš„èªè¨€
    detected_language = info.language if hasattr(info, 'language') else None

    for segment in segments:
        segments_list.append({
            "start": segment.start,
            "end": segment.end,
            "text": segment.text
        })

    return segments_list, detected_language


def perform_diarization_in_process(audio_path_str: str, max_speakers: Optional[int], hf_token: str) -> Optional[List[Dict]]:
    """åœ¨ç¨ç«‹é€²ç¨‹ä¸­åŸ·è¡Œ speaker diarizationï¼ˆå¯è¢«å¼·åˆ¶çµ‚æ­¢ï¼‰

    æ­¤å‡½æ•¸è¨­è¨ˆç‚ºåœ¨å–®ç¨çš„é€²ç¨‹ä¸­é‹è¡Œï¼Œå› æ­¤ä¸ä¾è³´å…¨å±€è®Šé‡

    Args:
        audio_path_str: éŸ³æª”è·¯å¾‘ï¼ˆå­—ä¸²æ ¼å¼ï¼‰
        max_speakers: æœ€å¤§è¬›è€…äººæ•¸ï¼ˆå¯é¸ï¼Œ2-10ï¼‰
        hf_token: Hugging Face Token

    Returns:
        List of diarization segments with format:
        [{"start": 0.0, "end": 5.2, "speaker": "SPEAKER_00"}, ...]
    """
    try:
        # åœ¨é€²ç¨‹ä¸­é‡æ–°è¼‰å…¥ pipelineï¼ˆå› ç‚ºç„¡æ³•è·¨é€²ç¨‹å‚³éï¼‰
        from pyannote.audio import Pipeline
        from huggingface_hub import login

        if hf_token:
            login(token=hf_token, add_to_git_credential=False)

        print(f"ğŸ”Š [é€²ç¨‹] æ­£åœ¨è¼‰å…¥ diarization pipeline...")
        import torch
        pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization-3.1")

        # M1 Mac MPS åŠ é€Ÿ
        if torch.backends.mps.is_available():
            pipeline.to(torch.device("mps"))
            print(f"ğŸ”Š [é€²ç¨‹] ä½¿ç”¨ MPS åŠ é€Ÿ")

        print(f"ğŸ”Š [é€²ç¨‹] æ­£åœ¨åˆ†æèªªè©±è€…...")

        # æº–å‚™ diarization åƒæ•¸
        diarization_kwargs = {}
        if max_speakers is not None and 2 <= max_speakers <= 10:
            diarization_kwargs["min_speakers"] = 1
            diarization_kwargs["max_speakers"] = max_speakers
            print(f"   [é€²ç¨‹] è¨­å®šè¬›è€…äººæ•¸ç¯„åœï¼š1-{max_speakers} äºº")

        print(f"   [é€²ç¨‹] Diarization åƒæ•¸ï¼š{diarization_kwargs}")
        diarization = pipeline(audio_path_str, **diarization_kwargs)

        segments = []
        for turn, _, speaker in diarization.itertracks(yield_label=True):
            segments.append({
                "start": turn.start,
                "end": turn.end,
                "speaker": speaker
            })

        print(f"âœ… [é€²ç¨‹] èªªè©±è€…åˆ†æå®Œæˆï¼Œè­˜åˆ¥åˆ° {len(set(s['speaker'] for s in segments))} ä½èªªè©±è€…")
        return segments

    except Exception as e:
        print(f"âš ï¸  [é€²ç¨‹] Speaker diarization å¤±æ•—ï¼š{e}")
        return None


def perform_diarization(audio_path: Path, max_speakers: Optional[int] = None) -> Optional[List[Dict]]:
    """åŸ·è¡Œ speaker diarizationï¼ˆç·šç¨‹ç‰ˆæœ¬ï¼Œç”¨æ–¼éåˆ†æ®µæ¨¡å¼ï¼‰

    Args:
        audio_path: éŸ³æª”è·¯å¾‘
        max_speakers: æœ€å¤§è¬›è€…äººæ•¸ï¼ˆå¯é¸ï¼Œ2-10ï¼‰

    Returns:
        List of diarization segments with format:
        [{"start": 0.0, "end": 5.2, "speaker": "SPEAKER_00"}, ...]
    """
    if not diarization_pipeline:
        return None

    try:
        print(f"ğŸ”Š æ­£åœ¨åˆ†æèªªè©±è€…...")

        # æº–å‚™ diarization åƒæ•¸
        diarization_kwargs = {}
        if max_speakers is not None and 2 <= max_speakers <= 10:
            # pyannote.audio éœ€è¦åŒæ™‚è¨­å®š min_speakers å’Œ max_speakers
            diarization_kwargs["min_speakers"] = 1
            diarization_kwargs["max_speakers"] = max_speakers
            print(f"   è¨­å®šè¬›è€…äººæ•¸ç¯„åœï¼š1-{max_speakers} äºº")

        print(f"   Diarization åƒæ•¸ï¼š{diarization_kwargs}")
        diarization = diarization_pipeline(str(audio_path), **diarization_kwargs)

        segments = []
        for turn, _, speaker in diarization.itertracks(yield_label=True):
            segments.append({
                "start": turn.start,
                "end": turn.end,
                "speaker": speaker
            })

        print(f"âœ… èªªè©±è€…åˆ†æå®Œæˆï¼Œè­˜åˆ¥åˆ° {len(set(s['speaker'] for s in segments))} ä½èªªè©±è€…")
        return segments

    except Exception as e:
        print(f"âš ï¸  Speaker diarization å¤±æ•—ï¼š{e}")
        return None


def merge_transcription_with_diarization(
    transcription_segments: List[Dict],
    diarization_segments: List[Dict]
) -> str:
    """åˆä½µè½‰éŒ„æ–‡å­—å’Œèªªè©±è€…æ¨™è¨˜

    Args:
        transcription_segments: Whisper è½‰éŒ„çµæœ (å¸¶æ™‚é–“æˆ³)
        diarization_segments: Speaker diarization çµæœ

    Returns:
        åˆä½µå¾Œçš„æ–‡å­—ï¼Œæ ¼å¼ï¼š[Speaker A] æ–‡å­—å…§å®¹
    """
    if not diarization_segments:
        # æ²’æœ‰ diarization çµæœï¼Œç›´æ¥è¿”å›ç´”æ–‡å­—
        return " ".join(seg.get("text", "") for seg in transcription_segments)

    # ç‚ºæ¯å€‹è½‰éŒ„ç‰‡æ®µåˆ†é…èªªè©±è€…
    result_lines = []
    current_speaker = None
    current_text = []

    for trans_seg in transcription_segments:
        trans_start = trans_seg.get("start", 0)
        trans_end = trans_seg.get("end", 0)
        trans_text = trans_seg.get("text", "")

        if not trans_text.strip():
            continue

        # æ‰¾åˆ°èˆ‡æ­¤è½‰éŒ„ç‰‡æ®µé‡ç–Šæœ€å¤šçš„èªªè©±è€…
        best_speaker = None
        max_overlap = 0

        for dia_seg in diarization_segments:
            dia_start = dia_seg["start"]
            dia_end = dia_seg["end"]

            # è¨ˆç®—é‡ç–Šæ™‚é–“
            overlap_start = max(trans_start, dia_start)
            overlap_end = min(trans_end, dia_end)
            overlap = max(0, overlap_end - overlap_start)

            if overlap > max_overlap:
                max_overlap = overlap
                best_speaker = dia_seg["speaker"]

        # å¦‚æœèªªè©±è€…æ”¹è®Šï¼Œè¼¸å‡ºä¹‹å‰çš„å…§å®¹
        if best_speaker != current_speaker and current_text:
            speaker_label = f"[{current_speaker}]" if current_speaker else ""
            result_lines.append(f"{speaker_label} {''.join(current_text)}")
            current_text = []

        current_speaker = best_speaker
        current_text.append(trans_text)

    # è¼¸å‡ºæœ€å¾Œä¸€æ®µ
    if current_text:
        speaker_label = f"[{current_speaker}]" if current_speaker else ""
        result_lines.append(f"{speaker_label} {''.join(current_text)}")

    return "\n\n".join(result_lines)


def transcribe_audio_in_chunks(
    audio_path: Path,
    model,
    chunk_duration_ms: int = CHUNK_DURATION_MS,
    task_id: str = None,
    diarize: bool = False,
    max_speakers: Optional[int] = None,
    language: Optional[str] = None
) -> str:
    """å°‡éŸ³æª”åˆ†æ®µå¾Œä¸¦è¡Œè½‰éŒ„ï¼Œæé«˜é€Ÿåº¦å’Œæº–ç¢ºåº¦

    Args:
        audio_path: éŸ³æª”è·¯å¾‘
        model: Whisper æ¨¡å‹
        chunk_duration_ms: æ¯æ®µé•·åº¦ï¼ˆæ¯«ç§’ï¼‰
        task_id: ä»»å‹™ ID
        diarize: æ˜¯å¦å•Ÿç”¨èªªè©±è€…è¾¨è­˜ï¼ˆæœƒå…ˆå°å®Œæ•´éŸ³æª”åš diarizationï¼Œå†åˆ‡åˆ†è½‰éŒ„ï¼‰
        max_speakers: æœ€å¤§è¬›è€…äººæ•¸ï¼ˆå¯é¸ï¼Œ2-10ï¼‰
    """
    print(f"ğŸ”Š æª¢æŸ¥éŸ³æª”ï¼š{audio_path.name}")

    # ä½¿ç”¨ ffprobe ç²å–éŸ³æª”è³‡è¨Šï¼Œä¸è¼‰å…¥åˆ°è¨˜æ†¶é«”
    import subprocess
    try:
        result = subprocess.run([
            'ffprobe', '-v', 'quiet', '-print_format', 'json',
            '-show_format', str(audio_path)
        ], capture_output=True, text=True, timeout=10)

        if result.returncode == 0:
            import json
            probe_data = json.loads(result.stdout)
            total_duration_seconds = float(probe_data['format']['duration'])
            total_duration_ms = int(total_duration_seconds * 1000)
            total_minutes = total_duration_ms / 1000 / 60
        else:
            # å¦‚æœ ffprobe å¤±æ•—ï¼Œå›é€€åˆ° pydubï¼ˆä½†æœƒä½”ç”¨è¨˜æ†¶é«”ï¼‰
            print(f"âš ï¸  ffprobe å¤±æ•—ï¼Œå›é€€åˆ° pydub")
            audio = AudioSegment.from_file(audio_path)
            total_duration_ms = len(audio)
            total_minutes = total_duration_ms / 1000 / 60
            del audio  # ç«‹å³é‡‹æ”¾è¨˜æ†¶é«”
    except Exception as e:
        print(f"âš ï¸  ç²å–éŸ³æª”è³‡è¨Šå¤±æ•—ï¼Œå›é€€åˆ° pydub: {e}")
        audio = AudioSegment.from_file(audio_path)
        total_duration_ms = len(audio)
        total_minutes = total_duration_ms / 1000 / 60
        del audio  # ç«‹å³é‡‹æ”¾è¨˜æ†¶é«”

    print(f"ğŸ“Š éŸ³æª”ç¸½é•·åº¦ï¼š{total_minutes:.1f} åˆ†é˜")

    # å¦‚æœéŸ³æª”ä¸é•·ï¼Œç›´æ¥è½‰éŒ„
    if total_duration_ms <= chunk_duration_ms:
        print(f"ğŸ“ éŸ³æª”é•·åº¦åœ¨ {chunk_duration_ms/1000/60:.0f} åˆ†é˜å…§ï¼Œç›´æ¥è½‰éŒ„...")
        text, segments, detected_language = transcribe_single_chunk(model, audio_path, language=language)
        return text, segments, detected_language

    # æ­¥é©Ÿ 1ï¼šå¦‚æœå•Ÿç”¨ diarizationï¼Œåœ¨èƒŒæ™¯ä¸¦è¡ŒåŸ·è¡Œèªªè©±è€…è¾¨è­˜
    diarization_future = None
    diarization_start_time = None
    diarization_executor = None

    # æš«æ™‚å¼·åˆ¶é—œé–‰èªªè©±è€…è¾¨è­˜ï¼Œæ¸¬è©¦æ˜¯å¦æ˜¯ GIL ç«¶çˆ­å•é¡Œ
    if False and diarize and diarization_pipeline:
        diarization_start_time = datetime.now(TZ_UTC8)
        print(f"ğŸ”Š åœ¨èƒŒæ™¯å•Ÿå‹•èªªè©±è€…è¾¨è­˜ï¼ˆèˆ‡è½‰éŒ„ä¸¦è¡ŒåŸ·è¡Œï¼‰...")
        if task_id:
            update_task_status(task_id, {
                "progress": "æ­£åœ¨åˆ†æèªªè©±è€…ï¼ˆèƒŒæ™¯åŸ·è¡Œï¼‰...",
                "diarization_status": "running"
            })

        # ä½¿ç”¨é€²ç¨‹æ± åŸ·è¡Œ diarizationï¼Œå¯ä»¥è¢«å¼·åˆ¶çµ‚æ­¢
        diarization_executor = ProcessPoolExecutor(max_workers=1)
        hf_token = os.getenv("HF_TOKEN", "")
        diarization_future = diarization_executor.submit(
            perform_diarization_in_process,
            str(audio_path),
            max_speakers,
            hf_token
        )

        # è¨˜éŒ„ diarization é€²ç¨‹ä¾›å–æ¶ˆæ™‚ä½¿ç”¨
        if task_id:
            with tasks_lock:
                task_diarization_processes[task_id] = diarization_executor

    # é•·éŸ³æª”ï¼šåˆ†æ®µè™•ç†
    num_chunks = (total_duration_ms + chunk_duration_ms - 1) // chunk_duration_ms
    print(f"ğŸ”„ éŸ³æª”è¼ƒé•·ï¼Œå°‡åˆ†ç‚º {num_chunks} æ®µè™•ç†ï¼ˆæ¯æ®µç´„ {chunk_duration_ms/1000/60:.0f} åˆ†é˜ï¼‰...")

    # åˆå§‹åŒ– chunks ç‹€æ…‹è¿½è¹¤
    if task_id:
        chunks_state = [
            {
                "chunk_id": i,
                "status": "pending",
                "started_at": None,
                "completed_at": None,
                "duration_seconds": None
            }
            for i in range(1, num_chunks + 1)
        ]

        update_task_status(task_id, {
            "total_chunks": num_chunks,
            "completed_chunks": 0,
            "chunks": chunks_state,
            "chunks_created": False  # åˆ‡åˆ†å°šæœªå®Œæˆ
            # ä¸è¨­ç½® estimated_completion_timeï¼Œå‰ç«¯æœƒé¡¯ç¤ºã€Œè¨ˆç®—ä¸­......ã€
        })

    # ç¬¬ä¸€æ­¥ï¼šæº–å‚™æ‰€æœ‰ chunksï¼ˆä½¿ç”¨ ffmpeg ç›´æ¥åˆ‡åˆ†ï¼Œé¿å…è¨˜æ†¶é«”å•é¡Œï¼‰
    print(f"ğŸ“¦ æº–å‚™åˆ‡åˆ†éŸ³æª”ï¼ˆä½¿ç”¨ ffmpeg æµå¼è™•ç†ï¼‰...")
    chunk_info_list = []  # å„²å­˜ (chunk_idx, start_ms, end_ms, temp_path)
    start_ms = 0
    chunk_idx = 1

    while start_ms < total_duration_ms:
        end_ms = min(start_ms + chunk_duration_ms, total_duration_ms)

        print(f"   æº–å‚™ç¬¬ {chunk_idx}/{num_chunks} æ®µ ({start_ms/1000/60:.1f}-{end_ms/1000/60:.1f} åˆ†é˜)...")

        # æª¢æŸ¥æ˜¯å¦å·²è¢«å–æ¶ˆ
        if task_id and task_cancelled.get(task_id, False):
            raise RuntimeError("ä»»å‹™å·²è¢«ä½¿ç”¨è€…å–æ¶ˆ")

        # ä½¿ç”¨ ffmpeg ç›´æ¥åˆ‡åˆ†ï¼Œä¸è¼‰å…¥åˆ°è¨˜æ†¶é«”
        temp_path = audio_path.parent / f"_temp_{audio_path.stem}_chunk_{chunk_idx}.wav"
        start_seconds = start_ms / 1000.0
        duration_seconds = (end_ms - start_ms) / 1000.0

        try:
            # ä½¿ç”¨ ffmpeg åˆ‡åˆ†éŸ³æª”ï¼ˆæµå¼è™•ç†ï¼Œä¸ä½”ç”¨è¨˜æ†¶é«”ï¼‰
            subprocess.run([
                'ffmpeg', '-y', '-i', str(audio_path),
                '-ss', str(start_seconds),
                '-t', str(duration_seconds),
                '-acodec', 'pcm_s16le',  # WAV æ ¼å¼
                '-ar', '16000',  # 16kHz æ¡æ¨£ç‡ï¼ˆWhisper æ¨è–¦ï¼‰
                '-ac', '1',  # å–®è²é“
                str(temp_path)
            ], check=True, capture_output=True, timeout=60)
        except subprocess.TimeoutExpired:
            print(f"   âš ï¸  åˆ‡åˆ†ç¬¬ {chunk_idx} æ®µè¶…æ™‚ï¼Œå˜—è©¦ä½¿ç”¨ pydub")
            # å›é€€åˆ° pydubï¼ˆè¼ƒæ…¢ä½†æ›´ç©©å®šï¼‰
            audio = AudioSegment.from_file(audio_path)
            chunk_audio = audio[start_ms:end_ms]
            chunk_audio.export(temp_path, format="wav")
            del audio, chunk_audio  # ç«‹å³é‡‹æ”¾
        except Exception as e:
            print(f"   âš ï¸  ffmpeg åˆ‡åˆ†å¤±æ•—: {e}ï¼Œå›é€€åˆ° pydub")
            # å›é€€åˆ° pydub
            audio = AudioSegment.from_file(audio_path)
            chunk_audio = audio[start_ms:end_ms]
            chunk_audio.export(temp_path, format="wav")
            del audio, chunk_audio  # ç«‹å³é‡‹æ”¾

        chunk_info_list.append((chunk_idx, start_ms, end_ms, temp_path))

        start_ms = end_ms
        chunk_idx += 1

    print(f"âœ… å·²æº–å‚™ {len(chunk_info_list)} å€‹éŸ³æª”ç‰‡æ®µï¼ˆè¨˜æ†¶é«”å‹å¥½æ¨¡å¼ï¼‰")

    # æ¨™è¨˜åˆ‡åˆ†å®Œæˆä¸¦è¨ˆç®—é ä¼°å®Œæˆæ™‚é–“
    if task_id:
        # æ ¹æ“šéŸ³æª”æ™‚é•·è¨ˆç®—é ä¼°è™•ç†æ™‚é–“ï¼šéŸ³æª”æ™‚é•·çš„ 3/5
        estimated_minutes = total_minutes * 3 / 5

        # å–å¾—ä»»å‹™é–‹å§‹æ™‚é–“ä¸¦è¨ˆç®—é ä¼°å®Œæˆæ™‚é–“
        task = run_async_in_thread(get_task_from_db(task_id))
        if task:
            started_at_str = task.get("started_at")
            if started_at_str:
                started_at = datetime.strptime(started_at_str, "%Y-%m-%d %H:%M:%S").replace(tzinfo=TZ_UTC8)
                estimated_completion = started_at + timedelta(minutes=estimated_minutes)
                estimated_completion_str = estimated_completion.strftime("%Y-%m-%d %H:%M:%S")
            else:
                estimated_completion_str = None
        else:
            estimated_completion_str = None

        update_task_status(task_id, {
            "chunks_created": True,
            "progress": f"å·²åˆ‡åˆ†ç‚º {num_chunks} å€‹ç‰‡æ®µï¼Œé–‹å§‹è½‰éŒ„...",
            "estimated_completion_time": estimated_completion_str
        })

    # ç¬¬äºŒæ­¥ï¼šå¾ªåºè½‰éŒ„æ‰€æœ‰ chunksï¼ˆé¿å…å·¢ç‹€ ThreadPoolExecutor æ­»é–ï¼‰
    # æš«æ™‚æ”¹ç‚ºå¾ªåºè™•ç†ï¼Œé¿å…èˆ‡å¤–å±¤ executor ç”¢ç”Ÿè¡çª
    print(f"ğŸš€ é–‹å§‹å¾ªåºè½‰éŒ„ï¼ˆå…± {num_chunks} æ®µï¼‰{'ï¼ŒåŒæ™‚é€²è¡Œèªªè©±è€…è¾¨è­˜' if diarization_future else ''}...")
    chunks_text = [None] * num_chunks  # é å…ˆåˆ†é…é™£åˆ—ä¿æŒé †åº
    all_segments = []  # å§‹çµ‚æ”¶é›†æ‰€æœ‰ segments

    try:
        # æš«æ™‚ä½¿ç”¨å¾ªåºè™•ç†é¿å…å·¢ç‹€ ThreadPoolExecutor æ­»é–
        # å¾ªåºè™•ç†æ¯å€‹ chunk
        detected_language = None
        for chunk_idx, start_ms, end_ms, temp_path in chunk_info_list:
            # æª¢æŸ¥æ˜¯å¦å·²è¢«å–æ¶ˆ
            if task_id and task_cancelled.get(task_id, False):
                raise RuntimeError("ä»»å‹™å·²è¢«ä½¿ç”¨è€…å–æ¶ˆ")

            # è¨ˆç®—æ™‚é–“åç§»
            time_offset_seconds = start_ms / 1000.0

            try:
                # è½‰éŒ„æ­¤ chunk
                print(f"   ğŸ”„ æ­£åœ¨è½‰éŒ„ç¬¬ {chunk_idx}/{num_chunks} æ®µ...")
                text, segments, lang = transcribe_single_chunk(
                    whisper_model,
                    temp_path,
                    language,
                    task_id,
                    chunk_idx,
                    time_offset_seconds,
                    True
                )

                # è¨˜éŒ„ç¬¬ä¸€å€‹ chunk çš„èªè¨€
                if detected_language is None and lang:
                    detected_language = lang

                # å„²å­˜çµæœ
                chunks_text[chunk_idx - 1] = text
                all_segments.extend(segments if segments else [])

                # æ›´æ–°é€²åº¦
                completed = chunk_idx
                if task_id:
                    update_task_status(task_id, {
                        "progress": f"æ­£åœ¨è½‰éŒ„éŸ³è¨Š... ({completed}/{num_chunks} æ®µå®Œæˆ)",
                        "completed_chunks": completed
                    }, persist_to_db=False)

            except Exception as e:
                print(f"   âŒ ç¬¬ {chunk_idx} æ®µè½‰éŒ„å¤±æ•—ï¼š{e}")
                raise

        # ä»¥ä¸‹æ˜¯åŸæœ¬çš„ä¸¦è¡Œè™•ç†ç¨‹å¼ç¢¼ï¼ˆæš«æ™‚åœç”¨ï¼‰
        if False:
            future_to_chunk = {}
            for chunk_idx, start_ms, end_ms, temp_path in chunk_info_list:
                # è¨ˆç®—é€™å€‹ chunk çš„æ™‚é–“åç§»ï¼ˆç›¸å°æ–¼å®Œæ•´éŸ³æª”çš„ç§’æ•¸ï¼‰
                time_offset_seconds = start_ms / 1000.0

                # å§‹çµ‚è¿”å›å¸¶æ™‚é–“æˆ³çš„ segments
                future = executor.submit(
                    transcribe_single_chunk,
                    model,
                    temp_path,
                    language,  # ä½¿ç”¨æŒ‡å®šçš„èªè¨€
                    task_id,
                    chunk_idx,
                    time_offset_seconds,
                    True  # return_segments å§‹çµ‚ç‚º True
                )
                future_to_chunk[future] = (chunk_idx, temp_path, time_offset_seconds)

            # ç­‰å¾…å®Œæˆä¸¦æ›´æ–°é€²åº¦
            detected_language = None  # ç”¨æ–¼è¨˜éŒ„ç¬¬ä¸€å€‹ chunk åµæ¸¬åˆ°çš„èªè¨€
            for future in as_completed(future_to_chunk):
                chunk_idx, temp_path, time_offset = future_to_chunk[future]

                # æª¢æŸ¥æ˜¯å¦å·²è¢«å–æ¶ˆ
                if task_id and task_cancelled.get(task_id, False):
                    executor.shutdown(wait=False, cancel_futures=True)
                    # å¼·åˆ¶çµ‚æ­¢ diarization é€²ç¨‹
                    if diarization_future and diarization_executor:
                        print(f"ğŸ›‘ æ­£åœ¨å¼·åˆ¶çµ‚æ­¢èªªè©±è€…è¾¨è­˜é€²ç¨‹...")
                        diarization_executor.shutdown(wait=False, cancel_futures=True)
                    raise RuntimeError("ä»»å‹™å·²è¢«ä½¿ç”¨è€…å–æ¶ˆ")

                try:
                    # è¨˜æ†¶é«”å„ªåŒ–ï¼šå¾è¨˜æ†¶é«”ç²å– chunk é–‹å§‹æ™‚é–“ï¼Œé¿å…æŸ¥è©¢è³‡æ–™åº«
                    chunk_start_time = None
                    if task_id:
                        with tasks_lock:
                            live_info = transcription_tasks.get(task_id, {})
                            chunks = live_info.get("chunks", [])
                            if chunk_idx - 1 < len(chunks):
                                start_str = chunks[chunk_idx - 1].get("started_at")
                                if start_str:
                                    try:
                                        chunk_start_time = datetime.strptime(start_str, "%Y-%m-%d %H:%M:%S")
                                    except:
                                        pass  # å¿½ç•¥è§£æéŒ¯èª¤

                    result = future.result()

                    # è™•ç†è¿”å›çµæœï¼ˆç¾åœ¨æ˜¯ (æ–‡å­—, segments, detected_language) å…ƒçµ„ï¼‰
                    chunk_text, chunk_segments, chunk_detected_language = result
                    all_segments.extend(chunk_segments)  # æ”¶é›†æ‰€æœ‰ segments

                    # è¨˜éŒ„ç¬¬ä¸€å€‹ chunk åµæ¸¬åˆ°çš„èªè¨€
                    if detected_language is None and chunk_detected_language:
                        detected_language = chunk_detected_language

                    chunks_text[chunk_idx - 1] = chunk_text
                    print(f"   âœ… å®Œæˆç¬¬ {chunk_idx}/{num_chunks} æ®µ")

                    # è¨ˆç®— chunk è™•ç†æ™‚é–“
                    chunk_duration = None
                    if chunk_start_time:
                        chunk_end_time = datetime.now(TZ_UTC8)
                        chunk_duration = (chunk_end_time - chunk_start_time.replace(tzinfo=TZ_UTC8)).total_seconds()

                    # æ›´æ–° chunk ç‹€æ…‹ç‚º completed
                    completed = sum(1 for t in chunks_text if t is not None)
                    if task_id:
                        # å¾è¨˜æ†¶é«”ç²å– chunks
                        with tasks_lock:
                            chunks = transcription_tasks.get(task_id, {}).get("chunks", [])
                            if not chunks:
                                task = run_async_in_thread(get_task_from_db(task_id))
                                if task:
                                    chunks = task.get("chunks", [])

                            if chunk_idx - 1 < len(chunks):
                                chunks[chunk_idx - 1]["status"] = "completed"
                                chunks[chunk_idx - 1]["completed_at"] = get_current_time()
                                if chunk_duration:
                                    chunks[chunk_idx - 1]["duration_seconds"] = round(chunk_duration, 1)

                        # åƒ…æ›´æ–°è¨˜æ†¶é«”ï¼ˆchunks, progress, completed_chunks éƒ½æ˜¯è¨˜æ†¶é«”å°ˆç”¨æ¬„ä½ï¼‰
                        update_task_status(task_id, {
                            "chunks": chunks,
                            "progress": f"æ­£åœ¨è½‰éŒ„éŸ³è¨Š... ({completed}/{num_chunks} æ®µå®Œæˆ)",
                            "completed_chunks": completed
                        }, persist_to_db=False)

                except Exception as e:
                    print(f"   âŒ ç¬¬ {chunk_idx} æ®µè½‰éŒ„å¤±æ•—ï¼š{e}")
                    raise

    finally:
        # æ¸…ç†æ‰€æœ‰è‡¨æ™‚æª”æ¡ˆ
        for _, _, _, temp_path in chunk_info_list:
            try:
                if temp_path.exists():
                    temp_path.unlink()
            except Exception as e:
                print(f"âš ï¸ æ¸…ç†è‡¨æ™‚æª”æ¡ˆå¤±æ•—ï¼š{e}")

        # ç¢ºä¿ diarization executor è¢«æ­£ç¢ºé—œé–‰
        if diarization_executor:
            try:
                diarization_executor.shutdown(wait=False, cancel_futures=True)
                print(f"âœ… Diarization é€²ç¨‹å·²é—œé–‰")
            except Exception as e:
                print(f"âš ï¸ é—œé–‰ diarization executor å¤±æ•—ï¼š{e}")

        # æ¸…ç† diarization é€²ç¨‹è¨˜éŒ„
        if task_id:
            with tasks_lock:
                task_diarization_processes.pop(task_id, None)

    print("âœ… æ‰€æœ‰éŸ³æª”ç‰‡æ®µè½‰éŒ„å®Œæˆ")

    # ç¬¬ä¸‰æ­¥ï¼šå¦‚æœå•Ÿç”¨äº† diarizationï¼Œç­‰å¾…èªªè©±è€…è¾¨è­˜å®Œæˆä¸¦åˆä½µè³‡è¨Š
    if diarization_future:
        print(f"â³ ç­‰å¾…èªªè©±è€…è¾¨è­˜å®Œæˆ...")
        if task_id:
            update_task_status(task_id, {"progress": "ç­‰å¾…èªªè©±è€…è¾¨è­˜å®Œæˆ..."})

        try:
            # ç­‰å¾… diarization å®Œæˆä¸¦å–å¾—çµæœ
            diarization_segments = diarization_future.result()
            if diarization_executor:
                diarization_executor.shutdown(wait=True)

            # è¨ˆç®— diarization å¯¦éš›è€—æ™‚
            if diarization_start_time:
                diarization_duration = (datetime.now(TZ_UTC8) - diarization_start_time).total_seconds()
            else:
                diarization_duration = 0

            if diarization_segments:
                num_speakers = len(set(s['speaker'] for s in diarization_segments))
                print(f"âœ… èªªè©±è€…è¾¨è­˜å®Œæˆï¼Œè­˜åˆ¥åˆ° {num_speakers} ä½èªªè©±è€… (è€—æ™‚ {format_duration(diarization_duration)})")
                if task_id:
                    update_task_status(task_id, {
                        "progress": f"èªªè©±è€…è¾¨è­˜å®Œæˆ ({num_speakers} ä½èªªè©±è€…ï¼Œè€—æ™‚ {format_duration(diarization_duration)})",
                        "diarization_status": "completed",
                        "diarization_num_speakers": num_speakers,
                        "diarization_duration_seconds": round(diarization_duration, 1)
                    })

                # åˆä½µèªªè©±è€…è³‡è¨Šèˆ‡è½‰éŒ„æ–‡å­—
                print(f"ğŸ”— æ­£åœ¨åˆä½µèªªè©±è€…è³‡è¨Šèˆ‡è½‰éŒ„æ–‡å­—...")
                if task_id:
                    update_task_status(task_id, {"progress": "æ­£åœ¨åˆä½µèªªè©±è€…è³‡è¨Š..."})

                # æŒ‰æ™‚é–“æ’åº segmentsï¼ˆç¢ºä¿é †åºæ­£ç¢ºï¼‰
                all_segments.sort(key=lambda s: s["start"])

                final_text = merge_transcription_with_diarization(all_segments, diarization_segments)
                print(f"âœ… èªªè©±è€…è³‡è¨Šåˆä½µå®Œæˆ")
                return final_text, all_segments, detected_language
            else:
                print(f"âš ï¸  èªªè©±è€…è¾¨è­˜å¤±æ•—ï¼Œè¿”å›ç´”æ–‡å­—è½‰éŒ„")
                if task_id:
                    update_task_status(task_id, {
                        "diarization_status": "failed"
                    })
                return " ".join(chunks_text), all_segments, detected_language

        except Exception as e:
            print(f"âš ï¸  ç­‰å¾…èªªè©±è€…è¾¨è­˜æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            print(f"   å°‡è¿”å›ç´”æ–‡å­—è½‰éŒ„")
            if task_id:
                update_task_status(task_id, {
                    "diarization_status": "failed"
                })
            return " ".join(chunks_text), all_segments, detected_language
    else:
        # æ²’æœ‰ diarizationï¼Œè¿”å›ç´”æ–‡å­—å’Œ segments
        return " ".join(chunks_text), all_segments, detected_language


def detect_language_with_llm(text: str, provider: str = "gemini") -> str:
    """ä½¿ç”¨ LLM è‡ªå‹•è¾¨è­˜æ–‡å­—èªè¨€

    Args:
        text: è¦è¾¨è­˜çš„æ–‡å­—ï¼ˆå»ºè­°ä½¿ç”¨å‰å¹¾ç™¾å­—å³å¯ï¼‰
        provider: ä½¿ç”¨çš„ LLM æä¾›è€… (gemini/openai)

    Returns:
        èªè¨€ä»£ç¢¼ (zh/en/ja/ko ç­‰)
    """
    # åªå–å‰ 100 å­—é€²è¡Œè¾¨è­˜ä»¥ç¯€çœæˆæœ¬
    sample_text = text[:100]

    prompt = f"""Please identify the primary language of the following text and respond with ONLY the language code (zh/en/ja/ko/es/fr/de/etc.).

Text:
{sample_text}

Language code:"""

    try:
        if provider == "gemini":
            result = call_gemini_with_retry(prompt).strip().lower()
        else:  # openai
            from openai import OpenAI
            client = OpenAI()
            resp = client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[
                    {"role": "system", "content": "You are a language detection assistant. Respond only with language codes."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
            )
            result = resp.choices[0].message.content.strip().lower()

        # é©—è­‰çµæœæ˜¯å¦ç‚ºæœ‰æ•ˆçš„èªè¨€ä»£ç¢¼
        valid_codes = ["zh", "en", "ja", "ko", "es", "fr", "de", "it", "pt", "ru", "ar", "hi", "th", "vi"]
        if result in valid_codes:
            return result

        # å¦‚æœä¸æ˜¯æ¨™æº–ä»£ç¢¼ï¼Œå˜—è©¦æ˜ å°„å¸¸è¦‹å›æ‡‰
        if "chinese" in result or "ä¸­æ–‡" in result:
            return "zh"
        elif "english" in result:
            return "en"
        elif "japanese" in result or "æ—¥æœ¬" in result:
            return "ja"
        elif "korean" in result or "éŸ“" in result:
            return "ko"

        # é è¨­è¿”å›ä¸­æ–‡
        print(f"âš ï¸ èªè¨€è¾¨è­˜çµæœä¸æ˜ç¢º: {result}ï¼Œé è¨­ä½¿ç”¨ä¸­æ–‡")
        return "zh"

    except Exception as e:
        print(f"âš ï¸ èªè¨€è¾¨è­˜å¤±æ•—: {e}ï¼Œé è¨­ä½¿ç”¨ä¸­æ–‡")
        return "zh"


def get_punctuation_prompt(language: str, text: str) -> tuple[str, str]:
    """æ ¹æ“šèªè¨€ç”Ÿæˆé©ç•¶çš„æ¨™é»æç¤ºèª

    Args:
        language: èªè¨€ä»£ç¢¼ (zh/en/ja/ko/ç­‰)ï¼Œç”± Whisper è‡ªå‹•åµæ¸¬æˆ–ç”¨æˆ¶æŒ‡å®š
        text: è¦è™•ç†çš„æ–‡å­—

    Returns:
        (system_message, user_message) å…ƒçµ„
    """
    if language == "zh":
        system_msg = "ä½ æ˜¯åš´è¬¹çš„é€å­—ç¨¿æ½¤é£¾åŠ©æ‰‹ï¼Œåªåšæ¨™é»èˆ‡åˆ†æ®µã€‚"
        user_msg = (
            "è«‹å°‡ä»¥ä¸‹ã€ä¸­æ–‡é€å­—ç¨¿ã€åŠ ä¸Šé©ç•¶æ¨™é»ç¬¦è™Ÿä¸¦åˆç†åˆ†æ®µã€‚"
            "ä¸è¦çœç•¥æˆ–æ·»åŠ å…§å®¹ï¼Œä¸è¦æ„è­¯ï¼Œä¿ç•™å›ºæœ‰åè©èˆ‡æ•¸å­—ã€‚"
            f"è¼¸å‡ºç´”æ–‡å­—å³å¯ï¼š\n\n{text}"
        )
    elif language == "en":
        system_msg = "You are a precise transcript editor. Only add punctuation and paragraphing."
        user_msg = (
            "Please add appropriate punctuation and paragraphing to the following English transcript. "
            "Do not omit or add content, do not paraphrase, preserve proper nouns and numbers. "
            f"Output plain text only:\n\n{text}"
        )
    elif language == "ja":
        system_msg = "ã‚ãªãŸã¯æ­£ç¢ºãªæ–‡å­—èµ·ã“ã—ç·¨é›†è€…ã§ã™ã€‚å¥èª­ç‚¹ã¨æ®µè½åˆ†ã‘ã®ã¿ã‚’è¡Œã„ã¾ã™ã€‚"
        user_msg = (
            "ä»¥ä¸‹ã®æ—¥æœ¬èªæ–‡å­—èµ·ã“ã—ã«é©åˆ‡ãªå¥èª­ç‚¹ã¨æ®µè½ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚"
            "å†…å®¹ã®çœç•¥ã‚„è¿½åŠ ã¯ã›ãšã€æ„è¨³ã›ãšã€å›ºæœ‰åè©ã¨æ•°å­—ã¯ãã®ã¾ã¾ä¿æŒã—ã¦ãã ã•ã„ã€‚"
            f"ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã¿å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š\n\n{text}"
        )
    elif language == "ko":
        system_msg = "ë‹¹ì‹ ì€ ì •í™•í•œ ì „ì‚¬ í¸ì§‘ìì…ë‹ˆë‹¤. êµ¬ë‘ì ê³¼ ë‹¨ë½ ë‚˜ëˆ„ê¸°ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤."
        user_msg = (
            "ë‹¤ìŒ í•œêµ­ì–´ ì „ì‚¬ì— ì ì ˆí•œ êµ¬ë‘ì ê³¼ ë‹¨ë½ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”. "
            "ë‚´ìš©ì„ ìƒëµí•˜ê±°ë‚˜ ì¶”ê°€í•˜ì§€ ë§ê³ , ì˜ì—­í•˜ì§€ ë§ê³ , ê³ ìœ ëª…ì‚¬ì™€ ìˆ«ìëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”. "
            f"ì¼ë°˜ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”:\n\n{text}"
        )
    else:
        # å…¶ä»–èªè¨€ä½¿ç”¨è‹±æ–‡æç¤º
        system_msg = "You are a precise transcript editor. Only add punctuation and paragraphing."
        user_msg = (
            f"Please add appropriate punctuation and paragraphing to the following transcript. "
            "Do not omit or add content, do not paraphrase, preserve proper nouns and numbers. "
            f"Output plain text only:\n\n{text}"
        )

    return system_msg, user_msg


def punctuate_with_openai(text: str, language: str = "zh") -> str:
    """ç”¨ OpenAI å¹«é€å­—ç¨¿åŠ æ¨™é»èˆ‡åˆ†æ®µï¼ˆæ”¯æ´å¤šèªè¨€ï¼‰

    Args:
        text: è¦åŠ æ¨™é»çš„æ–‡å­—
        language: èªè¨€ä»£ç¢¼ (zh/en/ja/ko/auto)ï¼Œauto æœƒè‡ªå‹•è¾¨è­˜
    """
    from openai import OpenAI
    client = OpenAI()

    # ç²å–å°æ‡‰èªè¨€çš„æç¤ºèª
    system_msg, user_msg = get_punctuation_prompt(language, text)

    resp = client.chat.completions.create(
        model=OPENAI_MODEL,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_msg},
        ],
        temperature=0.2,
    )
    return resp.choices[0].message.content.strip()


def call_gemini_with_retry(prompt: str, max_retries: int = None, return_usage: bool = False):
    """èª¿ç”¨ Gemini APIï¼Œæ”¯æ´è‡ªå‹•é‡è©¦å’Œ Key åˆ‡æ›ï¼Œé…é¡è€—ç›¡æ™‚è‡ªå‹•åˆ‡æ›åˆ°å¤šå±¤å‚™æ´æ¨¡å‹

    Args:
        prompt: æç¤ºæ–‡å­—
        max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸
        return_usage: æ˜¯å¦å›å‚³ token ä½¿ç”¨é‡è³‡è¨Š

    Returns:
        è‹¥ return_usage=False: å›å‚³æ–‡å­—å­—ä¸²
        è‹¥ return_usage=True: å›å‚³ (æ–‡å­—, token_usage_dict)
    """
    import google.generativeai as genai

    if max_retries is None:
        max_retries = len(GOOGLE_API_KEYS)

    last_error = None
    quota_exceeded_count = 0
    current_model = GEMINI_MODEL
    fallback_index = -1  # è¿½è¹¤ç•¶å‰ä½¿ç”¨çš„å‚™æ´æ¨¡å‹ç´¢å¼•
    tried_models = [GEMINI_MODEL]  # è¿½è¹¤å·²å˜—è©¦çš„æ¨¡å‹

    for attempt in range(max_retries * (len(GEMINI_FALLBACK_MODELS) + 1)):  # æ“´å¤§é‡è©¦æ¬¡æ•¸ä»¥æ”¯æ´å¤šå±¤å‚™æ´
        try:
            # ç²å–ä¸‹ä¸€å€‹ API Key
            api_key = get_next_google_api_key()
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel(current_model)

            # èª¿ç”¨ API
            resp = model.generate_content(
                [{"role": "user", "parts": [prompt]}],
                generation_config={"temperature": 0.2}
            )

            result = (resp.text or "").strip()
            if fallback_index >= 0:
                print(f"âœ… ä½¿ç”¨å‚™æ´æ¨¡å‹ {current_model} æˆåŠŸ")

            # æå– token ä½¿ç”¨é‡è³‡è¨Š
            if return_usage and hasattr(resp, 'usage_metadata'):
                usage = {
                    "prompt_tokens": getattr(resp.usage_metadata, 'prompt_token_count', 0),
                    "completion_tokens": getattr(resp.usage_metadata, 'candidates_token_count', 0),
                    "total_tokens": getattr(resp.usage_metadata, 'total_token_count', 0),
                    "model": current_model
                }
                print(f"ğŸ“Š Token ä½¿ç”¨: {usage['total_tokens']} (è¼¸å…¥: {usage['prompt_tokens']}, è¼¸å‡º: {usage['completion_tokens']})")
                return result, usage

            return result

        except Exception as e:
            last_error = e
            error_msg = str(e)

            # æª¢æŸ¥æ˜¯å¦ç‚º 429 é…é¡éŒ¯èª¤
            is_quota_error = "429" in error_msg or "quota" in error_msg.lower() or "Quota exceeded" in error_msg

            if is_quota_error:
                quota_exceeded_count += 1
                print(f"âš ï¸ Google API Key é…é¡å·²ç”¨å®Œ (å˜—è©¦ {attempt + 1}ï¼Œæ¨¡å‹: {current_model})")

                # å¦‚æœæ‰€æœ‰ keys éƒ½é…é¡è€—ç›¡ï¼Œå˜—è©¦åˆ‡æ›åˆ°ä¸‹ä¸€å€‹å‚™æ´æ¨¡å‹
                if quota_exceeded_count >= len(GOOGLE_API_KEYS):
                    fallback_index += 1

                    if fallback_index < len(GEMINI_FALLBACK_MODELS):
                        current_model = GEMINI_FALLBACK_MODELS[fallback_index]
                        print(f"ğŸ’¡ æ‰€æœ‰ API Keys åœ¨ {tried_models[-1]} çš„é…é¡å·²ç”¨å®Œï¼Œåˆ‡æ›åˆ°å‚™ç”¨æ¨¡å‹ {current_model}")
                        tried_models.append(current_model)
                        quota_exceeded_count = 0  # é‡ç½®è¨ˆæ•¸ï¼Œç”¨æ–°å‚™æ´æ¨¡å‹å†è©¦ä¸€è¼ª
                        continue
                    else:
                        # æ‰€æœ‰å‚™æ´æ¨¡å‹éƒ½ç”¨å®Œäº†
                        print(f"âŒ æ‰€æœ‰æ¨¡å‹ï¼ˆ{', '.join(tried_models)}ï¼‰çš„é…é¡éƒ½å·²ç”¨å®Œ")
                        raise RuntimeError(f"æ‰€æœ‰ Google API Keys éƒ½èª¿ç”¨å¤±æ•—ã€‚å·²å˜—è©¦æ¨¡å‹: {', '.join(tried_models)}ã€‚æœ€å¾ŒéŒ¯èª¤: {error_msg}") from last_error
            else:
                print(f"âš ï¸ Google API Key èª¿ç”¨å¤±æ•— (å˜—è©¦ {attempt + 1}): {error_msg}")

            # å¦‚æœé‚„æœ‰ key å¯ç”¨ï¼Œç¹¼çºŒå˜—è©¦
            if attempt < max_retries * (len(GEMINI_FALLBACK_MODELS) + 1) - 1:
                print(f"ğŸ”„ åˆ‡æ›åˆ°ä¸‹ä¸€å€‹ API Key...")
                continue
            else:
                print(f"âŒ æ‰€æœ‰ API Keys éƒ½å·²å˜—è©¦ï¼Œå¤±æ•—")
                raise RuntimeError(f"æ‰€æœ‰ Google API Keys éƒ½èª¿ç”¨å¤±æ•—ã€‚å·²å˜—è©¦æ¨¡å‹: {', '.join(tried_models)}ã€‚æœ€å¾ŒéŒ¯èª¤: {error_msg}") from last_error

    raise RuntimeError(f"ç„¡æ³•èª¿ç”¨ Gemini APIã€‚å·²å˜—è©¦æ¨¡å‹: {', '.join(tried_models)}") from last_error


def get_chunked_punctuation_prompt(language: str, chunk_text: str, chunk_idx: int, total_chunks: int) -> tuple[str, str]:
    """ç‚ºé•·æ–‡æœ¬åˆ†æ®µç”Ÿæˆæç¤ºèª

    Args:
        language: èªè¨€ä»£ç¢¼
        chunk_text: ç•¶å‰åˆ†æ®µæ–‡å­—
        chunk_idx: ç•¶å‰åˆ†æ®µç´¢å¼•ï¼ˆå¾1é–‹å§‹ï¼‰
        total_chunks: ç¸½åˆ†æ®µæ•¸

    Returns:
        (system_message, user_message) å…ƒçµ„
    """
    if language == "zh":
        system_msg = "ä½ æ˜¯åš´è¬¹çš„é€å­—ç¨¿æ½¤é£¾åŠ©æ‰‹ã€‚åªåšã€ä¸­æ–‡æ¨™é»è£œå…¨èˆ‡åˆç†åˆ†æ®µã€ï¼Œä¸è¦çœç•¥æˆ–æ·»åŠ å…§å®¹ï¼Œä¸è¦æ„è­¯ï¼Œéå¿…è¦ä¸è¦ç”¨åˆªç¯€è™Ÿï¼Œä¿ç•™å›ºæœ‰åè©èˆ‡æ•¸å­—ã€‚"
        if chunk_idx == 1:
            user_msg = f"è«‹ç‚ºä»¥ä¸‹ä¸­æ–‡é€å­—ç¨¿åŠ ä¸Šé©ç•¶æ¨™é»ä¸¦åˆ†æ®µï¼ˆé€™æ˜¯ç¬¬ 1 æ®µï¼‰ï¼š\n\n{chunk_text}"
        elif chunk_idx == total_chunks:
            user_msg = f"è«‹ç‚ºä»¥ä¸‹ä¸­æ–‡é€å­—ç¨¿åŠ ä¸Šé©ç•¶æ¨™é»ä¸¦åˆ†æ®µï¼ˆé€™æ˜¯æœ€å¾Œä¸€æ®µï¼Œæ¥çºŒå‰æ–‡ï¼‰ï¼š\n\n{chunk_text}"
        else:
            user_msg = f"è«‹ç‚ºä»¥ä¸‹ä¸­æ–‡é€å­—ç¨¿åŠ ä¸Šé©ç•¶æ¨™é»ä¸¦åˆ†æ®µï¼ˆé€™æ˜¯ç¬¬ {chunk_idx} æ®µï¼Œæ¥çºŒå‰æ–‡ï¼‰ï¼š\n\n{chunk_text}"
    elif language == "en":
        system_msg = "You are a precise transcript editor. Only add punctuation and paragraphing. Do not omit or add content, do not paraphrase, preserve proper nouns and numbers."
        if chunk_idx == 1:
            user_msg = f"Add punctuation and paragraphing to this English transcript (part 1):\n\n{chunk_text}"
        elif chunk_idx == total_chunks:
            user_msg = f"Add punctuation and paragraphing to this English transcript (final part, continuing from previous):\n\n{chunk_text}"
        else:
            user_msg = f"Add punctuation and paragraphing to this English transcript (part {chunk_idx}, continuing from previous):\n\n{chunk_text}"
    elif language == "ja":
        system_msg = "ã‚ãªãŸã¯æ­£ç¢ºãªæ–‡å­—èµ·ã“ã—ç·¨é›†è€…ã§ã™ã€‚å¥èª­ç‚¹ã¨æ®µè½åˆ†ã‘ã®ã¿ã‚’è¡Œã„ã¾ã™ã€‚å†…å®¹ã®çœç•¥ã‚„è¿½åŠ ã¯ã›ãšã€æ„è¨³ã›ãšã€å›ºæœ‰åè©ã¨æ•°å­—ã¯ãã®ã¾ã¾ä¿æŒã—ã¦ãã ã•ã„ã€‚"
        if chunk_idx == 1:
            user_msg = f"ä»¥ä¸‹ã®æ—¥æœ¬èªæ–‡å­—èµ·ã“ã—ã«å¥èª­ç‚¹ã¨æ®µè½ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ï¼ˆç¬¬1éƒ¨åˆ†ï¼‰ï¼š\n\n{chunk_text}"
        elif chunk_idx == total_chunks:
            user_msg = f"ä»¥ä¸‹ã®æ—¥æœ¬èªæ–‡å­—èµ·ã“ã—ã«å¥èª­ç‚¹ã¨æ®µè½ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ï¼ˆæœ€å¾Œã®éƒ¨åˆ†ã€å‰ã®ç¶šãï¼‰ï¼š\n\n{chunk_text}"
        else:
            user_msg = f"ä»¥ä¸‹ã®æ—¥æœ¬èªæ–‡å­—èµ·ã“ã—ã«å¥èª­ç‚¹ã¨æ®µè½ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ï¼ˆç¬¬{chunk_idx}éƒ¨åˆ†ã€å‰ã®ç¶šãï¼‰ï¼š\n\n{chunk_text}"
    elif language == "ko":
        system_msg = "ë‹¹ì‹ ì€ ì •í™•í•œ ì „ì‚¬ í¸ì§‘ìì…ë‹ˆë‹¤. êµ¬ë‘ì ê³¼ ë‹¨ë½ ë‚˜ëˆ„ê¸°ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤. ë‚´ìš©ì„ ìƒëµí•˜ê±°ë‚˜ ì¶”ê°€í•˜ì§€ ë§ê³ , ì˜ì—­í•˜ì§€ ë§ê³ , ê³ ìœ ëª…ì‚¬ì™€ ìˆ«ìëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”."
        if chunk_idx == 1:
            user_msg = f"ë‹¤ìŒ í•œêµ­ì–´ ì „ì‚¬ì— êµ¬ë‘ì ê³¼ ë‹¨ë½ì„ ì¶”ê°€í•´ì£¼ì„¸ìš” (1ë¶€):\n\n{chunk_text}"
        elif chunk_idx == total_chunks:
            user_msg = f"ë‹¤ìŒ í•œêµ­ì–´ ì „ì‚¬ì— êµ¬ë‘ì ê³¼ ë‹¨ë½ì„ ì¶”ê°€í•´ì£¼ì„¸ìš” (ë§ˆì§€ë§‰ ë¶€ë¶„, ì´ì „ ê³„ì†):\n\n{chunk_text}"
        else:
            user_msg = f"ë‹¤ìŒ í•œêµ­ì–´ ì „ì‚¬ì— êµ¬ë‘ì ê³¼ ë‹¨ë½ì„ ì¶”ê°€í•´ì£¼ì„¸ìš” ({chunk_idx}ë¶€, ì´ì „ ê³„ì†):\n\n{chunk_text}"
    else:
        # å…¶ä»–èªè¨€ä½¿ç”¨è‹±æ–‡æç¤º
        system_msg = "You are a precise transcript editor. Only add punctuation and paragraphing. Do not omit or add content, do not paraphrase, preserve proper nouns and numbers."
        if chunk_idx == 1:
            user_msg = f"Add punctuation and paragraphing to this transcript (part 1):\n\n{chunk_text}"
        elif chunk_idx == total_chunks:
            user_msg = f"Add punctuation and paragraphing to this transcript (final part, continuing from previous):\n\n{chunk_text}"
        else:
            user_msg = f"Add punctuation and paragraphing to this transcript (part {chunk_idx}, continuing from previous):\n\n{chunk_text}"

    return system_msg, user_msg


def punctuate_with_gemini(text: str, chunk_size: int = None, task_id: str = None, language: str = "zh") -> str:
    """ç”¨ Google Gemini å¹«é€å­—ç¨¿åŠ æ¨™é»èˆ‡åˆ†æ®µï¼ˆæ”¯æ´é•·æ–‡æœ¬åˆ†æ®µè™•ç†ã€å¤šèªè¨€ï¼‰

    Args:
        text: è¦åŠ æ¨™é»çš„æ–‡å­—
        chunk_size: åˆ†æ®µå¤§å°ï¼ˆå­—å…ƒæ•¸ï¼‰ï¼ŒNone å‰‡æ ¹æ“šèªè¨€è‡ªå‹•æ±ºå®š
        task_id: ä»»å‹™ IDï¼ˆç”¨æ–¼æ›´æ–°é€²åº¦å’Œè¨˜éŒ„ token ä½¿ç”¨é‡ï¼‰
        language: èªè¨€ä»£ç¢¼ (zh/en/ja/ko/ç­‰)ï¼Œç”± Whisper è‡ªå‹•åµæ¸¬æˆ–ç”¨æˆ¶æŒ‡å®š
    """
    if not GOOGLE_API_KEYS:
        raise RuntimeError("æœªè¨­å®šä»»ä½• GOOGLE_API_KEY")

    # Token ä½¿ç”¨é‡çµ±è¨ˆ
    total_prompt_tokens = 0
    total_completion_tokens = 0
    total_tokens = 0
    model_used = None

    # æ ¹æ“šèªè¨€æ±ºå®šåˆé©çš„åˆ†æ®µå¤§å°
    if chunk_size is None:
        if language in ['en', 'es', 'fr', 'de', 'it', 'pt']:
            # è‹±æ–‡ç­‰å­—æ¯èªè¨€ï¼šä½¿ç”¨è¼ƒå¤§çš„å­—å…ƒæ•¸ï¼ˆå› ç‚ºå–®è©åŒ…å«ç©ºæ ¼ï¼Œå­—å…ƒæ•¸è¼ƒå¤šï¼‰
            chunk_size = 8000  # ç´„ 1300-1600 å€‹è‹±æ–‡å–®è©
        else:
            # ä¸­æ–‡ã€æ—¥æ–‡ã€éŸ“æ–‡ç­‰ï¼šä½¿ç”¨æ¨™æº–å­—å…ƒæ•¸
            chunk_size = 3000  # ç´„ 3000 å€‹å­—ç¬¦
        print(f"ğŸ“ æ ¹æ“šèªè¨€ '{language}' è¨­å®šåˆ†æ®µå¤§å°ï¼š{chunk_size} å­—å…ƒ")

    # å¦‚æœæ–‡æœ¬ä¸é•·ï¼Œç›´æ¥è™•ç†
    if len(text) <= chunk_size:
        system_msg, user_msg = get_punctuation_prompt(language, text)
        prompt = system_msg + "\n\n" + user_msg
        result, usage = call_gemini_with_retry(prompt, return_usage=True)

        # è¨˜éŒ„ token ä½¿ç”¨é‡åˆ°è³‡æ–™åº«
        if task_id and usage:
            update_task_status(task_id, {
                "total_tokens_used": usage['total_tokens'],
                "prompt_tokens_used": usage['prompt_tokens'],
                "completion_tokens_used": usage['completion_tokens'],
                "llm_model_used": usage['model']
            })

        return result

    # é•·æ–‡æœ¬ï¼šåˆ†æ®µè™•ç†
    print(f"ğŸ“Š æ–‡æœ¬é•·åº¦ {len(text)} å­—ï¼Œå°‡åˆ†æ®µè™•ç†ï¼ˆæ¯æ®µç´„ {chunk_size} å­—ï¼‰...")
    chunks = []
    start = 0

    # æ ¹æ“šèªè¨€é¸æ“‡åˆ†æ®µæ¨™è¨˜
    split_markers = 'ã€‚ï¼Ÿï¼\n' if language in ['zh', 'ja'] else '.?!\n'

    while start < len(text):
        end = start + chunk_size
        if end < len(text):
            for i in range(end, max(start + chunk_size // 2, end - 200), -1):
                if text[i] in split_markers:
                    end = i + 1
                    break
        chunks.append(text[start:end])
        start = end

    print(f"ğŸ”„ å…±åˆ†ç‚º {len(chunks)} æ®µè™•ç†...")

    # è¨˜éŒ„æ¨™é»åˆ†æ®µç¸½æ•¸
    if task_id:
        update_task_status(task_id, {
            "punctuation_total_chunks": len(chunks),
            "punctuation_current_chunk": 0
        })

    results = []

    for idx, chunk in enumerate(chunks, 1):
        print(f"   è™•ç†ç¬¬ {idx}/{len(chunks)} æ®µ...")

        # æª¢æŸ¥æ˜¯å¦å·²è¢«å–æ¶ˆ
        if task_id and task_cancelled.get(task_id, False):
            raise RuntimeError("ä»»å‹™å·²è¢«ä½¿ç”¨è€…å–æ¶ˆ")

        # æ›´æ–°ä»»å‹™é€²åº¦ï¼ˆé¡¯ç¤ºç•¶å‰æ¨™é»è™•ç†æ®µæ•¸ï¼‰
        if task_id:
            update_task_status(task_id, {
                "punctuation_current_chunk": idx,
                "progress": f"æ­£åœ¨æ·»åŠ æ¨™é»ç¬¦è™Ÿ... (ç¬¬ {idx}/{len(chunks)} æ®µ)"
            }, persist_to_db=False)

        # ä½¿ç”¨èªè¨€æ„ŸçŸ¥çš„æç¤ºèª
        system_msg, user_msg = get_chunked_punctuation_prompt(language, chunk, idx, len(chunks))

        prompt = system_msg + "\n\n" + user_msg
        result, usage = call_gemini_with_retry(prompt, return_usage=True)
        results.append(result)

        # ç´¯ç© token ä½¿ç”¨é‡
        if usage:
            total_prompt_tokens += usage['prompt_tokens']
            total_completion_tokens += usage['completion_tokens']
            total_tokens += usage['total_tokens']
            model_used = usage['model']  # è¨˜éŒ„æœ€å¾Œä½¿ç”¨çš„æ¨¡å‹

    print("âœ… æ‰€æœ‰æ®µè½è™•ç†å®Œæˆï¼Œæ­£åœ¨åˆä½µ...")

    # è¨˜éŒ„ç¸½ token ä½¿ç”¨é‡åˆ°è³‡æ–™åº«
    if task_id:
        print(f"ğŸ“Š ç¸½ Token ä½¿ç”¨é‡: {total_tokens} (è¼¸å…¥: {total_prompt_tokens}, è¼¸å‡º: {total_completion_tokens})")
        update_task_status(task_id, {
            "total_tokens_used": total_tokens,
            "prompt_tokens_used": total_prompt_tokens,
            "completion_tokens_used": total_completion_tokens,
            "llm_model_used": model_used
        })

    return "\n\n".join(results)


def cleanup_old_audio_files(current_task_id: str):
    """æ¸…ç†èˆŠçš„éŸ³æª”ï¼Œä¿ç•™è¦å‰‡ï¼š
    1. æœ€æ–°çš„ä»»å‹™ï¼ˆcurrent_task_idï¼‰å§‹çµ‚ä¿ç•™
    2. ç”¨æˆ¶å‹¾é¸çš„ä»»å‹™ï¼ˆkeep_audio=Trueï¼‰æ°¸é ä¿ç•™ï¼ˆæœ€å¤š 3 å€‹ï¼‰
    3. ç³»çµ±æœ€å¤šä¿ç•™ 4 å€‹éŸ³æª”
    4. è¶…é 4 å€‹æ™‚ï¼Œå¾æ²’æœ‰å‹¾é¸çš„ä»»å‹™ä¸­ï¼ŒæŒ‰å®Œæˆæ™‚é–“å¾æœ€èˆŠçš„é–‹å§‹åˆªé™¤

    Args:
        current_task_id: ç•¶å‰æœ€æ–°ä»»å‹™çš„ ID

    Note:
        å¿…é ˆåœ¨ä»»å‹™ç‹€æ…‹æ›´æ–°ï¼ˆè¨­å®š audio_fileï¼‰ä¹‹å¾Œèª¿ç”¨
    """
    try:
        print(f"ğŸ§¹ é–‹å§‹æ¸…ç†èˆŠéŸ³æª”...")

        # æ”¶é›†æ‰€æœ‰å·²å®Œæˆä¸”æœ‰éŸ³æª”çš„ä»»å‹™ï¼ˆå¾ MongoDBï¼‰
        # è¨˜æ†¶é«”å„ªåŒ–ï¼šåªæŸ¥è©¢éœ€è¦çš„æ¬„ä½ï¼Œé™åˆ¶ç‚º 100 å€‹ï¼ˆè€Œéä¹‹å‰çš„ 1000ï¼‰
        tasks_with_audio = []

        # ä½¿ç”¨å„ªåŒ–çš„æŸ¥è©¢æ–¹æ³•ï¼ˆåªè¿”å›å¿…è¦æ¬„ä½ï¼Œä¸è¼‰å…¥ segmentsã€transcript ç­‰å¤§å‹æ•¸æ“šï¼‰
        all_tasks = run_async_in_thread(task_repo.find_tasks_with_audio(limit=100))

        for task in all_tasks:
            audio_file_path = get_task_field(task, "audio_file")
            if audio_file_path:
                audio_path = Path(audio_file_path)
                completed_at = get_task_field(task, "completed_at") or ""
                keep_audio = task.get("keep_audio", False)
                task_id = task.get("task_id")

                tasks_with_audio.append({
                    "task_id": task_id,
                    "audio_file": audio_path,
                    "completed_at": completed_at,
                    "keep_audio": keep_audio,
                    "is_current": task_id == current_task_id
                })

        # æ±ºå®šå“ªäº›éŸ³æª”è¦ä¿ç•™
        files_to_keep = set()

        # 1. æœ€æ–°çš„ä»»å‹™å§‹çµ‚ä¿ç•™
        if current_task_id:
            files_to_keep.add(current_task_id)
            print(f"   âœ“ ä¿ç•™æœ€æ–°ä»»å‹™éŸ³æª”ï¼š{current_task_id[:8]}...")

        # 2. ç”¨æˆ¶å‹¾é¸ä¿ç•™çš„ä»»å‹™ï¼ˆkeep_audio=Trueï¼‰æ°¸é ä¿ç•™
        keep_audio_tasks = [t for t in tasks_with_audio if t["keep_audio"]]
        for idx, task in enumerate(keep_audio_tasks):
            files_to_keep.add(task["task_id"])
            print(f"   âœ“ ä¿ç•™ç”¨æˆ¶å‹¾é¸éŸ³æª” #{idx+1}ï¼š{task['audio_file'].name}")

        # 3. å¦‚æœä¿ç•™çš„æª”æ¡ˆè¶…é 4 å€‹ï¼Œéœ€è¦å¾æ²’æœ‰å‹¾é¸çš„ä»»å‹™ä¸­åˆªé™¤æœ€èˆŠçš„
        # ç³»çµ±æœ€å¤šä¿ç•™ 4 å€‹éŸ³æª”
        MAX_AUDIO_FILES = 4

        if len(files_to_keep) >= MAX_AUDIO_FILES:
            # å·²ç¶“é”åˆ°æˆ–è¶…éä¸Šé™ï¼Œä¸éœ€è¦é¡å¤–ä¿ç•™
            print(f"   ç•¶å‰å·²ä¿ç•™ {len(files_to_keep)} å€‹éŸ³æª”ï¼ˆé”åˆ°ä¸Šé™ï¼‰")
        else:
            # é‚„æœ‰ç©ºé–“ï¼Œå¯ä»¥ä¿ç•™ä¸€äº›æœªå‹¾é¸çš„ä»»å‹™
            # å¾æœªå‹¾é¸çš„ä»»å‹™ä¸­ï¼ŒæŒ‰å®Œæˆæ™‚é–“æ’åºï¼Œä¿ç•™æœ€æ–°çš„
            uncheckd_tasks = [t for t in tasks_with_audio if not t["keep_audio"] and t["task_id"] not in files_to_keep]
            uncheckd_tasks.sort(key=lambda x: x["completed_at"], reverse=True)

            slots_remaining = MAX_AUDIO_FILES - len(files_to_keep)
            for idx, task in enumerate(uncheckd_tasks[:slots_remaining]):
                files_to_keep.add(task["task_id"])
                print(f"   âœ“ ä¿ç•™æœªå‹¾é¸ä»»å‹™ #{idx+1}ï¼š{task['audio_file'].name}")

        # 4. æ¨™è¨˜è¦åˆªé™¤çš„éŸ³æª”ï¼ˆæ‰€æœ‰ä¸åœ¨ä¿ç•™æ¸…å–®ä¸­çš„ï¼‰
        files_to_delete = []
        tasks_to_update = []

        for item in tasks_with_audio:
            if item["task_id"] not in files_to_keep:
                files_to_delete.append(item["audio_file"])
                tasks_to_update.append(item["task_id"])

        if not files_to_delete:
            print(f"   ç„¡éœ€æ¸…ç†ï¼Œç•¶å‰ä¿ç•™ {len(files_to_keep)} å€‹éŸ³æª”")
            return

        # 4. åˆªé™¤èˆŠéŸ³æª”
        deleted_count = 0
        for audio_file in files_to_delete:
            try:
                if audio_file.exists():
                    audio_file.unlink()
                    print(f"   ğŸ—‘ï¸ å·²åˆªé™¤èˆŠéŸ³æª”ï¼š{audio_file.name}")
                    deleted_count += 1
            except Exception as e:
                print(f"   âš ï¸ åˆªé™¤éŸ³æª”å¤±æ•— {audio_file.name}ï¼š{e}")

        # 5. æ›´æ–°ä»»å‹™è¨˜éŒ„ï¼Œæ¸…é™¤å·²åˆªé™¤éŸ³æª”çš„å¼•ç”¨ï¼ˆåœ¨ MongoDB ä¸­ï¼‰
        if tasks_to_update:
            for tid in tasks_to_update:
                updates = {
                    "audio_file": None,
                    "audio_filename": None,
                    "keep_audio": False,  # æ¸…é™¤ä¿ç•™æ¨™è¨˜
                    "updated_at": get_current_time()
                }
                run_async_in_thread(update_task_in_db(tid, updates))

            print(f"âœ… æ¸…ç†å®Œæˆï¼šåˆªé™¤äº† {deleted_count} å€‹èˆŠéŸ³æª”ï¼Œä¿ç•™ {len(files_to_keep)} å€‹")

    except Exception as e:
        print(f"âš ï¸ æ¸…ç†èˆŠéŸ³æª”å¤±æ•—ï¼š{e}")
        # æ¸…ç†å¤±æ•—ä¸æ‡‰è©²å½±éŸ¿ä¸»æµç¨‹ï¼Œæ‰€ä»¥ä¸æ‹‹å‡ºç•°å¸¸


def process_transcription_task(
    task_id: str,
    temp_audio_path: Path,
    filename: str,
    punct_provider: str,
    chunk_audio: bool,
    chunk_minutes: int,
    diarize: bool = False,
    max_speakers: Optional[int] = None,
    language: str = "zh",
    user_id: str = None
):
    """
    åœ¨èƒŒæ™¯ç·šç¨‹ä¸­åŸ·è¡Œè½‰éŒ„ä»»å‹™
    é€™å€‹å‡½æ•¸æœƒæ›´æ–°ä»»å‹™ç‹€æ…‹ï¼Œä¸¦è™•ç†æ‰€æœ‰è½‰éŒ„é‚è¼¯

    æ³¨æ„ï¼šdiarization åƒ…åœ¨éåˆ†æ®µæ¨¡å¼ä¸‹å¯ç”¨ï¼Œåˆ†æ®µæ¨¡å¼æœƒå¿½ç•¥æ­¤åƒæ•¸
    max_speakers: æœ€å¤§è¬›è€…äººæ•¸ï¼ˆå¯é¸ï¼Œ2-10ï¼‰
    """
    # å°‡ 'auto' è½‰æ›ç‚º Noneï¼Œè®“ Whisper è‡ªå‹•åµæ¸¬èªè¨€
    whisper_language = None if language == "auto" else language

    temp_dir = temp_audio_path.parent

    # ä¿å­˜éŸ³æª”åˆ° output ç›®éŒ„ï¼ˆä¿ç•™è½‰æ›å¾Œçš„ WAV æ ¼å¼ä»¥ç¢ºä¿ç€è¦½å™¨ç›¸å®¹æ€§ï¼‰
    audio_filename = f"{Path(filename).stem}_{task_id}.wav"
    permanent_audio_path = OUTPUT_DIR / audio_filename

    try:
        # è¨˜éŒ„æš«å­˜ç›®éŒ„
        with tasks_lock:
            task_temp_dirs[task_id] = temp_dir

        # åˆå§‹åŒ–è¨˜æ†¶é«”ï¼šç›´æ¥è¨­ç½® user_idï¼Œé¿å…é˜»å¡å¼ DB æŸ¥è©¢
        if user_id:
            with tasks_lock:
                if task_id not in transcription_tasks:
                    transcription_tasks[task_id] = {}
                # å„²å­˜ user_idï¼ˆæ”¯æ´å·¢ç‹€å’Œæ‰å¹³å…©ç¨®æ ¼å¼ï¼‰
                transcription_tasks[task_id]["user"] = {"user_id": user_id}
                transcription_tasks[task_id]["user_id"] = user_id  # æ‰å¹³æ ¼å¼ï¼ˆå‘å¾Œå…¼å®¹ï¼‰
            print(f"ğŸ“¥ [{task_id}] åˆå§‹åŒ–è¨˜æ†¶é«”ï¼ˆuser_id: {user_id}ï¼‰ï¼Œä¹‹å¾Œè¼ªè©¢å°‡é›¶ DB æŸ¥è©¢")

        # è¨˜éŒ„é–‹å§‹è™•ç†æ™‚é–“
        start_time = datetime.now(TZ_UTC8)

        # æ›´æ–°ç‹€æ…‹ï¼šè™•ç†ä¸­
        update_task_status(task_id, {
            "status": "processing",
            "progress": "æ­£åœ¨è½‰æ›éŸ³è¨Šæ ¼å¼...",
            "started_at": start_time.strftime("%Y-%m-%d %H:%M:%S")
        })

        # æª¢æŸ¥æ˜¯å¦å·²è¢«å–æ¶ˆ
        if task_cancelled.get(task_id, False):
            raise RuntimeError("ä»»å‹™å·²è¢«ä½¿ç”¨è€…å–æ¶ˆ")

        # è½‰æ›ç‚º WAV
        wav_path = temp_dir / "input.wav"
        print(f"ğŸ”„ [{task_id}] è½‰æª”ç‚º WAV...")
        try:
            # æ˜ç¢ºæŒ‡å®šä½¿ç”¨ ffmpeg ä½œç‚ºè½‰æª”å·¥å…·
            audio = AudioSegment.from_file(str(temp_audio_path))
            audio.export(str(wav_path), format="wav")
            # ç«‹å³é‡‹æ”¾è¨˜æ†¶é«”
            del audio
            import gc
            gc.collect()
        except Exception as e:
            import traceback
            print(f"âŒ [{task_id}] éŸ³è¨Šè½‰æª”å¤±æ•—ï¼š{e}")
            print(f"è©³ç´°éŒ¯èª¤ï¼š\n{traceback.format_exc()}")
            raise

        # æ¨™è¨˜éŸ³è¨Šè½‰æª”å®Œæˆ
        update_task_status(task_id, {
            "audio_converted": True,
            "progress": "éŸ³è¨Šè½‰æª”å®Œæˆï¼Œæº–å‚™è½‰éŒ„..."
        })

        # æå‰åˆªé™¤åŸå§‹ä¸Šå‚³æª”æ¡ˆï¼Œç¯€çœç£ç¢Ÿç©ºé–“ï¼ˆå·²è½‰æ›ç‚º WAVï¼Œä¸å†éœ€è¦ï¼‰
        try:
            if temp_audio_path.exists() and temp_audio_path != wav_path:
                temp_audio_path.unlink()
                print(f"ğŸ—‘ï¸  [{task_id}] å·²åˆªé™¤åŸå§‹ä¸Šå‚³æª”æ¡ˆï¼š{temp_audio_path.name}")
        except Exception as e:
            print(f"âš ï¸  [{task_id}] åˆªé™¤åŸå§‹æª”æ¡ˆå¤±æ•—ï¼ˆä¸å½±éŸ¿è™•ç†ï¼‰ï¼š{e}")

        # æª¢æŸ¥æ˜¯å¦å·²è¢«å–æ¶ˆ
        if task_cancelled.get(task_id, False):
            raise RuntimeError("ä»»å‹™å·²è¢«ä½¿ç”¨è€…å–æ¶ˆ")

        # åŸ·è¡Œè½‰éŒ„
        update_task_status(task_id, {"progress": "æ­£åœ¨è½‰éŒ„éŸ³è¨Š..."})

        all_segments = []  # ç”¨æ–¼å„²å­˜æ‰€æœ‰ segments
        detected_language = None  # ç”¨æ–¼å„²å­˜ Whisper åµæ¸¬åˆ°çš„èªè¨€

        if chunk_audio:
            # åˆ†æ®µæ¨¡å¼ï¼šç¾åœ¨æ”¯æ´ diarizationï¼ˆå…ˆå°å®Œæ•´éŸ³æª”åšèªªè©±è€…è¾¨è­˜ï¼Œå†åˆ†æ®µè½‰éŒ„ï¼‰
            chunk_duration_ms = chunk_minutes * 60 * 1000
            raw_text, all_segments, detected_language = transcribe_audio_in_chunks(
                wav_path,
                whisper_model,
                chunk_duration_ms,
                task_id=task_id,
                diarize=diarize,
                max_speakers=max_speakers,
                language=whisper_language
            )
        else:
            # éåˆ†æ®µæ¨¡å¼ï¼šå¯ä»¥ä½¿ç”¨ diarization
            if diarize and diarization_pipeline:
                print(f"ğŸ”Š [{task_id}] å•Ÿç”¨ speaker diarization...")

                # åŸ·è¡Œ speaker diarization
                diarization_start = datetime.now(TZ_UTC8)
                update_task_status(task_id, {
                    "progress": "æ­£åœ¨åˆ†æèªªè©±è€…...",
                    "diarization_status": "running"
                })
                diarization_segments = perform_diarization(wav_path, max_speakers=max_speakers)
                diarization_duration = (datetime.now(TZ_UTC8) - diarization_start).total_seconds()

                # åŸ·è¡Œè½‰éŒ„ï¼ˆå¸¶æ™‚é–“æˆ³ï¼‰
                update_task_status(task_id, {"progress": "æ­£åœ¨è½‰éŒ„éŸ³è¨Šï¼ˆå¸¶æ™‚é–“æˆ³ï¼‰..."})
                transcription_segments, detected_language = transcribe_with_timestamps(whisper_model, wav_path, language=whisper_language)
                all_segments = transcription_segments  # ä¿å­˜ segments

                # åˆä½µçµæœ
                if diarization_segments:
                    num_speakers = len(set(s['speaker'] for s in diarization_segments))
                    update_task_status(task_id, {
                        "progress": "æ­£åœ¨åˆä½µèªªè©±è€…è³‡è¨Š...",
                        "diarization_status": "completed",
                        "diarization_num_speakers": num_speakers,
                        "diarization_duration_seconds": round(diarization_duration, 1)
                    })
                    raw_text = merge_transcription_with_diarization(
                        transcription_segments,
                        diarization_segments
                    )
                else:
                    # diarization å¤±æ•—ï¼Œå›é€€åˆ°ç´”æ–‡å­—
                    update_task_status(task_id, {
                        "diarization_status": "failed"
                    })
                    raw_text = " ".join(seg["text"] for seg in transcription_segments)
            else:
                print(f"ğŸ“ [{task_id}] é–‹å§‹è½‰é€å­—ç¨¿...")
                raw_text, all_segments, detected_language = transcribe_single_chunk(whisper_model, wav_path, language=whisper_language)

        # æª¢æŸ¥æ˜¯å¦å·²è¢«å–æ¶ˆ
        if task_cancelled.get(task_id, False):
            raise RuntimeError("ä»»å‹™å·²è¢«ä½¿ç”¨è€…å–æ¶ˆ")

        print(f"âœ… [{task_id}] è½‰éŒ„å®Œæˆï¼ˆ{len(raw_text)} å­—ï¼‰")

        # æ±ºå®šä½¿ç”¨å“ªå€‹èªè¨€é€²è¡Œæ¨™é»è™•ç†
        # å¦‚æœç”¨æˆ¶é¸æ“‡ autoï¼Œä½¿ç”¨ Whisper åµæ¸¬çš„èªè¨€ï¼›å¦å‰‡ä½¿ç”¨ç”¨æˆ¶æŒ‡å®šçš„èªè¨€
        punct_language = detected_language if language == "auto" and detected_language else language
        if detected_language and language == "auto":
            print(f"ğŸ” [{task_id}] Whisper åµæ¸¬åˆ°çš„èªè¨€ï¼š{detected_language}")

        # åŠ æ¨™é»
        final_text = raw_text
        if punct_provider == "gemini":
            try:
                update_task_status(task_id, {
                    "punctuation_started": True,
                    "progress": "æ­£åœ¨æ·»åŠ æ¨™é»ç¬¦è™Ÿï¼ˆGeminiï¼‰..."
                })
                print(f"âœ¨ [{task_id}] ä½¿ç”¨ Gemini åŠ æ¨™é»èˆ‡åˆ†æ®µï¼ˆèªè¨€ï¼š{punct_language}ï¼‰...")
                final_text = punctuate_with_gemini(raw_text, task_id=task_id, language=punct_language)
                update_task_status(task_id, {"punctuation_completed": True})
            except Exception as e:
                print(f"âš ï¸ [{task_id}] Gemini åŠ æ¨™é»å¤±æ•—ï¼š{e}")
                print(f"ğŸ“ [{task_id}] å°‡ä½¿ç”¨ Whisper åŸå§‹è½‰éŒ„çµæœ")
                update_task_status(task_id, {
                    "punctuation_completed": False,
                    "punctuation_error": str(e),
                    "progress": "æ¨™é»ç¬¦è™Ÿæ·»åŠ å¤±æ•—ï¼Œä½¿ç”¨åŸå§‹è½‰éŒ„çµæœ"
                })
        elif punct_provider == "openai":
            try:
                update_task_status(task_id, {
                    "punctuation_started": True,
                    "progress": "æ­£åœ¨æ·»åŠ æ¨™é»ç¬¦è™Ÿï¼ˆOpenAIï¼‰..."
                })
                if not os.getenv("OPENAI_API_KEY"):
                    raise ValueError("æœªè¨­å®š OPENAI_API_KEY")
                print(f"âœ¨ [{task_id}] ä½¿ç”¨ OpenAI åŠ æ¨™é»èˆ‡åˆ†æ®µï¼ˆèªè¨€ï¼š{punct_language}ï¼‰...")
                final_text = punctuate_with_openai(raw_text, language=punct_language)
                update_task_status(task_id, {"punctuation_completed": True})
            except Exception as e:
                print(f"âš ï¸ [{task_id}] OpenAI åŠ æ¨™é»å¤±æ•—ï¼š{e}")
                print(f"ğŸ“ [{task_id}] å°‡ä½¿ç”¨ Whisper åŸå§‹è½‰éŒ„çµæœ")
                update_task_status(task_id, {
                    "punctuation_completed": False,
                    "punctuation_error": str(e),
                    "progress": "æ¨™é»ç¬¦è™Ÿæ·»åŠ å¤±æ•—ï¼Œä½¿ç”¨åŸå§‹è½‰éŒ„çµæœ"
                })

        print(f"ğŸ‰ [{task_id}] è™•ç†å®Œæˆï¼")

        # ä¿å­˜æ–‡å­—æª”åˆ° output/ ç›®éŒ„
        update_task_status(task_id, {"progress": "æ­£åœ¨ä¿å­˜çµæœ..."})
        timestamp = datetime.now(TZ_UTC8).strftime("%Y%m%d_%H%M%S")
        safe_filename = Path(filename).stem.replace(" ", "_")
        output_filename = f"{safe_filename}_{timestamp}_transcript.txt"
        permanent_output = OUTPUT_DIR / output_filename
        permanent_output.write_text(final_text, encoding="utf-8")
        print(f"ğŸ’¾ [{task_id}] æ–‡å­—æª”å·²ä¿å­˜ï¼š{permanent_output.relative_to(OUTPUT_DIR.parent)}")

        # ä¿å­˜ segments åˆ° JSON æª”æ¡ˆ
        segments_filename = f"{safe_filename}_{timestamp}_segments.json"
        segments_output = OUTPUT_DIR / segments_filename
        segments_output.write_text(json.dumps(all_segments, ensure_ascii=False, indent=2), encoding="utf-8")
        print(f"ğŸ“Š [{task_id}] Segments å·²ä¿å­˜ï¼š{segments_output.relative_to(OUTPUT_DIR.parent)}")

        # è¤‡è£½ WAV éŸ³æª”åˆ° output ç›®éŒ„ï¼ˆä¿ç•™ä»¥ä¾›æ’­æ”¾ï¼ŒWAV æ ¼å¼åœ¨ç€è¦½å™¨ä¸­æœ‰æœ€ä½³ç›¸å®¹æ€§ï¼‰
        import shutil
        shutil.copy2(wav_path, permanent_audio_path)
        print(f"ğŸµ [{task_id}] éŸ³æª”å·²ä¿å­˜ï¼ˆWAV æ ¼å¼ï¼‰ï¼š{permanent_audio_path.relative_to(OUTPUT_DIR.parent)}")

        # è¨ˆç®—ç¸½è™•ç†æ™‚é–“
        end_time = datetime.now(TZ_UTC8)
        duration_seconds = (end_time - start_time).total_seconds()

        # ç²å–éŸ³æª”æ™‚é•·ï¼ˆç”¨æ–¼é…é¡æ›´æ–°ï¼‰- ä½¿ç”¨ ffprobe é¿å…è¨˜æ†¶é«”å•é¡Œ
        try:
            result = subprocess.run([
                'ffprobe', '-v', 'quiet', '-print_format', 'json',
                '-show_format', str(wav_path)
            ], capture_output=True, text=True, timeout=10)

            if result.returncode == 0:
                import json
                probe_data = json.loads(result.stdout)
                audio_duration_seconds = float(probe_data['format']['duration'])
            else:
                # å›é€€åˆ° pydub
                audio = AudioSegment.from_wav(wav_path)
                audio_duration_seconds = len(audio) / 1000.0
                del audio
        except Exception as e:
            print(f"âš ï¸  [{task_id}] ç„¡æ³•ç²å–éŸ³æª”æ™‚é•·ï¼š{e}")
            audio_duration_seconds = 0

        # æå‰åˆªé™¤è‡¨æ™‚ WAV æª”æ¡ˆï¼Œç¯€çœç£ç¢Ÿç©ºé–“ï¼ˆå·²è¤‡è£½åˆ° output/ ç›®éŒ„ï¼‰
        try:
            if wav_path.exists():
                wav_path.unlink()
                print(f"ğŸ—‘ï¸  [{task_id}] å·²åˆªé™¤è‡¨æ™‚ WAV æª”æ¡ˆï¼š{wav_path.name}")
        except Exception as e:
            print(f"âš ï¸  [{task_id}] åˆªé™¤è‡¨æ™‚ WAV å¤±æ•—ï¼ˆä¸å½±éŸ¿è™•ç†ï¼‰ï¼š{e}")

        # æ›´æ–°ç‹€æ…‹ï¼šå®Œæˆ
        update_task_status(task_id, {
            "status": "completed",
            "progress": "è½‰éŒ„å®Œæˆ",
            "result_file": str(permanent_output),
            "result_filename": output_filename,
            "segments_file": str(segments_output),
            "segments_filename": segments_filename,
            "audio_file": str(permanent_audio_path),
            "audio_filename": audio_filename,
            "text_length": len(final_text),
            "completed_at": get_current_time(),
            "duration_seconds": round(duration_seconds, 1),
            "audio_duration_seconds": round(audio_duration_seconds, 1)  # ä¿å­˜éŸ³æª”æ™‚é•·
        })

        # æ›´æ–°ç”¨æˆ¶é…é¡ï¼ˆç•°æ­¥èª¿ç”¨ï¼‰
        try:
            # å¾ MongoDB ç²å–ä»»å‹™ä»¥å–å¾— user_id
            task = run_async_in_thread(get_task_from_db(task_id))
            user_id = task.get("user_id") if task else None

            if user_id:
                # åœ¨åŒæ­¥ç·šç¨‹ä¸­é‹è¡Œç•°æ­¥é…é¡æ›´æ–°
                import asyncio
                from src.database.mongodb import MongoDB

                try:
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    db = MongoDB.get_db()
                    loop.run_until_complete(
                        QuotaManager.increment_usage(db, user_id, audio_duration_seconds)
                    )
                    loop.close()
                    print(f"âœ… [{task_id}] é…é¡å·²æ›´æ–°ï¼š+{round(audio_duration_seconds/60, 2)} åˆ†é˜")
                except Exception as quota_err:
                    print(f"âš ï¸  [{task_id}] é…é¡æ›´æ–°å¤±æ•—ï¼š{quota_err}")
        except Exception as e:
            print(f"âš ï¸  [{task_id}] é…é¡æ›´æ–°éç¨‹å‡ºéŒ¯ï¼š{e}")

        # æ¸…ç†èˆŠçš„éŸ³æª”ï¼ˆåªä¿ç•™æœ€æ–°çš„ 3 å€‹ï¼‰
        # å¿…é ˆåœ¨ update_task_status ä¹‹å¾Œï¼Œé€™æ¨£ç•¶å‰ä»»å‹™çš„éŸ³æª”æ‰æœƒè¢«è¨ˆå…¥
        cleanup_old_audio_files(task_id)

        print(f"â±ï¸ [{task_id}] ç¸½è™•ç†æ™‚é–“ï¼š{format_duration(duration_seconds)}")

    except Exception as e:
        # æª¢æŸ¥æ˜¯å¦ç‚ºå–æ¶ˆæ“ä½œ
        is_cancelled = "å–æ¶ˆ" in str(e)

        if is_cancelled:
            print(f"ğŸ›‘ [{task_id}] ä»»å‹™å·²è¢«ä½¿ç”¨è€…å–æ¶ˆ")
            update_task_status(task_id, {
                "status": "cancelled",
                "progress": "ä»»å‹™å·²å–æ¶ˆ",
                "error": "ä½¿ç”¨è€…å–æ¶ˆäº†ä»»å‹™"
            })
        else:
            # æ›´æ–°ç‹€æ…‹ï¼šå¤±æ•—
            print(f"âŒ [{task_id}] éŒ¯èª¤ï¼š{e}")
            update_task_status(task_id, {
                "status": "failed",
                "progress": f"éŒ¯èª¤ï¼š{str(e)}",
                "error": str(e)
            })

    finally:
        # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
        cleanup_temp_dir(temp_dir)

        # æ¸…ç†ä»»å‹™ç›¸é—œè¨˜éŒ„
        with tasks_lock:
            task_temp_dirs.pop(task_id, None)
            task_cancelled.pop(task_id, None)
            task_diarization_processes.pop(task_id, None)
            # ç¢ºä¿å¾ transcription_tasks ä¸­ç§»é™¤ï¼ˆä»¥é˜² update_task_status æœªè§¸ç™¼æ¸…ç†ï¼‰
            transcription_tasks.pop(task_id, None)

        # å¼·åˆ¶åƒåœ¾å›æ”¶ä»¥é‡‹æ”¾è¨˜æ†¶é«”
        import gc
        gc.collect()
        print(f"ğŸ§¹ [{task_id}] è¨˜æ†¶é«”æ¸…ç†å®Œæˆ")


async def periodic_memory_cleanup():
    """å®šæœŸæ¸…ç†è¨˜æ†¶é«”ä¸­çš„å­¤ç«‹è³‡æ–™ï¼ˆèƒŒæ™¯ä»»å‹™ï¼‰"""
    import gc

    while True:
        try:
            # æ¯ 10 åˆ†é˜åŸ·è¡Œä¸€æ¬¡
            await asyncio.sleep(600)

            print("ğŸ§¹ åŸ·è¡Œå®šæœŸè¨˜æ†¶é«”æ¸…ç†...")

            # å¾è³‡æ–™åº«æŸ¥è©¢æ‰€æœ‰é€²è¡Œä¸­çš„ä»»å‹™
            # è¨˜æ†¶é«”å„ªåŒ–ï¼šæ¸›å°‘æŸ¥è©¢æ•¸é‡ï¼ˆ100 â†’ 20ï¼‰ä¸¦åªæŸ¥è©¢ task_id
            if task_repo:
                active_task_ids = set()
                active_tasks = await task_repo.collection.find(
                    {"status": {"$in": ["pending", "processing"]}},
                    {"task_id": 1}  # åªæŸ¥è©¢ task_id
                ).limit(20).to_list(length=20)

                active_task_ids = {task["task_id"] for task in active_tasks if "task_id" in task}

                # æ¸…ç†ä¸åœ¨é€²è¡Œä¸­åˆ—è¡¨çš„è¨˜æ†¶é«”è³‡æ–™
                with tasks_lock:
                    # æ¸…ç† transcription_tasks
                    orphaned_tasks = [tid for tid in transcription_tasks.keys() if tid not in active_task_ids]
                    for tid in orphaned_tasks:
                        transcription_tasks.pop(tid, None)
                        print(f"  ğŸ—‘ï¸  æ¸…ç†å­¤ç«‹ä»»å‹™è¨˜æ†¶é«”: {tid}")

                    # æ¸…ç†å…¶ä»–å­—å…¸
                    for tid in list(task_temp_dirs.keys()):
                        if tid not in active_task_ids:
                            task_temp_dirs.pop(tid, None)

                    for tid in list(task_cancelled.keys()):
                        if tid not in active_task_ids:
                            task_cancelled.pop(tid, None)

                    for tid in list(task_diarization_processes.keys()):
                        if tid not in active_task_ids:
                            task_diarization_processes.pop(tid, None)

                # å¼·åˆ¶åƒåœ¾å›æ”¶
                gc.collect()

                if orphaned_tasks:
                    print(f"âœ… è¨˜æ†¶é«”æ¸…ç†å®Œæˆï¼Œæ¸…é™¤ {len(orphaned_tasks)} å€‹å­¤ç«‹ä»»å‹™")
                else:
                    print("âœ… è¨˜æ†¶é«”æ¸…ç†å®Œæˆï¼Œç„¡å­¤ç«‹è³‡æ–™")

        except Exception as e:
            print(f"âš ï¸  å®šæœŸè¨˜æ†¶é«”æ¸…ç†å¤±æ•—: {e}")


async def cleanup_orphaned_tasks():
    """æ¸…ç†ç•°å¸¸ä¸­æ–·çš„ä»»å‹™ï¼ˆç¨‹å¼é‡å•Ÿæ™‚åŸ·è¡Œï¼‰"""
    if task_repo is None:
        return

    try:
        # æŸ¥æ‰¾æ‰€æœ‰è™•æ–¼ pending æˆ– processing ç‹€æ…‹çš„ä»»å‹™
        # è¨˜æ†¶é«”å„ªåŒ–ï¼šé™åˆ¶æ•¸é‡ä¸¦åªæŸ¥è©¢éœ€è¦çš„æ¬„ä½
        orphaned_tasks = await task_repo.collection.find(
            {"status": {"$in": ["pending", "processing"]}},
            {"_id": 1, "task_id": 1, "status": 1, "timestamps": 1}  # åªæŸ¥è©¢éœ€è¦çš„æ¬„ä½
        ).limit(50).to_list(length=50)  # é™åˆ¶æœ€å¤š 50 å€‹

        if not orphaned_tasks:
            print("âœ… æ²’æœ‰ç™¼ç¾ç•°å¸¸ä¸­æ–·çš„ä»»å‹™")
            return

        print(f"âš ï¸  ç™¼ç¾ {len(orphaned_tasks)} å€‹ç•°å¸¸ä¸­æ–·çš„ä»»å‹™ï¼Œæ­£åœ¨æ¸…ç†...")

        # å°‡é€™äº›ä»»å‹™æ¨™è¨˜ç‚ºå¤±æ•—
        current_time = get_current_time()
        for task in orphaned_tasks:
            task_id = task.get("task_id", "unknown")

            # æ›´æ–°ä»»å‹™ç‹€æ…‹
            update_data = {
                "status": "failed",
                "progress": "ä¼ºæœå™¨é‡å•Ÿï¼Œä»»å‹™å·²ä¸­æ–·",
                "error": "ä»»å‹™åŸ·è¡ŒæœŸé–“ä¼ºæœå™¨é‡å•Ÿï¼Œä»»å‹™å·²è¢«æ¨™è¨˜ç‚ºå¤±æ•—"
            }

            # æ”¯æ´å·¢ç‹€çµæ§‹çš„æ™‚é–“æˆ³æ›´æ–°
            if "timestamps" in task:
                update_data["timestamps.updated_at"] = current_time
                update_data["timestamps.completed_at"] = current_time
            else:
                update_data["updated_at"] = current_time
                update_data["completed_at"] = current_time

            await task_repo.collection.update_one(
                {"_id": task["_id"]},
                {"$set": update_data}
            )
            print(f"   âœ“ ä»»å‹™ {task_id} å·²æ¨™è¨˜ç‚ºå¤±æ•—")

        print(f"âœ… å·²æ¸…ç† {len(orphaned_tasks)} å€‹ç•°å¸¸ä¸­æ–·çš„ä»»å‹™")

    except Exception as e:
        print(f"âŒ æ¸…ç†ç•°å¸¸ä¸­æ–·ä»»å‹™æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")


# ---------- API Endpoints ----------

@app.on_event("startup")
async def startup_event():
    """å•Ÿå‹•æ™‚è¼‰å…¥ Whisper æ¨¡å‹å’Œä»»å‹™è¨˜éŒ„"""
    global whisper_model, current_model_name, task_repo, main_loop

    # ç²å–ä¸»äº‹ä»¶å¾ªç’°
    main_loop = asyncio.get_running_loop()

    # é€£æ¥ MongoDB
    print(f"ğŸ”Œ æ­£åœ¨é€£æ¥ MongoDB...")
    try:
        await MongoDB.connect()
        print(f"âœ… å·²é€£æ¥åˆ° MongoDB: {os.getenv('MONGODB_DB_NAME', 'whisper_transcriber')}")
    except Exception as e:
        print(f"âŒ MongoDB é€£æ¥å¤±æ•—: {e}")
        print(f"   è«‹ç¢ºä¿ MongoDB æ­£åœ¨é‹è¡Œä¸¦æª¢æŸ¥ .env é…ç½®")
        raise

    # åˆå§‹åŒ– TaskRepository å’Œ TagRepository
    print(f"ğŸ“‚ æ­£åœ¨åˆå§‹åŒ–ä»»å‹™è³‡æ–™åº«...")
    db = MongoDB.get_db()
    global task_repo, tag_repo
    task_repo = TaskRepository(db)
    tag_repo = TagRepository(db)

    # å»ºç«‹ç´¢å¼•ï¼ˆå¦‚æœå°šæœªå»ºç«‹ï¼‰
    try:
        await task_repo.create_indexes()
        print(f"âœ… ä»»å‹™è³‡æ–™åº«ç´¢å¼•å»ºç«‹å®Œæˆ")
    except Exception as e:
        print(f"âš ï¸  ç´¢å¼•å»ºç«‹å¤±æ•—: {e}")

    # çµ±è¨ˆä»»å‹™æ•¸é‡
    task_count = await db.tasks.count_documents({})
    print(f"âœ… ä»»å‹™è³‡æ–™åº«å·²å°±ç·’ï¼ˆå…± {task_count} å€‹ä»»å‹™ï¼‰")

    # æ¸…ç†ç•°å¸¸ä¸­æ–·çš„ä»»å‹™
    await cleanup_orphaned_tasks()

    # å•Ÿå‹•å®šæœŸè¨˜æ†¶é«”æ¸…ç†ä»»å‹™ï¼ˆæ¯ 10 åˆ†é˜åŸ·è¡Œä¸€æ¬¡ï¼‰
    asyncio.create_task(periodic_memory_cleanup())

    # è¼‰å…¥æ¨™ç±¤è¨­å®š
    load_tag_settings()

    # è¼‰å…¥ Faster-Whisper æ¨¡å‹
    current_model_name = DEFAULT_MODEL
    print(f"ğŸ™ æ­£åœ¨è¼‰å…¥ Faster-Whisper æ¨¡å‹ï¼š{current_model_name}...")
    print(f"ğŸ”§ é…ç½®ï¼šdevice=auto, compute_type=int8ï¼ˆé‡å° M1 å„ªåŒ–ï¼‰")
    whisper_model = WhisperModel(
        current_model_name,
        device="auto",  # è‡ªå‹•é¸æ“‡ CPU
        compute_type="int8",  # ä½¿ç”¨ INT8 é‡åŒ–ï¼Œç¯€çœè¨˜æ†¶é«”ä¸¦æå‡é€Ÿåº¦
        cpu_threads=1,  # æ¯å€‹æ¨ç†ä»»å‹™ç”¨ 1 ç·šç¨‹ï¼ˆé¿å…èˆ‡ diarization ç«¶çˆ­ï¼‰
        num_workers=4  # å…è¨± 4 å€‹ä»»å‹™åŒæ™‚æ¨ç†
    )
    print(f"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼Œæœå‹™å·²å°±ç·’ï¼")

    # è¼‰å…¥ Speaker Diarization æ¨¡å‹ï¼ˆå¯é¸ï¼‰
    global diarization_pipeline
    diarization_pipeline = None

    if DIARIZATION_AVAILABLE:
        hf_token = os.getenv("HF_TOKEN")
        if hf_token:
            try:
                # ä½¿ç”¨ huggingface_hub ç™»å…¥
                from huggingface_hub import login
                login(token=hf_token, add_to_git_credential=False)

                print("ğŸ”Š æ­£åœ¨è¼‰å…¥ Speaker Diarization æ¨¡å‹...")
                import torch
                diarization_pipeline = Pipeline.from_pretrained(
                    "pyannote/speaker-diarization-3.1"
                )

                # M1 Mac MPS åŠ é€Ÿ
                if torch.backends.mps.is_available():
                    diarization_pipeline.to(torch.device("mps"))
                    print("âœ… Speaker Diarization æ¨¡å‹è¼‰å…¥å®Œæˆï¼ˆä½¿ç”¨ MPS åŠ é€Ÿï¼‰ï¼")
                else:
                    print("âœ… Speaker Diarization æ¨¡å‹è¼‰å…¥å®Œæˆï¼")
            except Exception as e:
                print(f"âš ï¸  Speaker Diarization æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼š{e}")
                print("   è«‹ç¢ºèªå·²åœ¨ Hugging Face åŒæ„ä½¿ç”¨æ¢æ¬¾ï¼šhttps://huggingface.co/pyannote/speaker-diarization-3.1")
        else:
            print("â„¹ï¸  æœªè¨­å®š HF_TOKENï¼Œspeaker diarization åŠŸèƒ½ä¸å¯ç”¨")
            print("   å¦‚éœ€ä½¿ç”¨ï¼Œè«‹ï¼š")
            print("   1. è¨ªå• https://huggingface.co/settings/tokens")
            print("   2. å‰µå»º access token")
            print("   3. åœ¨ .env æ·»åŠ ï¼šHF_TOKEN=your_token_here")


@app.on_event("shutdown")
async def shutdown_event():
    """é—œé–‰æ™‚æ–·é–‹ MongoDB é€£æ¥"""
    print(f"ğŸ”Œ æ­£åœ¨é—œé–‰ MongoDB é€£æ¥...")
    await MongoDB.close()


@app.get("/")
async def root():
    """æœå‹™ç‹€æ…‹"""
    # å¾ MongoDB æŸ¥è©¢æ´»èºä»»å‹™æ•¸é‡
    from src.database.mongodb import MongoDB
    db = MongoDB.get_db()
    active_count = await db.tasks.count_documents({
        "status": {"$in": ["pending", "processing"]}
    })

    return {
        "service": "Whisper è½‰éŒ„æœå‹™",
        "version": "2.0.0",
        "status": "running",
        "model": current_model_name,
        "output_dir": str(OUTPUT_DIR.relative_to(OUTPUT_DIR.parent)),
        "concurrent_limit": executor._max_workers,
        "active_transcriptions": active_count,
        "endpoints": {
            "POST /transcribe": "ä¸Šå‚³éŸ³æª”é€²è¡Œè½‰éŒ„ï¼ˆç•°æ­¥ï¼Œç«‹å³è¿”å› task_idï¼‰",
            "GET /transcribe/{task_id}": "æŸ¥è©¢ä»»å‹™ç‹€æ…‹",
            "GET /transcribe/{task_id}/download": "ä¸‹è¼‰è½‰éŒ„çµæœ",
            "GET /transcribe/active/list": "åˆ—å‡ºæ‰€æœ‰ä»»å‹™ï¼ˆå«é€²è¡Œä¸­ï¼‰",
            "GET /transcripts": "åˆ—å‡ºå·²ä¿å­˜çš„æ–‡å­—æª”",
            "GET /health": "å¥åº·æª¢æŸ¥",
            "GET /docs": "Swagger API æ–‡æª”"
        }
    }


@app.post("/transcribe")
async def transcribe(
    file: UploadFile = File(..., description="éŸ³æª” (æ”¯æ´ mp3/m4a/wav/mp4 ç­‰æ ¼å¼)"),
    punct_provider: str = Form("gemini", description="æ¨™é»æä¾›è€… (openai/gemini/none)"),
    chunk_audio: bool = Form(True, description="æ˜¯å¦ä½¿ç”¨åˆ†æ®µæ¨¡å¼"),
    chunk_minutes: int = Form(10, description="åˆ†æ®µé•·åº¦ï¼ˆåˆ†é˜ï¼‰"),
    diarize: bool = Form(False, description="æ˜¯å¦å•Ÿç”¨èªªè©±è€…è¾¨è­˜"),
    max_speakers: Optional[int] = Form(None, description="æœ€å¤§è¬›è€…äººæ•¸ï¼ˆå¯é¸ï¼Œ2-10ï¼‰"),
    language: str = Form("zh", description="è½‰éŒ„èªè¨€ (zh/en/ja/ko/auto)"),
    tags: Optional[str] = Form(None, description="æ¨™ç±¤ï¼ˆJSON é™£åˆ—å­—ä¸²ï¼Œå¦‚ '[\"ç’°å®‡å°ˆæ¡ˆ\",\"2025\"]'ï¼‰"),
    current_user: dict = Depends(get_current_user),
    db = Depends(get_database)
):
    """
    ä¸Šå‚³éŸ³æª”é€²è¡Œè½‰éŒ„ï¼ˆç•°æ­¥æ¨¡å¼ï¼‰

    ç«‹å³è¿”å›ä»»å‹™ IDï¼Œè½‰éŒ„åœ¨èƒŒæ™¯åŸ·è¡Œ
    ä½¿ç”¨ GET /transcribe/{task_id} æŸ¥è©¢ç‹€æ…‹
    ä½¿ç”¨ GET /transcribe/{task_id}/download ä¸‹è¼‰çµæœ

    - **file**: éŸ³æª”æª”æ¡ˆ
    - **punct_provider**: æ¨™é»æä¾›è€… (openai/gemini/none)
    - **chunk_audio**: æ˜¯å¦ä½¿ç”¨åˆ†æ®µæ¨¡å¼ï¼ˆé•·éŸ³æª”å»ºè­°é–‹å•Ÿï¼‰
    - **chunk_minutes**: åˆ†æ®µé•·åº¦ï¼ˆåˆ†é˜ï¼‰
    - **diarize**: æ˜¯å¦å•Ÿç”¨èªªè©±è€…è¾¨è­˜ï¼ˆspeaker diarizationï¼‰
    - **max_speakers**: æœ€å¤§è¬›è€…äººæ•¸ï¼ˆå¯é¸ï¼Œ2-10ï¼Œç•™ç©ºå‰‡è‡ªå‹•åµæ¸¬ï¼‰
    """
    global whisper_model

    if not whisper_model:
        raise HTTPException(status_code=503, detail="æ¨¡å‹å°šæœªè¼‰å…¥å®Œæˆ")

    # ç”Ÿæˆä»»å‹™ ID
    task_id = str(uuid.uuid4())

    # å»ºç«‹è‡¨æ™‚ç›®éŒ„ä¸¦ä¿å­˜ä¸Šå‚³çš„æª”æ¡ˆ
    temp_dir = Path(tempfile.mkdtemp())
    file_suffix = Path(file.filename).suffix
    temp_audio = temp_dir / f"input{file_suffix}"

    try:
        with temp_audio.open("wb") as f:
            content = await file.read()
            f.write(content)

        print(f"ğŸ“ æ”¶åˆ°æª”æ¡ˆï¼š{file.filename} ({len(content) / 1024 / 1024:.2f} MB)")

        # æª¢æŸ¥ diarization å¯ç”¨æ€§
        if diarize and not diarization_pipeline:
            raise HTTPException(
                status_code=400,
                detail="Speaker diarization åŠŸèƒ½æœªå•Ÿç”¨ã€‚è«‹è¨­å®š HF_TOKEN ç’°å¢ƒè®Šæ•¸ä¸¦é‡å•Ÿæœå‹™ã€‚"
            )

        # è§£ææ¨™ç±¤ï¼ˆå¦‚æœæœ‰æä¾›ï¼‰
        task_tags = []
        if tags:
            try:
                import json
                task_tags = json.loads(tags)
            except:
                task_tags = []

        # å‰µå»ºä»»å‹™è¨˜éŒ„åˆ° MongoDBï¼ˆå·¢ç‹€çµæ§‹ï¼‰
        current_time = get_current_time()
        task_data = {
            "_id": task_id,  # ä½¿ç”¨ task_id ä½œç‚º _id
            "task_id": task_id,

            # ä½¿ç”¨è€…è³‡è¨Š
            "user": {
                "user_id": str(current_user["_id"]),
                "user_email": current_user["email"]
            },

            # æª”æ¡ˆè³‡è¨Š
            "file": {
                "filename": file.filename,
                "size_mb": round(len(content) / 1024 / 1024, 2)
            },

            # è½‰éŒ„é…ç½®
            "config": {
                "punct_provider": punct_provider,
                "chunk_audio": chunk_audio,
                "chunk_minutes": chunk_minutes,
                "diarize": diarize,
                "max_speakers": max_speakers,
                "language": language
            },

            # ç‹€æ…‹
            "status": "pending",

            # ä½¿ç”¨è€…è¨­å®šèˆ‡æ¨™ç±¤
            "tags": task_tags,
            "keep_audio": False,

            # æ™‚é–“æˆ³è¨˜
            "timestamps": {
                "created_at": current_time,
                "updated_at": current_time,
                "started_at": None,
                "completed_at": None
            }
        }

        # ä¿å­˜åˆ° MongoDB
        await task_repo.create(task_data)
        print(f"âœ… [{task_id}] ä»»å‹™å·²å»ºç«‹åˆ° MongoDB")

        # åœ¨èƒŒæ™¯ç·šç¨‹ä¸­åŸ·è¡Œè½‰éŒ„
        loop = asyncio.get_event_loop()
        loop.run_in_executor(
            executor,
            process_transcription_task,
            task_id,
            temp_audio,
            file.filename,
            punct_provider,
            chunk_audio,
            chunk_minutes,
            diarize,
            max_speakers,
            language,
            str(current_user["_id"])  # å‚³é user_id
        )

        # ç«‹å³è¿”å›ä»»å‹™è³‡è¨Š
        return JSONResponse({
            "task_id": task_id,
            "status": "pending",
            "message": "è½‰éŒ„ä»»å‹™å·²æäº¤ï¼Œè«‹ä½¿ç”¨ task_id æŸ¥è©¢ç‹€æ…‹",
            "filename": file.filename,
            "created_at": get_task_field(task_data, "created_at"),
            "status_url": f"/transcribe/{task_id}",
            "download_url": f"/transcribe/{task_id}/download"
        })

    except Exception as e:
        # ç™¼ç”ŸéŒ¯èª¤æ™‚æ¸…ç†
        cleanup_temp_dir(temp_dir)
        print(f"âŒ æäº¤ä»»å‹™å¤±æ•—ï¼š{e}")
        raise HTTPException(status_code=500, detail=f"æäº¤ä»»å‹™å¤±æ•—ï¼š{str(e)}")


@app.get("/transcribe/{task_id}")
async def get_task_status(
    task_id: str,
    current_user: dict = Depends(get_current_user)
):
    """æŸ¥è©¢è½‰éŒ„ä»»å‹™ç‹€æ…‹ï¼ˆéœ€èªè­‰ï¼Œåªèƒ½æŸ¥çœ‹è‡ªå·±çš„ä»»å‹™ï¼‰"""

    # å®Œå…¨è¨˜æ†¶é«”æ¨¡å¼ï¼šé€²è¡Œä¸­ä»»å‹™é›¶ DB æŸ¥è©¢ï¼ˆåƒ main åˆ†æ”¯ä¸€æ¨£ï¼‰
    with tasks_lock:
        live_task_info = transcription_tasks.get(task_id)

    # å¦‚æœä»»å‹™åœ¨è¨˜æ†¶é«”ä¸­ï¼ˆæ­£åœ¨è™•ç†ï¼‰ï¼Œå®Œå…¨ä½¿ç”¨è¨˜æ†¶é«”æ•¸æ“š
    if live_task_info:
        # æ¬Šé™é©—è­‰ï¼šæª¢æŸ¥è¨˜æ†¶é«”ä¸­çš„ user_id
        task_user_id = live_task_info.get("user_id") or live_task_info.get("user", {}).get("user_id")
        if task_user_id != str(current_user["_id"]):
            raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨æˆ–ç„¡æ¬Šè¨ªå•")

        # ç›´æ¥ä½¿ç”¨è¨˜æ†¶é«”æ•¸æ“šï¼Œé›¶ DB æŸ¥è©¢ï¼
        task_in_db = live_task_info
        print(f"âš¡ [{task_id}] å¾è¨˜æ†¶é«”è¿”å›ï¼ˆé›¶ DB æŸ¥è©¢ï¼‰")
    else:
        # ä»»å‹™ä¸åœ¨è¨˜æ†¶é«”ä¸­ï¼ˆå·²å®Œæˆæˆ–ä¸å­˜åœ¨ï¼‰ï¼ŒæŸ¥è©¢ MongoDB
        task_in_db = await task_repo.get_by_id_and_user(task_id, str(current_user["_id"]))

        if not task_in_db:
            raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨æˆ–ç„¡æ¬Šè¨ªå•")

        print(f"ğŸ’¾ [{task_id}] å¾ MongoDB è¿”å›ï¼ˆå·²å®Œæˆä»»å‹™ï¼‰")

    # ä¸éœ€è¦å†åˆä½µå³æ™‚è³‡è¨Šï¼ˆå·²ç¶“åœ¨è¨˜æ†¶é«”æ•¸æ“šä¸­äº†ï¼‰
    if False and task_in_db.get("status") in ["processing", "pending"] and live_task_info:
        # å°‡è¨˜æ†¶é«”ä¸­çš„ 'progress' ç­‰å³æ™‚æ¬„ä½ï¼Œæ›´æ–°åˆ°å¾è³‡æ–™åº«å–å‡ºçš„ç‰©ä»¶ä¸Š
        task_in_db["progress"] = live_task_info.get("progress", task_in_db.get("progress"))
        
        diarization_status = live_task_info.get("diarization_status")
        if diarization_status:
            task_in_db["diarization_status"] = diarization_status

    # 4. ä½¿ç”¨ enrich_task_data å‡½æ•¸æ·»åŠ å…¶ä»–è¨ˆç®—æ¬„ä½ (ä¿ç•™ç¾æœ‰é‚è¼¯)
    enriched_task_dict = enrich_task_data(task_in_db)

    # 5. å›å‚³åŸå§‹å­—å…¸ä»¥é¿å… Pydantic é©—è­‰éŒ¯èª¤
    return enriched_task_dict


@app.get("/transcribe/{task_id}/download")
async def download_task_result(
    task_id: str,
    current_user: dict = Depends(check_quota)
):
    """ä¸‹è¼‰è½‰éŒ„çµæœï¼ˆéœ€èªè­‰ï¼Œåªèƒ½ä¸‹è¼‰è‡ªå·±çš„ä»»å‹™ï¼‰"""
    # å¾ MongoDB ç²å–ä»»å‹™ï¼ˆå«æ¬Šé™æª¢æŸ¥ï¼‰
    task = await task_repo.get_by_id_and_user(task_id, str(current_user["_id"]))

    if not task:
        raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨æˆ–ç„¡æ¬Šè¨ªå•")

    if task["status"] != "completed":
        raise HTTPException(
            status_code=400,
            detail=f"ä»»å‹™å°šæœªå®Œæˆï¼ˆç•¶å‰ç‹€æ…‹ï¼š{task['status']}ï¼‰"
        )

    result_file_path = get_task_field(task, "result_file")
    if not result_file_path:
        raise HTTPException(status_code=404, detail="çµæœæª”æ¡ˆä¸å­˜åœ¨")

    result_file = Path(result_file_path)
    if not result_file.exists():
        raise HTTPException(status_code=404, detail="çµæœæª”æ¡ˆä¸å­˜åœ¨")

    # ä½¿ç”¨è‡ªè¨‚åç¨±ä½œç‚ºä¸‹è¼‰æª”åï¼ˆå¦‚æœæœ‰è¨­å®šï¼‰ï¼Œå¦å‰‡ä½¿ç”¨åŸå§‹æª”å
    if task.get("custom_name"):
        download_filename = task["custom_name"]
        # ç§»é™¤å¸¸è¦‹çš„éŸ³è¨Šå‰¯æª”å
        import os
        name_without_ext = download_filename
        for ext in ['.mp3', '.wav', '.m4a', '.flac', '.ogg', '.aac', '.wma']:
            if download_filename.lower().endswith(ext):
                name_without_ext = download_filename[:-len(ext)]
                break
        # ç¢ºä¿æª”åæœ‰ .txt å‰¯æª”å
        if not name_without_ext.endswith('.txt'):
            download_filename = name_without_ext + '.txt'
        else:
            download_filename = name_without_ext
    else:
        download_filename = get_task_field(task, "result_filename") or "result.txt"

    return FileResponse(
        result_file,
        media_type="text/plain",
        filename=download_filename,
        headers={
            "Cache-Control": "no-cache, no-store, must-revalidate",
            "Pragma": "no-cache",
            "Expires": "0"
        }
    )


@app.get("/transcribe/{task_id}/audio")
async def get_task_audio(
    task_id: str,
    current_user: dict = Depends(check_quota)
):
    """ç²å–ä»»å‹™çš„éŸ³æª”ï¼ˆéœ€èªè­‰ï¼Œåªèƒ½è¨ªå•è‡ªå·±çš„ä»»å‹™ï¼‰"""
    # å¾ MongoDB ç²å–ä»»å‹™ï¼ˆå«æ¬Šé™æª¢æŸ¥ï¼‰
    task = await task_repo.get_by_id_and_user(task_id, str(current_user["_id"]))

    if not task:
        raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨æˆ–ç„¡æ¬Šè¨ªå•")

    if task["status"] != "completed":
        raise HTTPException(
            status_code=400,
            detail=f"ä»»å‹™å°šæœªå®Œæˆï¼ˆç•¶å‰ç‹€æ…‹ï¼š{task['status']}ï¼‰"
        )

    # æª¢æŸ¥æ˜¯å¦æœ‰éŸ³æª”
    audio_file_path = get_task_field(task, "audio_file")
    if not audio_file_path:
        raise HTTPException(status_code=404, detail="æ­¤ä»»å‹™æ²’æœ‰ä¿å­˜éŸ³æª”ï¼ˆå¯èƒ½æ˜¯è¼ƒèˆŠçš„ä»»å‹™ï¼‰")

    audio_file = Path(audio_file_path)
    if not audio_file.exists():
        raise HTTPException(status_code=404, detail="éŸ³æª”ä¸å­˜åœ¨")

    # æ ¹æ“šæª”æ¡ˆå‰¯æª”åæ±ºå®š media type
    suffix = audio_file.suffix.lower()
    media_types = {
        ".mp3": "audio/mpeg",
        ".m4a": "audio/mp4",
        ".wav": "audio/wav",
        ".ogg": "audio/ogg",
        ".flac": "audio/flac"
    }
    media_type = media_types.get(suffix, "audio/mpeg")

    audio_filename = get_task_field(task, "audio_filename") or ("audio" + suffix)
    return FileResponse(
        audio_file,
        media_type=media_type,
        filename=audio_filename
    )


@app.get("/transcribe/{task_id}/segments")
async def get_task_segments(
    task_id: str,
    current_user: dict = Depends(check_quota)
):
    """ç²å–ä»»å‹™çš„ segments timing æ•¸æ“šï¼ˆéœ€èªè­‰ï¼Œåªèƒ½è¨ªå•è‡ªå·±çš„ä»»å‹™ï¼‰"""
    # å¾ MongoDB ç²å–ä»»å‹™ï¼ˆå«æ¬Šé™æª¢æŸ¥ï¼‰
    task = await task_repo.get_by_id_and_user(task_id, str(current_user["_id"]))

    if not task:
        raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨æˆ–ç„¡æ¬Šè¨ªå•")

    if task["status"] != "completed":
        raise HTTPException(
            status_code=400,
            detail=f"ä»»å‹™å°šæœªå®Œæˆï¼ˆç•¶å‰ç‹€æ…‹ï¼š{task['status']}ï¼‰"
        )

    # æª¢æŸ¥æ˜¯å¦æœ‰ segments æª”æ¡ˆ
    segments_file_path = get_task_field(task, "segments_file")
    if not segments_file_path:
        raise HTTPException(status_code=404, detail="æ­¤ä»»å‹™æ²’æœ‰ segments æ•¸æ“šï¼ˆå¯èƒ½æ˜¯è¼ƒèˆŠçš„ä»»å‹™ï¼‰")

    segments_file = Path(segments_file_path)
    if not segments_file.exists():
        raise HTTPException(status_code=404, detail="Segments æª”æ¡ˆä¸å­˜åœ¨")

    try:
        segments_data = json.loads(segments_file.read_text(encoding="utf-8"))
        return JSONResponse({"segments": segments_data})
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è®€å– segments å¤±æ•—ï¼š{str(e)}")


@app.put("/transcribe/{task_id}/content")
async def update_transcript_content(
    task_id: str,
    update_data: TranscriptContentUpdate,
    current_user: dict = Depends(get_current_user)
):
    """æ›´æ–°é€å­—ç¨¿å…§å®¹ï¼ˆéœ€èªè­‰ï¼Œåªèƒ½ç·¨è¼¯è‡ªå·±çš„ä»»å‹™ï¼‰"""
    # å¾ MongoDB ç²å–ä»»å‹™ï¼ˆå«æ¬Šé™æª¢æŸ¥ï¼‰
    task = await task_repo.get_by_id_and_user(task_id, str(current_user["_id"]))

    if not task:
        raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨æˆ–ç„¡æ¬Šè¨ªå•")

    if task["status"] != "completed":
        raise HTTPException(
            status_code=400,
            detail=f"ä»»å‹™å°šæœªå®Œæˆï¼Œç„¡æ³•ç·¨è¼¯ï¼ˆç•¶å‰ç‹€æ…‹ï¼š{task['status']}ï¼‰"
        )

    result_file_path = get_task_field(task, "result_file")
    if not result_file_path:
        raise HTTPException(status_code=404, detail="çµæœæª”æ¡ˆä¸å­˜åœ¨")

    result_file = Path(result_file_path)
    if not result_file.exists():
        raise HTTPException(status_code=404, detail="çµæœæª”æ¡ˆä¸å­˜åœ¨")

    # ç²å–æ–°å…§å®¹
    new_content = update_data.content
    if not new_content:
        raise HTTPException(status_code=400, detail="å…§å®¹ä¸èƒ½ç‚ºç©º")

    try:
        # é™¤éŒ¯ï¼šé¡¯ç¤ºæª”æ¡ˆè·¯å¾‘å’Œå…§å®¹å‰100å­—
        print(f"ğŸ“ [{task_id}] æº–å‚™æ›´æ–°æª”æ¡ˆï¼š{result_file}")
        print(f"   æ–°å…§å®¹å‰100å­—ï¼š{new_content[:100]}")

        # ä¿å­˜æ–°å…§å®¹åˆ°æª”æ¡ˆ
        result_file.write_text(new_content, encoding="utf-8")

        # é™¤éŒ¯ï¼šé©—è­‰æª”æ¡ˆæ˜¯å¦çœŸçš„è¢«æ›´æ–°
        saved_content = result_file.read_text(encoding="utf-8")
        print(f"   æª”æ¡ˆå·²æ›´æ–°ï¼Œé©—è­‰å‰100å­—ï¼š{saved_content[:100]}")

        # æ›´æ–°è³‡æ–™åº«ä¸­çš„ä»»å‹™è¨˜éŒ„
        await task_repo.update_content(task_id, str(current_user["_id"]), new_content)

        print(f"âœ… [{task_id}] é€å­—ç¨¿å·²æ›´æ–°ï¼ˆæ–°é•·åº¦ï¼š{len(new_content)} å­—ï¼‰")

        return {
            "message": "é€å­—ç¨¿å·²æˆåŠŸæ›´æ–°",
            "task_id": task_id,
            "text_length": len(new_content)
        }

    except Exception as e:
        print(f"âŒ [{task_id}] æ›´æ–°é€å­—ç¨¿å¤±æ•—ï¼š{e}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°å¤±æ•—ï¼š{str(e)}")


@app.put("/transcribe/{task_id}/metadata")
async def update_task_metadata(
    task_id: str,
    metadata: TaskMetadataUpdate,
    current_user: dict = Depends(get_current_user)
):
    """æ›´æ–°ä»»å‹™çš„è‡ªè¨‚åç¨±ï¼ˆéœ€èªè­‰ï¼Œåªèƒ½ç·¨è¼¯è‡ªå·±çš„ä»»å‹™ï¼‰"""
    # å¾ MongoDB ç²å–ä»»å‹™ï¼ˆå«æ¬Šé™æª¢æŸ¥ï¼‰
    task = await task_repo.get_by_id_and_user(task_id, str(current_user["_id"]))

    if not task:
        raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨æˆ–ç„¡æ¬Šè¨ªå•")

    try:
        # æ›´æ–°è³‡æ–™åº«ä¸­çš„ä»»å‹™å…ƒæ•¸æ“š
        success = await task_repo.update_metadata(
            task_id,
            str(current_user["_id"]),
            custom_name=metadata.custom_name
        )

        if not success:
            raise HTTPException(status_code=500, detail="æ›´æ–°å¤±æ•—")

        print(f"ğŸ“ [{task_id}] æ›´æ–°è‡ªè¨‚åç¨±ï¼š{metadata.custom_name}")

        # ç²å–æ›´æ–°å¾Œçš„ä»»å‹™
        updated_task = await task_repo.get_by_id(task_id)

        return {
            "message": "ä»»å‹™åç¨±å·²æ›´æ–°",
            "task_id": task_id,
            "custom_name": updated_task.get("custom_name")
        }

    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ [{task_id}] æ›´æ–°ä»»å‹™åç¨±å¤±æ•—ï¼š{e}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°å¤±æ•—ï¼š{str(e)}")


@app.put("/transcribe/{task_id}")
async def update_task_tags(
    task_id: str,
    tag_update: TaskTagsUpdate,
    current_user: dict = Depends(get_current_user)
):
    """æ›´æ–°ä»»å‹™çš„æ¨™ç±¤ï¼ˆéœ€èªè­‰ï¼Œåªèƒ½ç·¨è¼¯è‡ªå·±çš„ä»»å‹™ï¼‰"""
    # å¾ MongoDB ç²å–ä»»å‹™ï¼ˆå«æ¬Šé™æª¢æŸ¥ï¼‰
    task = await task_repo.get_by_id_and_user(task_id, str(current_user["_id"]))

    if not task:
        raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨æˆ–ç„¡æ¬Šè¨ªå•")

    try:
        # æ›´æ–°è³‡æ–™åº«ä¸­çš„ä»»å‹™æ¨™ç±¤
        success = await task_repo.update_tags(
            task_id,
            str(current_user["_id"]),
            tag_update.tags
        )

        if not success:
            raise HTTPException(status_code=500, detail="æ›´æ–°å¤±æ•—")

        print(f"ğŸ·ï¸  [{task_id}] æ›´æ–°æ¨™ç±¤ï¼š{tag_update.tags}")

        return {
            "message": "æ¨™ç±¤å·²æ›´æ–°",
            "task_id": task_id,
            "tags": tag_update.tags
        }

    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ [{task_id}] æ›´æ–°æ¨™ç±¤å¤±æ•—ï¼š{e}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°å¤±æ•—ï¼š{str(e)}")


@app.put("/transcribe/{task_id}/keep-audio")
async def update_keep_audio(
    task_id: str,
    keep_audio_update: KeepAudioUpdate,
    current_user: dict = Depends(get_current_user)
):
    """æ›´æ–°ä»»å‹™çš„éŸ³æª”ä¿ç•™ç‹€æ…‹ï¼ˆéœ€èªè­‰ï¼Œåªèƒ½ç·¨è¼¯è‡ªå·±çš„ä»»å‹™ï¼‰"""
    # å¾ MongoDB ç²å–ä»»å‹™ï¼ˆå«æ¬Šé™æª¢æŸ¥ï¼‰
    task = await task_repo.get_by_id_and_user(task_id, str(current_user["_id"]))

    if not task:
        raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨æˆ–ç„¡æ¬Šè¨ªå•")

    # æª¢æŸ¥ä»»å‹™æ˜¯å¦æœ‰éŸ³æª”
    audio_file_path = get_task_field(task, "audio_file")
    if task.get("status") != "completed" or not audio_file_path:
        raise HTTPException(status_code=400, detail="æ­¤ä»»å‹™æ²’æœ‰éŸ³æª”å¯ä»¥ä¿ç•™")

    try:
        # å¦‚æœè¦è¨­ç½®ç‚º Trueï¼Œæª¢æŸ¥å·²å‹¾é¸çš„æ•¸é‡
        if keep_audio_update.keep_audio:
            # è¨ˆç®—ç•¶å‰æœ‰å¤šå°‘å€‹ä»»å‹™è¢«æ¨™è¨˜ç‚ºä¿ç•™éŸ³æª”
            current_keep_count = await task_repo.count_keep_audio_tasks(str(current_user["_id"]))

            # æª¢æŸ¥ç•¶å‰ä»»å‹™æ˜¯å¦å·²ç¶“å‹¾é¸
            if not task.get("keep_audio", False):
                # å¦‚æœç•¶å‰ä»»å‹™é‚„æ²’å‹¾é¸ï¼Œå‰‡éœ€è¦æª¢æŸ¥æ˜¯å¦è¶…éé™åˆ¶
                if current_keep_count >= 3:
                    raise HTTPException(status_code=400, detail="æœ€å¤šåªèƒ½å‹¾é¸ 3 å€‹éŸ³æª”ä¿ç•™")

        # æ›´æ–°è³‡æ–™åº«ä¸­çš„éŸ³æª”ä¿ç•™ç‹€æ…‹
        success = await task_repo.update_keep_audio(
            task_id,
            str(current_user["_id"]),
            keep_audio_update.keep_audio
        )

        if not success:
            raise HTTPException(status_code=500, detail="æ›´æ–°å¤±æ•—")

        print(f"ğŸ“Œ [{task_id}] æ›´æ–°éŸ³æª”ä¿ç•™ç‹€æ…‹ï¼š{keep_audio_update.keep_audio}")

        return {
            "message": "éŸ³æª”ä¿ç•™ç‹€æ…‹å·²æ›´æ–°",
            "task_id": task_id,
            "keep_audio": keep_audio_update.keep_audio
        }

    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ [{task_id}] æ›´æ–°éŸ³æª”ä¿ç•™ç‹€æ…‹å¤±æ•—ï¼š{e}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°å¤±æ•—ï¼š{str(e)}")


@app.get("/tags")
async def get_all_tags(current_user: dict = Depends(get_current_user)):
    """ç²å–ç•¶å‰ç”¨æˆ¶çš„æ‰€æœ‰æ¨™ç±¤åŠå…¶é¡è‰²ï¼ˆéœ€èªè­‰ï¼‰"""
    # å¾ MongoDB ç²å–ç”¨æˆ¶çš„æ‰€æœ‰æ¨™ç±¤
    user_tags = await task_repo.get_all_user_tags(str(current_user["_id"]))

    # è¿”å›æ¨™ç±¤åŠå…¶é¡è‰²
    tags_with_colors = []
    for tag in sorted(user_tags):
        tags_with_colors.append({
            "name": tag,
            "color": tag_colors.get(tag, None)  # å¦‚æœæ²’æœ‰è¨­å®šé¡è‰²å‰‡ç‚º None
        })

    return {
        "tags": tags_with_colors,
        "count": len(tags_with_colors)
    }


@app.put("/tags/{tag_name}/color")
async def update_tag_color(tag_name: str, color_update: TagColorUpdate):
    """æ›´æ–°æ¨™ç±¤çš„é¡è‰²"""
    try:
        tag_colors[tag_name] = color_update.color
        print(f"ğŸ¨ æ›´æ–°æ¨™ç±¤é¡è‰²ï¼š{tag_name} -> {color_update.color}")

        # ä¿å­˜åˆ°ç£ç¢Ÿ
        save_tag_settings()

        return {
            "message": "æ¨™ç±¤é¡è‰²å·²æ›´æ–°",
            "tag": tag_name,
            "color": color_update.color
        }

    except Exception as e:
        print(f"âŒ æ›´æ–°æ¨™ç±¤é¡è‰²å¤±æ•—ï¼š{e}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°å¤±æ•—ï¼š{str(e)}")


@app.get("/tags/order")
async def get_tag_order():
    """ç²å–æ¨™ç±¤é †åº"""
    return {
        "order": tag_order,
        "count": len(tag_order)
    }


@app.put("/tags/order")
async def update_tag_order(order_update: TagOrderUpdate):
    """æ›´æ–°æ¨™ç±¤é †åº"""
    try:
        global tag_order
        tag_order = order_update.order
        print(f"ğŸ“‹ æ›´æ–°æ¨™ç±¤é †åºï¼š{len(tag_order)} å€‹æ¨™ç±¤")

        # ä¿å­˜åˆ°ç£ç¢Ÿ
        save_tag_settings()

        return {
            "message": "æ¨™ç±¤é †åºå·²æ›´æ–°",
            "order": tag_order,
            "count": len(tag_order)
        }

    except Exception as e:
        print(f"âŒ æ›´æ–°æ¨™ç±¤é †åºå¤±æ•—ï¼š{e}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°å¤±æ•—ï¼š{str(e)}")


# ==================== æ–°çš„ Tag CRUD APIï¼ˆåŸºæ–¼IDçš„æ¨™ç±¤ç®¡ç†ï¼‰ ====================

@app.post("/api/tags", response_model=TagResponse, status_code=201)
async def create_tag(
    tag_data: TagCreate,
    current_user: dict = Depends(get_current_user)
):
    """å»ºç«‹æ–°æ¨™ç±¤ï¼ˆéœ€èªè­‰ï¼‰"""
    try:
        tag = await tag_repo.create(
            user_id=str(current_user["_id"]),
            name=tag_data.name,
            color=tag_data.color
        )
        return TagResponse(**tag)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        print(f"âŒ å»ºç«‹æ¨™ç±¤å¤±æ•—ï¼š{e}")
        raise HTTPException(status_code=500, detail=f"å»ºç«‹å¤±æ•—ï¼š{str(e)}")


@app.get("/api/tags", response_model=List[TagResponse])
async def get_all_tags_new(current_user: dict = Depends(get_current_user)):
    """ç²å–ç•¶å‰ç”¨æˆ¶çš„æ‰€æœ‰æ¨™ç±¤ï¼ˆéœ€èªè­‰ï¼‰"""
    try:
        tags = await tag_repo.get_all_by_user(str(current_user["_id"]))
        return [TagResponse(**tag) for tag in tags]
    except Exception as e:
        print(f"âŒ ç²å–æ¨™ç±¤å¤±æ•—ï¼š{e}")
        raise HTTPException(status_code=500, detail=f"ç²å–å¤±æ•—ï¼š{str(e)}")


@app.get("/api/tags/{tag_id}", response_model=TagResponse)
async def get_tag(
    tag_id: str,
    current_user: dict = Depends(get_current_user)
):
    """ç²å–å–®å€‹æ¨™ç±¤ï¼ˆéœ€èªè­‰ï¼‰"""
    tag = await tag_repo.get_by_id(tag_id, str(current_user["_id"]))
    if not tag:
        raise HTTPException(status_code=404, detail="æ¨™ç±¤ä¸å­˜åœ¨æˆ–ç„¡æ¬Šè¨ªå•")
    return TagResponse(**tag)


@app.put("/api/tags/{tag_id}", response_model=TagResponse)
async def update_tag(
    tag_id: str,
    tag_data: TagUpdate,
    current_user: dict = Depends(get_current_user)
):
    """æ›´æ–°æ¨™ç±¤ï¼ˆéœ€èªè­‰ï¼‰"""
    try:
        # å¦‚æœè¦æ›´æ–°åç¨±ï¼Œéœ€è¦åŒæ­¥æ›´æ–°æ‰€æœ‰ä½¿ç”¨è©²æ¨™ç±¤çš„ä»»å‹™
        old_tag = await tag_repo.get_by_id(tag_id, str(current_user["_id"]))
        if not old_tag:
            raise HTTPException(status_code=404, detail="æ¨™ç±¤ä¸å­˜åœ¨æˆ–ç„¡æ¬Šè¨ªå•")

        # æ›´æ–°æ¨™ç±¤
        success = await tag_repo.update(
            tag_id=tag_id,
            user_id=str(current_user["_id"]),
            name=tag_data.name,
            color=tag_data.color
        )

        if not success:
            raise HTTPException(status_code=500, detail="æ›´æ–°å¤±æ•—")

        # å¦‚æœåç¨±æœ‰è®Šæ›´ï¼Œæ›´æ–°æ‰€æœ‰ä»»å‹™ä¸­çš„æ¨™ç±¤
        if tag_data.name and tag_data.name != old_tag["name"]:
            db = MongoDB.get_db()
            updated_tasks = await tag_repo.rename_tag_in_tasks(
                user_id=str(current_user["_id"]),
                old_name=old_tag["name"],
                new_name=tag_data.name,
                tasks_collection=db.tasks
            )
            print(f"ğŸ“ å·²æ›´æ–° {updated_tasks} å€‹ä»»å‹™ä¸­çš„æ¨™ç±¤åç¨±")

        # è¿”å›æ›´æ–°å¾Œçš„æ¨™ç±¤
        updated_tag = await tag_repo.get_by_id(tag_id, str(current_user["_id"]))
        return TagResponse(**updated_tag)

    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ æ›´æ–°æ¨™ç±¤å¤±æ•—ï¼š{e}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°å¤±æ•—ï¼š{str(e)}")


@app.delete("/api/tags/{tag_id}", status_code=204)
async def delete_tag(
    tag_id: str,
    current_user: dict = Depends(get_current_user)
):
    """åˆªé™¤æ¨™ç±¤ï¼ˆéœ€èªè­‰ï¼Œæœƒå¾æ‰€æœ‰ä»»å‹™ä¸­ç§»é™¤ï¼‰"""
    try:
        # ç²å–æ¨™ç±¤è³‡è¨Š
        tag = await tag_repo.get_by_id(tag_id, str(current_user["_id"]))
        if not tag:
            raise HTTPException(status_code=404, detail="æ¨™ç±¤ä¸å­˜åœ¨æˆ–ç„¡æ¬Šè¨ªå•")

        # å¾æ‰€æœ‰ä»»å‹™ä¸­ç§»é™¤è©²æ¨™ç±¤
        db = MongoDB.get_db()
        updated_tasks = await tag_repo.remove_tag_from_tasks(
            user_id=str(current_user["_id"]),
            tag_name=tag["name"],
            tasks_collection=db.tasks
        )
        print(f"ğŸ“ å·²å¾ {updated_tasks} å€‹ä»»å‹™ä¸­ç§»é™¤æ¨™ç±¤")

        # åˆªé™¤æ¨™ç±¤
        success = await tag_repo.delete(tag_id, str(current_user["_id"]))
        if not success:
            raise HTTPException(status_code=500, detail="åˆªé™¤å¤±æ•—")

        return None

    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ åˆªé™¤æ¨™ç±¤å¤±æ•—ï¼š{e}")
        raise HTTPException(status_code=500, detail=f"åˆªé™¤å¤±æ•—ï¼š{str(e)}")


@app.put("/api/tags/order")
async def update_tags_order(
    order_data: TagOrderUpdateModel,
    current_user: dict = Depends(get_current_user)
):
    """æ›´æ–°æ¨™ç±¤é †åºï¼ˆéœ€èªè­‰ï¼‰"""
    try:
        updated_count = await tag_repo.update_order(
            user_id=str(current_user["_id"]),
            tag_ids=order_data.tag_ids
        )

        return {
            "message": "æ¨™ç±¤é †åºå·²æ›´æ–°",
            "updated_count": updated_count
        }

    except Exception as e:
        print(f"âŒ æ›´æ–°æ¨™ç±¤é †åºå¤±æ•—ï¼š{e}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°å¤±æ•—ï¼š{str(e)}")


@app.post("/transcribe/{task_id}/cancel")
async def cancel_task(
    task_id: str,
    current_user: dict = Depends(get_current_user)
):
    """å–æ¶ˆæ­£åœ¨åŸ·è¡Œçš„ä»»å‹™ï¼ˆéœ€èªè­‰ï¼Œåªèƒ½å–æ¶ˆè‡ªå·±çš„ä»»å‹™ï¼‰"""
    # å¾ MongoDB ç²å–ä»»å‹™ï¼ˆå«æ¬Šé™æª¢æŸ¥ï¼‰
    task = await task_repo.get_by_id_and_user(task_id, str(current_user["_id"]))

    if not task:
        raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨æˆ–ç„¡æ¬Šè¨ªå•")

    # åªèƒ½å–æ¶ˆé€²è¡Œä¸­æˆ–ç­‰å¾…ä¸­çš„ä»»å‹™
    if task["status"] not in ["pending", "processing"]:
        raise HTTPException(
            status_code=400,
            detail=f"ç„¡æ³•å–æ¶ˆå·²çµæŸçš„ä»»å‹™ï¼ˆç•¶å‰ç‹€æ…‹ï¼š{task['status']}ï¼‰"
        )

    # æ¨™è¨˜ä»»å‹™ç‚ºå·²å–æ¶ˆï¼ˆé‹è¡Œæ™‚ç‹€æ…‹ï¼‰
    task_cancelled[task_id] = True

    # ç«‹å³çµ‚æ­¢ diarization é€²ç¨‹ï¼ˆå¦‚æœæ­£åœ¨é‹è¡Œï¼‰
    diarization_executor = task_diarization_processes.get(task_id)
    if diarization_executor:
        print(f"ğŸ›‘ æ­£åœ¨å¼·åˆ¶çµ‚æ­¢èªªè©±è€…è¾¨è­˜é€²ç¨‹...")
        try:
            diarization_executor.shutdown(wait=False, cancel_futures=True)
            print(f"âœ… èªªè©±è€…è¾¨è­˜é€²ç¨‹å·²çµ‚æ­¢")
        except Exception as e:
            print(f"âš ï¸ çµ‚æ­¢ diarization é€²ç¨‹å¤±æ•—ï¼š{e}")
        task_diarization_processes.pop(task_id, None)

    # ç«‹å³æ¸…ç†æš«å­˜ç›®éŒ„ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    temp_dir = task_temp_dirs.get(task_id)
    if temp_dir:
        cleanup_temp_dir(temp_dir)
        task_temp_dirs.pop(task_id, None)

    # æ›´æ–°è³‡æ–™åº«ä¸­çš„ä»»å‹™ç‹€æ…‹
    await task_repo.mark_as_cancelled(task_id, str(current_user["_id"]))

    print(f"ğŸ›‘ ä»»å‹™ {task_id} å·²è¢«æ¨™è¨˜ç‚ºå–æ¶ˆ")

    return {
        "message": "ä»»å‹™å–æ¶ˆæŒ‡ä»¤å·²ç™¼é€",
        "task_id": task_id,
        "note": "ä»»å‹™å°‡åœ¨ç•¶å‰æ­¥é©Ÿå®Œæˆå¾Œåœæ­¢"
    }


@app.delete("/transcribe/{task_id}")
async def delete_task(
    task_id: str,
    current_user: dict = Depends(get_current_user)
):
    """åˆªé™¤ä»»å‹™åŠå…¶ç›¸é—œæª”æ¡ˆï¼ˆéœ€èªè­‰ï¼Œåªèƒ½åˆªé™¤è‡ªå·±çš„ä»»å‹™ï¼‰"""
    # å¾ MongoDB ç²å–ä»»å‹™ï¼ˆå«æ¬Šé™æª¢æŸ¥ï¼‰
    task = await task_repo.get_by_id_and_user(task_id, str(current_user["_id"]))

    if not task:
        raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨æˆ–ç„¡æ¬Šè¨ªå•")

    # ä¸å…è¨±åˆªé™¤é€²è¡Œä¸­çš„ä»»å‹™
    if task["status"] in ["pending", "processing"]:
        raise HTTPException(
            status_code=400,
            detail=f"ç„¡æ³•åˆªé™¤é€²è¡Œä¸­çš„ä»»å‹™ï¼ˆç•¶å‰ç‹€æ…‹ï¼š{task['status']}ï¼‰ï¼Œè«‹å…ˆå–æ¶ˆä»»å‹™"
        )

    deleted_files = []

    # åˆªé™¤çµæœæª”æ¡ˆï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    result_file_path = get_task_field(task, "result_file")
    if task["status"] == "completed" and result_file_path:
        result_file = Path(result_file_path)
        try:
            if result_file.exists():
                result_file.unlink()
                deleted_files.append(result_file.name)
                print(f"ğŸ—‘ï¸ å·²åˆªé™¤è½‰éŒ„æª”æ¡ˆï¼š{result_file.name}")
        except Exception as e:
            print(f"âš ï¸ åˆªé™¤è½‰éŒ„æª”æ¡ˆå¤±æ•—ï¼š{e}")

    # åˆªé™¤ segments æª”æ¡ˆï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    segments_file_path = get_task_field(task, "segments_file")
    if task["status"] == "completed" and segments_file_path:
        segments_file = Path(segments_file_path)
        try:
            if segments_file.exists():
                segments_file.unlink()
                deleted_files.append(segments_file.name)
                print(f"ğŸ—‘ï¸ å·²åˆªé™¤ segments æª”æ¡ˆï¼š{segments_file.name}")
        except Exception as e:
            print(f"âš ï¸ åˆªé™¤ segments æª”æ¡ˆå¤±æ•—ï¼š{e}")

    # åˆªé™¤éŸ³æª”ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    audio_file_path = get_task_field(task, "audio_file")
    if task["status"] == "completed" and audio_file_path:
        audio_file = Path(audio_file_path)
        try:
            if audio_file.exists():
                audio_file.unlink()
                deleted_files.append(audio_file.name)
                print(f"ğŸ—‘ï¸ å·²åˆªé™¤éŸ³æª”ï¼š{audio_file.name}")
        except Exception as e:
            print(f"âš ï¸ åˆªé™¤éŸ³æª”å¤±æ•—ï¼š{e}")

    # å¾è³‡æ–™åº«ä¸­åˆªé™¤ä»»å‹™
    await task_repo.delete(task_id, str(current_user["_id"]))

    return {
        "message": "ä»»å‹™å·²åˆªé™¤",
        "task_id": task_id,
        "deleted_files": deleted_files
    }


@app.post("/transcribe/batch/delete")
async def batch_delete_tasks(
    request: BatchDeleteRequest,
    current_user: dict = Depends(get_current_user)
):
    """æ‰¹æ¬¡åˆªé™¤ä»»å‹™ï¼ˆéœ€èªè­‰ï¼Œåªèƒ½åˆªé™¤è‡ªå·±çš„ä»»å‹™ï¼‰"""
    # å…ˆç²å–è¦åˆªé™¤çš„ä»»å‹™ä»¥åˆªé™¤ç›¸é—œæª”æ¡ˆ
    tasks_to_delete = []
    for task_id in request.task_ids:
        task = await task_repo.get_by_id_and_user(task_id, str(current_user["_id"]))
        if task:
            tasks_to_delete.append(task)

    # åˆªé™¤ç›¸é—œæª”æ¡ˆ
    deleted_files_map = {}
    for task in tasks_to_delete:
        task_id = task["_id"]
        deleted_files = []

        # åˆªé™¤çµæœæª”æ¡ˆ
        result_file_path = get_task_field(task, "result_file")
        if task["status"] == "completed" and result_file_path:
            result_file = Path(result_file_path)
            try:
                if result_file.exists():
                    result_file.unlink()
                    deleted_files.append(result_file.name)
            except Exception as e:
                print(f"âš ï¸ åˆªé™¤è½‰éŒ„æª”æ¡ˆå¤±æ•—ï¼š{e}")

        # åˆªé™¤ segments æª”æ¡ˆ
        segments_file_path = get_task_field(task, "segments_file")
        if task["status"] == "completed" and segments_file_path:
            segments_file = Path(segments_file_path)
            try:
                if segments_file.exists():
                    segments_file.unlink()
                    deleted_files.append(segments_file.name)
            except Exception as e:
                print(f"âš ï¸ åˆªé™¤ segments æª”æ¡ˆå¤±æ•—ï¼š{e}")

        # åˆªé™¤éŸ³æª”
        audio_file_path = get_task_field(task, "audio_file")
        if task["status"] == "completed" and audio_file_path:
            audio_file = Path(audio_file_path)
            try:
                if audio_file.exists():
                    audio_file.unlink()
                    deleted_files.append(audio_file.name)
            except Exception as e:
                print(f"âš ï¸ åˆªé™¤éŸ³æª”å¤±æ•—ï¼š{e}")

        deleted_files_map[task_id] = deleted_files

    # å¾è³‡æ–™åº«æ‰¹æ¬¡åˆªé™¤ä»»å‹™
    deleted_count, deleted_ids = await task_repo.bulk_delete(
        request.task_ids,
        str(current_user["_id"])
    )

    # çµ„ç¹”è¿”å›çµæœ
    deleted_tasks = [
        {"task_id": task_id, "deleted_files": deleted_files_map.get(task_id, [])}
        for task_id in deleted_ids
    ]

    failed_tasks = [
        {"task_id": task_id, "reason": "ä»»å‹™ä¸å­˜åœ¨ã€ç„¡æ¬Šè¨ªå•æˆ–æ­£åœ¨é€²è¡Œä¸­"}
        for task_id in request.task_ids if task_id not in deleted_ids
    ]

    return {
        "message": f"æˆåŠŸåˆªé™¤ {deleted_count} å€‹ä»»å‹™",
        "deleted_count": deleted_count,
        "failed_count": len(failed_tasks),
        "deleted_tasks": deleted_tasks,
        "failed_tasks": failed_tasks
    }


@app.post("/transcribe/batch/tags/add")
async def batch_add_tags(
    request: BatchTagsRequest,
    current_user: dict = Depends(get_current_user)
):
    """æ‰¹æ¬¡åŠ å…¥æ¨™ç±¤ï¼ˆéœ€èªè­‰ï¼Œåªèƒ½ç·¨è¼¯è‡ªå·±çš„ä»»å‹™ï¼‰"""
    # ä½¿ç”¨ TaskRepository æ‰¹æ¬¡æ·»åŠ æ¨™ç±¤
    modified_count = await task_repo.bulk_update_tags_add(
        request.task_ids,
        str(current_user["_id"]),
        request.tags
    )

    return {
        "message": f"æˆåŠŸç‚º {modified_count} å€‹ä»»å‹™åŠ å…¥æ¨™ç±¤",
        "updated_count": modified_count,
        "failed_count": len(request.task_ids) - modified_count,
        "tags": request.tags
    }


@app.post("/transcribe/batch/tags/remove")
async def batch_remove_tags(
    request: BatchTagsRequest,
    current_user: dict = Depends(get_current_user)
):
    """æ‰¹æ¬¡ç§»é™¤æ¨™ç±¤ï¼ˆéœ€èªè­‰ï¼Œåªèƒ½ç·¨è¼¯è‡ªå·±çš„ä»»å‹™ï¼‰"""
    # ä½¿ç”¨ TaskRepository æ‰¹æ¬¡ç§»é™¤æ¨™ç±¤
    modified_count = await task_repo.bulk_update_tags_remove(
        request.task_ids,
        str(current_user["_id"]),
        request.tags
    )

    return {
        "message": f"æˆåŠŸå¾ {modified_count} å€‹ä»»å‹™ç§»é™¤æ¨™ç±¤",
        "updated_count": modified_count,
        "failed_count": len(request.task_ids) - modified_count,
        "tags": request.tags
    }


def enrich_task_data(task: Dict[str, Any]) -> Dict[str, Any]:
    """ç‚ºä»»å‹™è³‡æ–™æ·»åŠ è¨ˆç®—æ¬„ä½ï¼ˆé ä¼°æ™‚é–“ã€æ ¼å¼åŒ–æ™‚é•·ç­‰ï¼‰

    æ”¯æ´å·¢ç‹€çµæ§‹ï¼šstats.duration_seconds, timestamps.* ç­‰

    è¨˜æ†¶é«”å„ªåŒ–ï¼šåªæ·»åŠ éœ€è¦çš„è¨ˆç®—æ¬„ä½ï¼Œé¿å…æ·±åº¦è¤‡è£½
    """
    # ä¸ä½¿ç”¨ copy()ï¼Œç›´æ¥åœ¨åŸç‰©ä»¶ä¸Šæ·»åŠ æ¬„ä½ï¼ˆMongoDB å·²è¿”å›æ–°ç‰©ä»¶ï¼‰
    # é€™æ¨£å¯ä»¥æ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨

    # æ·»åŠ é€²åº¦ç™¾åˆ†æ¯”
    if task.get("status") in ["pending", "processing"]:
        task["progress_percentage"] = calculate_progress_percentage(task)

    # æ·»åŠ é ä¼°å‰©é¤˜æ™‚é–“
    if task.get("status") == "processing":
        # ä½¿ç”¨å›ºå®šçš„é ä¼°å®Œæˆæ™‚é–“ï¼ˆåœ¨åˆ‡åˆ†å®Œæˆæ™‚è¨ˆç®—ä¸€æ¬¡ï¼‰
        estimated_completion_time = task.get("estimated_completion_time")

        if estimated_completion_time:
            # æ ¼å¼åŒ–å®Œæˆæ™‚é–“ç‚º MM/DD HH:MM
            try:
                completion_dt = datetime.strptime(estimated_completion_time, "%Y-%m-%d %H:%M:%S")
                task["estimated_completion_text"] = completion_dt.strftime("%m/%d %H:%M")
            except:
                task["estimated_completion_text"] = "è¨ˆç®—ä¸­......"
        else:
            # å°šæœªåˆ‡åˆ†å®Œæˆï¼Œé¡¯ç¤ºè¨ˆç®—ä¸­
            task["estimated_completion_text"] = "è¨ˆç®—ä¸­......"

    # æ·»åŠ æ ¼å¼åŒ–çš„ç¸½è™•ç†æ™‚é•·ï¼ˆå¦‚æœå·²å®Œæˆï¼‰
    # æ”¯æ´å·¢ç‹€çµæ§‹ï¼šstats.duration_seconds
    if task.get("status") == "completed":
        duration_seconds = None

        # å˜—è©¦å¾å·¢ç‹€çµæ§‹ç²å–
        if "stats" in task and task["stats"] and "duration_seconds" in task["stats"]:
            duration_seconds = task["stats"]["duration_seconds"]
        # å‘å¾Œå…¼å®¹ï¼šæ‰å¹³çµæ§‹
        elif "duration_seconds" in task:
            duration_seconds = task["duration_seconds"]

        if duration_seconds:
            task["duration_text"] = format_duration(duration_seconds)

    return task


@app.get("/transcribe/active/list")
async def list_active_tasks(current_user: dict = Depends(get_current_user)):
    """åˆ—å‡ºç•¶å‰ç”¨æˆ¶çš„è½‰éŒ„ä»»å‹™ï¼ˆéœ€èªè­‰ï¼‰- è¨˜æ†¶é«”å„ªåŒ–ç‰ˆ"""
    user_id = str(current_user["_id"])

    # è¨˜æ†¶é«”å„ªåŒ–ï¼šåªæŸ¥è©¢éœ€è¦çš„æ•¸é‡ï¼Œä¸è¦ä¸€æ¬¡æŸ¥ 100 å€‹
    # åªç²å–æœ€è¿‘ 30 å€‹ä»»å‹™ï¼ˆè¶³å¤ é¡¯ç¤ºï¼‰
    all_tasks = await task_repo.find_by_user(user_id, limit=30)
    active = await task_repo.find_active_by_user(user_id)

    # åˆä½µè¨˜æ†¶é«”ä¸­çš„å³æ™‚é€²åº¦è³‡è¨Šï¼ˆåªé‡å°é€²è¡Œä¸­çš„ä»»å‹™ï¼‰
    with tasks_lock:
        for task in active:
            task_id = task.get("task_id")
            if task_id and task_id in transcription_tasks:
                # åªæ›´æ–°å³æ™‚æ¬„ä½ï¼Œä¸è¤‡è£½æ•´å€‹ç‰©ä»¶
                live_info = transcription_tasks[task_id]
                if "progress" in live_info:
                    task["progress"] = live_info["progress"]
                if "progress_percentage" in live_info:
                    task["progress_percentage"] = live_info["progress_percentage"]
                if "diarization_status" in live_info:
                    task["diarization_status"] = live_info["diarization_status"]

    # ç‚ºæ‰€æœ‰ä»»å‹™æ·»åŠ è¨ˆç®—æ¬„ä½ï¼ˆä½¿ç”¨ç”Ÿæˆå™¨æ¸›å°‘è¨˜æ†¶é«”ï¼‰
    active_enriched = [enrich_task_data(task) for task in active]

    # åª enrich éœ€è¦è¿”å›çš„ä»»å‹™ï¼ˆæœ€å¤š 20 å€‹ï¼‰
    all_tasks_sorted = sorted(all_tasks, key=lambda t: (
        t.get("timestamps", {}).get("created_at") if isinstance(t.get("timestamps"), dict)
        else t.get("created_at", "")
    ), reverse=True)[:20]

    all_tasks_enriched = [enrich_task_data(task) for task in all_tasks_sorted]

    # é‡‹æ”¾ä¸éœ€è¦çš„è³‡æ–™ä¸¦å¼·åˆ¶åƒåœ¾å›æ”¶
    del all_tasks, all_tasks_sorted, active
    import gc
    gc.collect()

    return {
        "active_count": len(active_enriched),
        "total_count": await task_repo.count_by_user(user_id),  # ç›´æ¥æŸ¥è©¢è¨ˆæ•¸ï¼Œä¸è¼‰å…¥å…¨éƒ¨
        "active_tasks": active_enriched,  # å·²æ’åº
        "all_tasks": all_tasks_enriched  # å·²æ’åºä¸¦é™åˆ¶ç‚º 20 å€‹
    }


@app.get("/transcripts")
async def list_transcripts():
    """åˆ—å‡ºå·²ä¿å­˜çš„è½‰éŒ„æ–‡å­—æª”"""
    try:
        transcripts = []
        for txt_file in sorted(OUTPUT_DIR.glob("*.txt"), key=lambda x: x.stat().st_mtime, reverse=True):
            stat = txt_file.stat()
            transcripts.append({
                "filename": txt_file.name,
                "size_kb": round(stat.st_size / 1024, 2),
                "created": datetime.fromtimestamp(stat.st_mtime).strftime("%Y-%m-%d %H:%M:%S"),
                "path": str(txt_file.relative_to(OUTPUT_DIR.parent))
            })

        return {
            "total": len(transcripts),
            "output_dir": str(OUTPUT_DIR.relative_to(OUTPUT_DIR.parent)),
            "transcripts": transcripts
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/health")
async def health():
    """å¥åº·æª¢æŸ¥"""
    return {
        "status": "healthy",
        "model_loaded": whisper_model is not None,
        "model_name": current_model_name
    }


# ==================== éŸ³è¨Šç·¨è¼¯åŠŸèƒ½ ====================

# éŸ³è¨Šç‰‡æ®µå„²å­˜ç›®éŒ„
AUDIO_CLIPS_DIR = OUTPUT_DIR / "audio_clips"
AUDIO_CLIPS_DIR.mkdir(exist_ok=True)

# ç‰‡æ®µå…ƒæ•¸æ“šå„²å­˜
audio_clips: Dict[str, Dict[str, Any]] = {}
clips_lock = Lock()


def save_audio_clip(audio_segment: AudioSegment, source_filename: str, clip_id: str = None) -> Dict[str, Any]:
    """
    ä¿å­˜éŸ³è¨Šç‰‡æ®µåˆ°ç£ç¢Ÿ

    Args:
        audio_segment: pydub AudioSegment å°è±¡
        source_filename: ä¾†æºæª”æ¡ˆåç¨±
        clip_id: å¯é¸çš„ç‰‡æ®µ IDï¼ˆç”¨æ–¼åˆä½µï¼‰

    Returns:
        {
            "clip_id": str,
            "filename": str,
            "duration": float,
            "path": str
        }
    """
    if clip_id is None:
        clip_id = str(uuid.uuid4())

    timestamp = datetime.now(TZ_UTC8).strftime("%Y%m%d_%H%M%S")
    filename = f"clip_{timestamp}_{clip_id[:8]}.mp3"
    filepath = AUDIO_CLIPS_DIR / filename

    # å°å‡ºç‚º MP3
    audio_segment.export(str(filepath), format="mp3", bitrate="192k")

    clip_data = {
        "clip_id": clip_id,
        "filename": filename,
        "duration": len(audio_segment) / 1000.0,  # æ¯«ç§’è½‰ç§’
        "path": str(filepath),
        "source": source_filename,
        "created_at": get_current_time()
    }

    with clips_lock:
        audio_clips[clip_id] = clip_data

    return clip_data


@app.post("/audio/clip")
async def clip_audio(
    audio_file: UploadFile = File(...),
    regions: str = Form(..., description="å€æ®µ JSON é™£åˆ—ï¼Œæ ¼å¼ï¼š[{start, end, id}]")
):
    """
    å‰ªè¼¯éŸ³è¨Šæ–‡ä»¶ä¸­çš„æŒ‡å®šå€æ®µ

    - **audio_file**: åŸå§‹éŸ³è¨Šæ–‡ä»¶
    - **regions**: JSON å­—ä¸²ï¼ŒåŒ…å«å€æ®µé™£åˆ— [{"start": 10.5, "end": 25.3, "id": "xxx"}]

    Returns:
        {
            "clips": [
                {"clip_id": "...", "filename": "...", "duration": 14.8},
                ...
            ]
        }
    """
    temp_dir = Path(tempfile.mkdtemp())

    try:
        # ä¿å­˜ä¸Šå‚³çš„éŸ³æª”
        file_suffix = Path(audio_file.filename).suffix
        temp_audio_path = temp_dir / f"input{file_suffix}"

        with temp_audio_path.open("wb") as f:
            content = await audio_file.read()
            f.write(content)

        print(f"ğŸµ è¼‰å…¥éŸ³æª”é€²è¡Œå‰ªè¼¯ï¼š{audio_file.filename}")

        # è¼‰å…¥éŸ³æª”
        audio = AudioSegment.from_file(str(temp_audio_path))

        # è§£æå€æ®µ
        import json
        regions_data = json.loads(regions)

        if not regions_data or len(regions_data) == 0:
            raise HTTPException(status_code=400, detail="æœªæä¾›ä»»ä½•å€æ®µ")

        # å‰ªè¼¯æ¯å€‹å€æ®µ
        clips = []
        for idx, region in enumerate(regions_data):
            start_ms = int(region["start"] * 1000)
            end_ms = int(region["end"] * 1000)

            # é‚Šç•Œæª¢æŸ¥
            if end_ms > len(audio):
                end_ms = len(audio)

            if start_ms >= end_ms:
                raise HTTPException(
                    status_code=400,
                    detail=f"å€æ®µ {idx + 1} çš„èµ·å§‹æ™‚é–“å¤§æ–¼æˆ–ç­‰æ–¼çµæŸæ™‚é–“"
                )

            # æå–ç‰‡æ®µ
            clip_segment = audio[start_ms:end_ms]

            # ä¿å­˜ç‰‡æ®µ
            clip_data = save_audio_clip(clip_segment, audio_file.filename)
            clips.append({
                "clip_id": clip_data["clip_id"],
                "filename": clip_data["filename"],
                "duration": clip_data["duration"]
            })

        print(f"âœ… æˆåŠŸå‰ªè¼¯ {len(clips)} å€‹ç‰‡æ®µ")

        return JSONResponse({
            "clips": clips,
            "source_file": audio_file.filename,
            "total_clips": len(clips)
        })

    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="regions æ ¼å¼éŒ¯èª¤ï¼Œéœ€ç‚ºæœ‰æ•ˆ JSON")
    except Exception as e:
        print(f"âŒ å‰ªè¼¯å¤±æ•—ï¼š{e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"å‰ªè¼¯å¤±æ•—ï¼š{str(e)}")
    finally:
        # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
        cleanup_temp_dir(temp_dir)


@app.post("/audio/merge")
async def merge_audio(
    clip_ids: str = Form(..., description="è¦åˆä½µçš„ç‰‡æ®µ ID é™£åˆ—ï¼ˆJSON å­—ä¸²ï¼‰"),
    mode: str = Form("different-files", description="åˆä½µæ¨¡å¼")
):
    """
    åˆä½µå¤šå€‹éŸ³è¨Šç‰‡æ®µ

    - **clip_ids**: ç‰‡æ®µ ID é™£åˆ—ï¼ˆJSON å­—ä¸²ï¼‰
    - **mode**: åˆä½µæ¨¡å¼
        - "different-files": åˆä½µä¸åŒéŸ³æª”ï¼ˆä¸­é–“ç„¡é–“éš”ï¼‰
        - "same-file-clips": åˆä½µåŒä¸€éŸ³æª”çš„ç‰‡æ®µï¼ˆä¿æŒåŸå§‹æ™‚é–“é †åºï¼‰

    Returns:
        {
            "merged_id": "...",
            "filename": "...",
            "duration": 120.5
        }
    """
    try:
        # è§£æ clip_ids
        import json
        clip_ids_list = json.loads(clip_ids)

        if len(clip_ids_list) < 2:
            raise HTTPException(status_code=400, detail="è‡³å°‘éœ€è¦ 2 å€‹ç‰‡æ®µ")

        # å–å¾—æ‰€æœ‰ç‰‡æ®µ
        with clips_lock:
            clips_to_merge = []
            for clip_id in clip_ids_list:
                if clip_id not in audio_clips:
                    raise HTTPException(status_code=404, detail=f"ç‰‡æ®µ {clip_id} ä¸å­˜åœ¨")
                clips_to_merge.append(audio_clips[clip_id])

        print(f"ğŸ”— åˆä½µ {len(clips_to_merge)} å€‹ç‰‡æ®µï¼ˆæ¨¡å¼ï¼š{mode}ï¼‰")

        # è¼‰å…¥æ‰€æœ‰ç‰‡æ®µ
        segments = []
        for clip_data in clips_to_merge:
            segment = AudioSegment.from_file(clip_data["path"])
            segments.append(segment)

        # åˆä½µé‚è¼¯
        merged = segments[0]
        for seg in segments[1:]:
            merged += seg

        # ä¿å­˜åˆä½µçµæœ
        merged_id = str(uuid.uuid4())
        merged_data = save_audio_clip(
            merged,
            f"merged_{len(clips_to_merge)}_clips",
            merged_id
        )

        duration_str = format_duration(merged_data['duration'])
        print(f"âœ… åˆä½µå®Œæˆï¼š{merged_data['filename']} ({duration_str})")

        return JSONResponse({
            "merged_id": merged_data["clip_id"],
            "filename": merged_data["filename"],
            "duration": merged_data["duration"]
        })

    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="clip_ids æ ¼å¼éŒ¯èª¤")
    except Exception as e:
        print(f"âŒ åˆä½µå¤±æ•—ï¼š{e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"åˆä½µå¤±æ•—ï¼š{str(e)}")


@app.get("/audio/download/{clip_id}")
async def download_clip(clip_id: str):
    """ä¸‹è¼‰éŸ³è¨Šç‰‡æ®µæˆ–åˆä½µçµæœ"""
    with clips_lock:
        if clip_id not in audio_clips:
            raise HTTPException(status_code=404, detail="ç‰‡æ®µä¸å­˜åœ¨")

        clip_data = audio_clips[clip_id]

    filepath = Path(clip_data["path"])

    if not filepath.exists():
        raise HTTPException(status_code=404, detail="æª”æ¡ˆå·²è¢«åˆªé™¤")

    return FileResponse(
        path=str(filepath),
        filename=clip_data["filename"],
        media_type="audio/mpeg"
    )


@app.post("/audio/cleanup")
async def cleanup_old_clips(max_age_hours: int = 24):
    """
    æ¸…ç†è¶…éæŒ‡å®šæ™‚é–“çš„éŸ³è¨Šç‰‡æ®µ

    - **max_age_hours**: æœ€å¤§ä¿ç•™æ™‚é–“ï¼ˆå°æ™‚ï¼‰ï¼Œé è¨­ 24 å°æ™‚
    """
    from datetime import datetime, timedelta

    cutoff_time = datetime.now(TZ_UTC8) - timedelta(hours=max_age_hours)

    deleted_count = 0
    with clips_lock:
        clips_to_delete = []

        for clip_id, clip_data in audio_clips.items():
            # è§£æå‰µå»ºæ™‚é–“
            created_str = clip_data.get("created_at", "")
            try:
                created_time = datetime.strptime(created_str, "%Y-%m-%d %H:%M:%S")
                created_time = created_time.replace(tzinfo=TZ_UTC8)

                if created_time < cutoff_time:
                    clips_to_delete.append(clip_id)
            except:
                continue

        # åˆªé™¤éæœŸç‰‡æ®µ
        for clip_id in clips_to_delete:
            clip_data = audio_clips[clip_id]
            filepath = Path(clip_data["path"])

            if filepath.exists():
                filepath.unlink()

            del audio_clips[clip_id]
            deleted_count += 1

    print(f"ğŸ§¹ æ¸…ç†äº† {deleted_count} å€‹éæœŸéŸ³è¨Šç‰‡æ®µ")

    return JSONResponse({
        "deleted_count": deleted_count,
        "message": f"æˆåŠŸæ¸…ç† {deleted_count} å€‹è¶…é {max_age_hours} å°æ™‚çš„ç‰‡æ®µ"
    })


@app.post("/audio/convert-to-web-format")
async def convert_audio_to_web_format(
    audio_file: UploadFile = File(..., description="è¦è½‰æ›çš„éŸ³æª”")
):
    """
    å°‡éŸ³è¨Šæª”æ¡ˆè½‰æ›ç‚ºç€è¦½å™¨ç›¸å®¹çš„æ ¼å¼ (MP3)
    ç”¨æ–¼è§£æ±ºç€è¦½å™¨ç„¡æ³•è§£ç¢¼æŸäº›æ ¼å¼çš„å•é¡Œ
    """
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_dir_path = Path(temp_dir)

        # å„²å­˜ä¸Šå‚³çš„æª”æ¡ˆ
        temp_input_path = temp_dir_path / audio_file.filename
        with open(temp_input_path, "wb") as f:
            content = await audio_file.read()
            f.write(content)

        try:
            # ä½¿ç”¨ pydub è¼‰å…¥éŸ³è¨Šï¼ˆæ”¯æ´å„ç¨®æ ¼å¼ï¼‰
            print(f"ğŸ”„ æ­£åœ¨è½‰æ›éŸ³è¨Šæ ¼å¼ï¼š{audio_file.filename}")
            audio = AudioSegment.from_file(str(temp_input_path))

            # è½‰æ›ç‚º MP3 æ ¼å¼ï¼ˆç€è¦½å™¨å»£æ³›æ”¯æ´ï¼‰
            # ä½¿ç”¨è¼ƒé«˜çš„ä½å…ƒç‡ä»¥ä¿æŒéŸ³è³ª
            output_filename = f"converted_{Path(audio_file.filename).stem}.mp3"
            output_path = AUDIO_CLIPS_DIR / output_filename

            audio.export(
                str(output_path),
                format="mp3",
                bitrate="192k",
                parameters=["-ar", "44100"]  # 44.1kHz å–æ¨£ç‡
            )

            # å–å¾—éŸ³è¨Šè³‡è¨Š
            duration = len(audio) / 1000.0
            file_size = output_path.stat().st_size

            # å„²å­˜åˆ° clips ç®¡ç†
            clip_id = str(uuid.uuid4())
            with clips_lock:
                audio_clips[clip_id] = {
                    "clip_id": clip_id,
                    "filename": output_filename,
                    "path": str(output_path),
                    "duration": duration,
                    "size": file_size,
                    "created_at": datetime.now(),
                    "original_filename": audio_file.filename
                }

            print(f"âœ… éŸ³è¨Šè½‰æ›å®Œæˆï¼š{output_filename} ({duration:.2f}ç§’, {file_size / 1024 / 1024:.2f}MB)")

            return JSONResponse({
                "clip_id": clip_id,
                "filename": output_filename,
                "duration": duration,
                "size": file_size,
                "format": "mp3",
                "message": "éŸ³è¨Šå·²æˆåŠŸè½‰æ›ç‚ºç€è¦½å™¨ç›¸å®¹æ ¼å¼"
            })

        except Exception as e:
            print(f"âŒ éŸ³è¨Šè½‰æ›å¤±æ•—ï¼š{str(e)}")
            raise HTTPException(status_code=400, detail=f"éŸ³è¨Šè½‰æ›å¤±æ•—ï¼š{str(e)}")


# ==================== Admin Dashboard API ====================

@app.get("/api/admin/statistics")
async def get_admin_statistics():
    """ç²å–å¾Œå°çµ±è¨ˆè³‡æ–™ï¼ˆæš«æ™‚ç„¡éœ€èªè­‰ï¼‰"""
    try:
        db = MongoDB.get_db()

        # 1. ç¸½é«”çµ±è¨ˆ
        total_tasks = await db.tasks.count_documents({})
        completed_tasks = await db.tasks.count_documents({"status": "completed"})
        processing_tasks = await db.tasks.count_documents({"status": "processing"})
        failed_tasks = await db.tasks.count_documents({"status": "failed"})

        # 2. Token ä½¿ç”¨çµ±è¨ˆ
        token_pipeline = [
            {
                "$match": {
                    "stats.token_usage.total": {"$exists": True, "$ne": None}
                }
            },
            {
                "$group": {
                    "_id": None,
                    "total_tokens": {"$sum": "$stats.token_usage.total"},
                    "total_prompt_tokens": {"$sum": "$stats.token_usage.prompt"},
                    "total_completion_tokens": {"$sum": "$stats.token_usage.completion"},
                    "tasks_with_tokens": {"$sum": 1}
                }
            }
        ]
        token_stats_cursor = db.tasks.aggregate(token_pipeline)
        token_stats_list = await token_stats_cursor.to_list(length=1)
        token_stats = token_stats_list[0] if token_stats_list else {
            "total_tokens": 0,
            "total_prompt_tokens": 0,
            "total_completion_tokens": 0,
            "tasks_with_tokens": 0
        }

        # 3. æ¨¡å‹ä½¿ç”¨çµ±è¨ˆ
        model_pipeline = [
            {
                "$match": {
                    "stats.token_usage.model": {"$exists": True, "$ne": None}
                }
            },
            {
                "$group": {
                    "_id": "$stats.token_usage.model",
                    "count": {"$sum": 1},
                    "total_tokens": {"$sum": "$stats.token_usage.total"}
                }
            },
            {
                "$sort": {"count": -1}
            }
        ]
        model_stats_cursor = db.tasks.aggregate(model_pipeline)
        model_stats = await model_stats_cursor.to_list(length=None)

        # 4. æ¯æ—¥çµ±è¨ˆï¼ˆæœ€è¿‘ 30 å¤©ï¼‰
        from datetime import datetime, timedelta
        thirty_days_ago = (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d")

        daily_pipeline = [
            {
                "$match": {
                    "timestamps.created_at": {"$gte": thirty_days_ago}
                }
            },
            {
                "$group": {
                    "_id": {"$substr": ["$timestamps.created_at", 0, 10]},
                    "tasks_count": {"$sum": 1},
                    "tokens_used": {"$sum": {"$ifNull": ["$stats.token_usage.total", 0]}}
                }
            },
            {
                "$sort": {"_id": 1}
            }
        ]
        daily_stats_cursor = db.tasks.aggregate(daily_pipeline)
        daily_stats = await daily_stats_cursor.to_list(length=None)

        # 5. ä½¿ç”¨è€…çµ±è¨ˆ
        user_stats_pipeline = [
            {
                "$group": {
                    "_id": "$user.user_id",
                    "tasks_count": {"$sum": 1},
                    "tokens_used": {"$sum": {"$ifNull": ["$stats.token_usage.total", 0]}}
                }
            },
            {
                "$sort": {"tasks_count": -1}
            },
            {
                "$limit": 10
            }
        ]
        user_stats_cursor = db.tasks.aggregate(user_stats_pipeline)
        top_users = await user_stats_cursor.to_list(length=None)

        # 6. å¹³å‡è™•ç†æ™‚é–“
        avg_duration_pipeline = [
            {
                "$match": {
                    "status": "completed",
                    "stats.duration_seconds": {"$exists": True, "$ne": None}
                }
            },
            {
                "$group": {
                    "_id": None,
                    "avg_duration": {"$avg": "$stats.duration_seconds"},
                    "min_duration": {"$min": "$stats.duration_seconds"},
                    "max_duration": {"$max": "$stats.duration_seconds"}
                }
            }
        ]
        duration_stats_cursor = db.tasks.aggregate(avg_duration_pipeline)
        duration_stats_list = await duration_stats_cursor.to_list(length=1)
        duration_stats = duration_stats_list[0] if duration_stats_list else {
            "avg_duration": 0,
            "min_duration": 0,
            "max_duration": 0
        }

        # 7. æ¨™é»ç¬¦è™Ÿæœå‹™ä½¿ç”¨çµ±è¨ˆ
        punct_pipeline = [
            {
                "$group": {
                    "_id": "$config.punct_provider",
                    "count": {"$sum": 1}
                }
            },
            {
                "$sort": {"count": -1}
            }
        ]
        punct_stats_cursor = db.tasks.aggregate(punct_pipeline)
        punct_stats = await punct_stats_cursor.to_list(length=None)

        return {
            "overview": {
                "total_tasks": total_tasks,
                "completed_tasks": completed_tasks,
                "processing_tasks": processing_tasks,
                "failed_tasks": failed_tasks,
                "success_rate": round(completed_tasks / total_tasks * 100, 2) if total_tasks > 0 else 0
            },
            "token_usage": {
                "total_tokens": token_stats.get("total_tokens", 0),
                "prompt_tokens": token_stats.get("total_prompt_tokens", 0),
                "completion_tokens": token_stats.get("total_completion_tokens", 0),
                "tasks_with_tokens": token_stats.get("tasks_with_tokens", 0),
                "avg_tokens_per_task": round(token_stats.get("total_tokens", 0) / token_stats.get("tasks_with_tokens", 1), 2) if token_stats.get("tasks_with_tokens", 0) > 0 else 0
            },
            "model_usage": [
                {
                    "model": stat["_id"] or "æœªçŸ¥",
                    "count": stat["count"],
                    "total_tokens": stat.get("total_tokens", 0)
                }
                for stat in model_stats
            ],
            "daily_stats": [
                {
                    "date": stat["_id"],
                    "tasks_count": stat["tasks_count"],
                    "tokens_used": stat["tokens_used"]
                }
                for stat in daily_stats
            ],
            "top_users": [
                {
                    "user_id": stat["_id"],
                    "tasks_count": stat["tasks_count"],
                    "tokens_used": stat["tokens_used"]
                }
                for stat in top_users
            ],
            "performance": {
                "avg_duration_seconds": round(duration_stats.get("avg_duration", 0), 2),
                "min_duration_seconds": round(duration_stats.get("min_duration", 0), 2),
                "max_duration_seconds": round(duration_stats.get("max_duration", 0), 2)
            },
            "punct_provider_usage": [
                {
                    "provider": stat["_id"] or "none",
                    "count": stat["count"]
                }
                for stat in punct_stats
            ]
        }

    except Exception as e:
        print(f"âŒ ç²å–çµ±è¨ˆè³‡æ–™å¤±æ•—ï¼š{e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"ç²å–çµ±è¨ˆå¤±æ•—ï¼š{str(e)}")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Whisper è½‰éŒ„æœå‹™")
    parser.add_argument("--host", default="0.0.0.0", help="ç¶å®šçš„ IP åœ°å€")
    parser.add_argument("--port", type=int, default=8000, help="ç¶å®šçš„ç«¯å£")
    parser.add_argument("--model", default=DEFAULT_MODEL, help="Whisper æ¨¡å‹åç¨±")
    args = parser.parse_args()

    # æ›´æ–°é è¨­æ¨¡å‹
    DEFAULT_MODEL = args.model

    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   Whisper è½‰éŒ„æœå‹™ - FastAPI ç‰ˆæœ¬   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

æœå‹™åœ°å€: http://{args.host}:{args.port}
API æ–‡æª”: http://{args.host}:{args.port}/docs
Whisper æ¨¡å‹: {args.model}
Google API Keys: {len(GOOGLE_API_KEYS)} å€‹å·²è¼‰å…¥

""")

    uvicorn.run(app, host=args.host, port=args.port)
 