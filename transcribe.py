import os
import sys
import argparse
from pathlib import Path
import whisper
from pydub import AudioSegment

# â€”â€” å¯èª¿åƒæ•¸ â€”â€” #
DEFAULT_MODEL = "small"          # tiny | base | small | medium
USE_WAV = True
OPENAI_MODEL = "gpt-4o-mini"     # ç”¨ä¾†åŠ æ¨™é»ï¼ˆOpenAIï¼‰
GEMINI_MODEL = "gemini-2.0-flash" # ç”¨ä¾†åŠ æ¨™é»ï¼ˆGeminiï¼‰
CHUNK_DURATION_MS = 10 * 60 * 1000  # éŸ³æª”åˆ†æ®µé•·åº¦ï¼ˆæ¯«ç§’ï¼‰ï¼Œé è¨­ 10 åˆ†é˜


# export GOOGLE_API_KEY="AIzaSyDk8uzpNcvWjp8yPELvcApvABMnehzlT_E"
# python3 transcribe.py -i 574181344843137331.mp4 -m small --punct-provider gemini

# ---------- Audio Chunking ----------

def transcribe_audio_in_chunks(audio_path: Path, model, chunk_duration_ms: int = CHUNK_DURATION_MS) -> str:
    """å°‡éŸ³æª”åˆ†æ®µå¾Œé€æ®µè½‰éŒ„ï¼Œæé«˜æº–ç¢ºåº¦"""
    from pydub import AudioSegment
    from pydub.silence import detect_nonsilent

    print(f"ğŸ”Š è¼‰å…¥éŸ³æª”ï¼š{audio_path.name}")
    audio = AudioSegment.from_file(audio_path)
    total_duration_ms = len(audio)
    total_minutes = total_duration_ms / 1000 / 60

    print(f"ğŸ“Š éŸ³æª”ç¸½é•·åº¦ï¼š{total_minutes:.1f} åˆ†é˜")

    # å¦‚æœéŸ³æª”ä¸é•·ï¼Œç›´æ¥è½‰éŒ„
    if total_duration_ms <= chunk_duration_ms:
        print(f"ğŸ“ éŸ³æª”é•·åº¦åœ¨ {chunk_duration_ms/1000/60:.0f} åˆ†é˜å…§ï¼Œç›´æ¥è½‰éŒ„...")
        result = model.transcribe(str(audio_path), language="zh", fp16=False)
        return (result.get("text") or "").strip()

    # é•·éŸ³æª”ï¼šåˆ†æ®µè™•ç†
    num_chunks = (total_duration_ms + chunk_duration_ms - 1) // chunk_duration_ms
    print(f"ğŸ”„ éŸ³æª”è¼ƒé•·ï¼Œå°‡åˆ†ç‚º {num_chunks} æ®µè™•ç†ï¼ˆæ¯æ®µç´„ {chunk_duration_ms/1000/60:.0f} åˆ†é˜ï¼‰...")

    chunks_text = []
    start_ms = 0
    chunk_idx = 1

    while start_ms < total_duration_ms:
        end_ms = min(start_ms + chunk_duration_ms, total_duration_ms)

        # å¦‚æœä¸æ˜¯æœ€å¾Œä¸€æ®µï¼Œå˜—è©¦åœ¨éœéŸ³è™•åˆ‡åˆ†
        if end_ms < total_duration_ms:
            # åœ¨é å®šåˆ‡åˆ†é»å‰å¾Œ 30 ç§’å…§æ‰¾éœéŸ³
            search_start = max(end_ms - 30000, start_ms)
            search_end = min(end_ms + 30000, total_duration_ms)
            search_segment = audio[search_start:search_end]

            # åµæ¸¬ééœéŸ³å€é–“ï¼ˆéœéŸ³é–¾å€¼ -40dBï¼Œæœ€å°éœéŸ³é•·åº¦ 500msï¼‰
            nonsilent_ranges = detect_nonsilent(
                search_segment,
                min_silence_len=500,
                silence_thresh=-40
            )

            # æ‰¾åˆ°æœ€æ¥è¿‘é å®šåˆ‡åˆ†é»çš„éœéŸ³é–“éš”
            if nonsilent_ranges:
                target_pos = end_ms - search_start
                best_split = end_ms
                min_diff = float('inf')

                for i in range(len(nonsilent_ranges) - 1):
                    gap_start = nonsilent_ranges[i][1]
                    gap_end = nonsilent_ranges[i + 1][0]
                    gap_mid = (gap_start + gap_end) // 2

                    diff = abs(gap_mid - target_pos)
                    if diff < min_diff:
                        min_diff = diff
                        best_split = search_start + gap_mid

                # å¦‚æœæ‰¾åˆ°çš„åˆ‡åˆ†é»åœ¨åˆç†ç¯„åœå…§ï¼Œå°±ä½¿ç”¨å®ƒ
                if abs(best_split - end_ms) < 60000:  # 60 ç§’å…§
                    end_ms = best_split

        print(f"   è™•ç†ç¬¬ {chunk_idx}/{num_chunks} æ®µ ({start_ms/1000/60:.1f}-{end_ms/1000/60:.1f} åˆ†é˜)...")

        # æå–éŸ³æª”ç‰‡æ®µ
        chunk_audio = audio[start_ms:end_ms]

        # å„²å­˜è‡¨æ™‚æª”æ¡ˆ
        temp_path = audio_path.parent / f"_temp_chunk_{chunk_idx}.wav"
        chunk_audio.export(temp_path, format="wav")

        # è½‰éŒ„
        result = model.transcribe(str(temp_path), language="zh", fp16=False)
        chunk_text = (result.get("text") or "").strip()
        chunks_text.append(chunk_text)

        # åˆªé™¤è‡¨æ™‚æª”æ¡ˆ
        temp_path.unlink()

        start_ms = end_ms
        chunk_idx += 1

    print("âœ… æ‰€æœ‰éŸ³æª”ç‰‡æ®µè½‰éŒ„å®Œæˆ")
    return " ".join(chunks_text)

# ---------- Punctuation Providers ----------

def punctuate_with_openai(text: str) -> str:
    """ç”¨ OpenAI å¹«é€å­—ç¨¿åŠ æ¨™é»èˆ‡åˆ†æ®µ"""
    from openai import OpenAI
    client = OpenAI()  # è®€ OPENAI_API_KEY
    prompt = (
        "è«‹å°‡ä»¥ä¸‹ã€ä¸­æ–‡é€å­—ç¨¿ã€åŠ ä¸Šé©ç•¶æ¨™é»ç¬¦è™Ÿä¸¦åˆç†åˆ†æ®µã€‚"
        "ä¸è¦çœç•¥æˆ–æ·»åŠ å…§å®¹ï¼Œä¸è¦æ„è­¯ï¼Œä¿ç•™å›ºæœ‰åè©èˆ‡æ•¸å­—ã€‚"
        "è¼¸å‡ºç´”æ–‡å­—å³å¯ï¼š\n\n"
        f"{text}"
    )
    resp = client.chat.completions.create(
        model=OPENAI_MODEL,
        messages=[
            {"role": "system", "content": "ä½ æ˜¯åš´è¬¹çš„é€å­—ç¨¿æ½¤é£¾åŠ©æ‰‹ï¼Œåªåšæ¨™é»èˆ‡åˆ†æ®µã€‚"},
            {"role": "user", "content": prompt},
        ],
        temperature=0.2,
    )
    return resp.choices[0].message.content.strip()

def punctuate_with_gemini(text: str, chunk_size: int = 3000) -> str:
    """ç”¨ Google Gemini å¹«é€å­—ç¨¿åŠ æ¨™é»èˆ‡åˆ†æ®µï¼ˆæ”¯æ´é•·æ–‡æœ¬åˆ†æ®µè™•ç†ï¼‰"""
    import google.generativeai as genai
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        raise RuntimeError("æœªè¨­å®š GOOGLE_API_KEY")
    genai.configure(api_key=api_key)

    model = genai.GenerativeModel(GEMINI_MODEL)

    # å¦‚æœæ–‡æœ¬ä¸é•·ï¼Œç›´æ¥è™•ç†
    if len(text) <= chunk_size:
        system_msg = (
            "ä½ æ˜¯åš´è¬¹çš„é€å­—ç¨¿æ½¤é£¾åŠ©æ‰‹ã€‚åªåšã€ä¸­æ–‡æ¨™é»è£œå…¨èˆ‡åˆç†åˆ†æ®µã€ï¼Œ"
            "ä¸è¦çœç•¥æˆ–æ·»åŠ å…§å®¹ï¼Œä¸è¦æ„è­¯ï¼Œéå¿…è¦ä¸è¦ç”¨åˆªç¯€è™Ÿï¼Œä¿ç•™å›ºæœ‰åè©èˆ‡æ•¸å­—ï¼š "
        )
        user_msg = (
            "è«‹ç‚ºä»¥ä¸‹ä¸­æ–‡é€å­—ç¨¿åŠ ä¸Šé©ç•¶æ¨™é»ä¸¦åˆ†æ®µï¼š\n\n" + text
        )
        resp = model.generate_content(
            [{"role": "user", "parts": [system_msg + "\n\n" + user_msg]}],
            generation_config={"temperature": 0.2}
        )
        return (resp.text or "").strip()

    # é•·æ–‡æœ¬ï¼šåˆ†æ®µè™•ç†
    print(f"ğŸ“Š æ–‡æœ¬é•·åº¦ {len(text)} å­—ï¼Œå°‡åˆ†æ®µè™•ç†ï¼ˆæ¯æ®µç´„ {chunk_size} å­—ï¼‰...")
    chunks = []
    start = 0

    while start < len(text):
        end = start + chunk_size
        # é¿å…åˆ‡æ–·å¥å­ï¼Œå¾€å‰æ‰¾æœ€è¿‘çš„å¥è™Ÿã€å•è™Ÿã€é©šå˜†è™Ÿæˆ–æ›è¡Œ
        if end < len(text):
            for i in range(end, max(start + chunk_size // 2, end - 200), -1):
                if text[i] in 'ã€‚ï¼Ÿï¼\n':
                    end = i + 1
                    break
        chunks.append(text[start:end])
        start = end

    print(f"ğŸ”„ å…±åˆ†ç‚º {len(chunks)} æ®µè™•ç†...")
    results = []

    for idx, chunk in enumerate(chunks, 1):
        print(f"   è™•ç†ç¬¬ {idx}/{len(chunks)} æ®µ...")
        system_msg = (
            "ä½ æ˜¯åš´è¬¹çš„é€å­—ç¨¿æ½¤é£¾åŠ©æ‰‹ã€‚åªåšã€ä¸­æ–‡æ¨™é»è£œå…¨èˆ‡åˆç†åˆ†æ®µã€ï¼Œ"
            "ä¸è¦çœç•¥æˆ–æ·»åŠ å…§å®¹ï¼Œä¸è¦æ„è­¯ï¼Œéå¿…è¦ä¸è¦ç”¨åˆªç¯€è™Ÿï¼Œä¿ç•™å›ºæœ‰åè©èˆ‡æ•¸å­—ã€‚"
        )
        if idx == 1:
            user_msg = f"è«‹ç‚ºä»¥ä¸‹ä¸­æ–‡é€å­—ç¨¿åŠ ä¸Šé©ç•¶æ¨™é»ä¸¦åˆ†æ®µï¼ˆé€™æ˜¯ç¬¬ 1 æ®µï¼‰ï¼š\n\n{chunk}"
        elif idx == len(chunks):
            user_msg = f"è«‹ç‚ºä»¥ä¸‹ä¸­æ–‡é€å­—ç¨¿åŠ ä¸Šé©ç•¶æ¨™é»ä¸¦åˆ†æ®µï¼ˆé€™æ˜¯æœ€å¾Œä¸€æ®µï¼Œæ¥çºŒå‰æ–‡ï¼‰ï¼š\n\n{chunk}"
        else:
            user_msg = f"è«‹ç‚ºä»¥ä¸‹ä¸­æ–‡é€å­—ç¨¿åŠ ä¸Šé©ç•¶æ¨™é»ä¸¦åˆ†æ®µï¼ˆé€™æ˜¯ç¬¬ {idx} æ®µï¼Œæ¥çºŒå‰æ–‡ï¼‰ï¼š\n\n{chunk}"

        resp = model.generate_content(
            [{"role": "user", "parts": [system_msg + "\n\n" + user_msg]}],
            generation_config={"temperature": 0.2}
        )
        results.append((resp.text or "").strip())

    print("âœ… æ‰€æœ‰æ®µè½è™•ç†å®Œæˆï¼Œæ­£åœ¨åˆä½µ...")
    return "\n\n".join(results)

# ---------- Main ----------

def main():
    parser = argparse.ArgumentParser(
        description="Whisper è½‰é€å­—ç¨¿ â†’ï¼ˆOpenAI/Geminiï¼‰è‡ªå‹•åŠ ä¸­æ–‡æ¨™é»èˆ‡åˆ†æ®µ"
    )
    parser.add_argument("-i", "--input", required=True, help="è¼¸å…¥éŸ³æª”è·¯å¾‘ï¼ˆm4a/mp3/mp3/wav ç­‰ï¼‰")
    parser.add_argument("-m", "--model", default=DEFAULT_MODEL, help="Whisper æ¨¡å‹ï¼ˆtiny/base/small/medium/large-v2ï¼‰")
    parser.add_argument("--no-wav", action="store_true", help="ç›´æ¥ç”¨åŸæª”ï¼Œä¸è½‰æˆ wav")
    parser.add_argument("--punct-provider", choices=["openai", "gemini", "none"],
                        default="openai", help="é¸æ“‡åŠ æ¨™é»æä¾›è€…ï¼ˆopenai|gemini|noneï¼‰")
    parser.add_argument("--chunk-audio", action="store_true", help="å°‡éŸ³æª”åˆ†æ®µå¾Œå†è½‰éŒ„ï¼ˆæé«˜é•·éŸ³æª”æº–ç¢ºåº¦ï¼‰")
    parser.add_argument("--chunk-minutes", type=int, default=10, help="éŸ³æª”åˆ†æ®µé•·åº¦ï¼ˆåˆ†é˜ï¼‰ï¼Œé è¨­ 10 åˆ†é˜")
    args = parser.parse_args()

    audio_path = Path(args.input).expanduser().resolve()
    if not audio_path.exists():
        print(f"âŒ æ‰¾ä¸åˆ°éŸ³æª”ï¼š{audio_path}")
        sys.exit(1)

    raw_txt_path = audio_path.with_suffix(".raw.txt")
    pretty_txt_path = audio_path.with_suffix(".txt")

    # è‹¥å·²æœ‰æœ€çµ‚ txtï¼Œç›´æ¥ç•¥é
    if pretty_txt_path.exists():
        print(f"ğŸ“‚ åµæ¸¬åˆ°å·²å­˜åœ¨å¸¶æ¨™é»é€å­—ç¨¿ï¼š{pretty_txt_path.name}ï¼Œè·³éè™•ç†ã€‚")
        return

    # è‹¥å·²æœ‰ raw.txtï¼Œå°±ç›´æ¥é€²å…¥æ¨™é»éšæ®µ
    if raw_txt_path.exists():
        print(f"ğŸ“‚ åµæ¸¬åˆ°å·²å­˜åœ¨åŸå§‹é€å­—ç¨¿ï¼š{raw_txt_path.name}ï¼Œè·³éè½‰éŒ„ã€‚")
        raw_text = raw_txt_path.read_text(encoding="utf-8")
    else:
        # éœ€è¦è½‰éŒ„
        use_wav = not args.no_wav and USE_WAV
        to_transcribe = audio_path
        if use_wav:
            wav_path = audio_path.with_suffix(".wav")
            if wav_path.exists():
                print(f"ğŸ“‚ åµæ¸¬åˆ°å·²å­˜åœ¨ WAV æª”ï¼Œç›´æ¥ä½¿ç”¨ï¼š{wav_path.name}")
            else:
                print(f"ğŸ”„ è½‰æª”ç‚º WAVï¼š{wav_path.name}")
                AudioSegment.from_file(audio_path).export(wav_path, format="wav")
            to_transcribe = wav_path

        print(f"ğŸ™ è¼‰å…¥ Whisper æ¨¡å‹ï¼š{args.model}")
        model = whisper.load_model(args.model)

        # æ ¹æ“šåƒæ•¸é¸æ“‡åˆ†æ®µæˆ–æ•´æ®µè½‰éŒ„
        if args.chunk_audio:
            chunk_duration_ms = args.chunk_minutes * 60 * 1000
            raw_text = transcribe_audio_in_chunks(to_transcribe, model, chunk_duration_ms)
        else:
            print(f"ğŸ“ é–‹å§‹è½‰é€å­—ç¨¿ï¼š{to_transcribe.name}")
            result = model.transcribe(str(to_transcribe), language="zh", fp16=False)
            raw_text = (result.get("text") or "").strip()

        raw_txt_path.write_text(raw_text, encoding="utf-8")
        print(f"âœ… å·²å„²å­˜åŸå§‹é€å­—ç¨¿ï¼š{raw_txt_path.name}")

    # æ˜¯å¦åŠ æ¨™é»
    if args.punct_provider == "none":
        print("â„¹ï¸ æœªé¸æ“‡æ¨™é»æä¾›è€…ï¼ˆ--punct-provider noneï¼‰ï¼Œåƒ…è¼¸å‡ºåŸå§‹é€å­—ç¨¿ã€‚")
        return

    try:
        if args.punct_provider == "gemini":
            print("âœ¨ ä½¿ç”¨ Gemini åŠ æ¨™é»èˆ‡åˆ†æ®µ ...")
            pretty = punctuate_with_gemini(raw_text)
        else:
            # é è¨­ openai
            if not os.getenv("OPENAI_API_KEY"):
                print("âš ï¸ æœªè¨­å®š OPENAI_API_KEYï¼Œç„¡æ³•ä½¿ç”¨ OpenAI æ¨™é»ã€‚ä½ å¯æ”¹ç”¨ --punct-provider geminiã€‚")
                return
            print("âœ¨ ä½¿ç”¨ OpenAI åŠ æ¨™é»èˆ‡åˆ†æ®µ ...")
            pretty = punctuate_with_openai(raw_text)

        pretty_txt_path.write_text(pretty, encoding="utf-8")
        print(f"ğŸ‰ å·²è¼¸å‡ºå¸¶æ¨™é»é€å­—ç¨¿ï¼š{pretty_txt_path.name}")
    except Exception as e:
        print(f"âš ï¸ åŠ æ¨™é»å¤±æ•—ï¼ˆ{e}ï¼‰ï¼Œå·²ä¿ç•™åŸå§‹é€å­—ç¨¿ï¼š{raw_txt_path.name}")

if __name__ == "__main__":
    main()
